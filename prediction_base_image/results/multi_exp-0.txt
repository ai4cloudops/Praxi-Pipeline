MULTILABEL EXPERIMENT REPORT
Generated Mon Jul 10 20:37:48 2023

Args: -b 26 --learning_rate 1.5 --passes 30 --csoaa 35 --kill_cache --cache_file a.cache

EXPERIMENT WITH 164 TEST CHANGESETS
F1 SCORE : 0.553 weighted, 0.529 micro-avg'd, 0.444 macro-avg'd
PRECISION: 0.626 weighted, 0.476 micro-avg'd, 0.369 macro-avg'd
RECALL   : 0.594 weighted, 0.594 micro-avg'd, 0.795 macro-avg'd

 -----------------CLASSIFICATION REPORT-----------------
                          precision    recall  f1-score   support

              SQLAlchemy       0.29      1.00      0.44         2
                  Scrapy       0.29      1.00      0.44         2
                  Theano       0.29      1.00      0.44         2
                 astropy       0.29      1.00      0.44         2
          beautifulsoup4       0.29      1.00      0.44         2
               biopython       0.29      1.00      0.44         2
                   bokeh       0.29      1.00      0.44         2
                   boto3       0.80      0.40      0.53        30
                   cmake       0.88      0.48      0.62        31
                    dask       0.40      1.00      0.57         2
                    deap       0.29      1.00      0.44         2
                  jinja2       0.69      0.52      0.59        21
              matplotlib       0.74      0.47      0.57        30
                networkx       0.00      0.00      0.00         0
                 nilearn       0.29      1.00      0.44         2
  nvidia-cuda-nvrtc-cu11       0.64      0.33      0.44        21
nvidia-cuda-runtime-cu11       0.29      1.00      0.44         2
           opencv-python       0.29      1.00      0.44         2
                  pandas       0.83      0.92      0.87        26
                  pillow       0.17      0.20      0.18         5
                  plotly       0.29      1.00      0.44         2
                 pycaret       0.29      1.00      0.44         2
                 pyspark       0.29      1.00      0.44         2
                   redis       0.29      1.00      0.44         2
                requests       0.29      1.00      0.44         2
            scikit-image       0.00      0.00      0.00         0
            scikit-learn       0.29      1.00      0.44         2
                   scipy       0.80      0.91      0.85        22
                   scoop       0.29      1.00      0.44         2
              simplejson       0.29      1.00      0.44         2
                     six       0.29      1.00      0.44         2
             statsmodels       0.29      1.00      0.44         2
                  triton       0.15      0.10      0.12        20
           triton==2.0.0       0.18      1.00      0.31         2
                   wheel       0.64      0.47      0.55        19

               micro avg       0.48      0.59      0.53       271
               macro avg       0.37      0.79      0.44       271
            weighted avg       0.63      0.59      0.55       271
             samples avg       0.85      0.70      0.73       271

