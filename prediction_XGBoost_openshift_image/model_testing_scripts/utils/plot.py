import matplotlib
# matplotlib.rcParams['pdf.fonttype'] = 42
# matplotlib.rcParams['ps.fonttype'] = 42	
matplotlib.rcParams['text.usetex'] = True
import matplotlib.pyplot as plt
import numpy as np
import math, copy
import scipy
# from pylab import *
from matplotlib import cm

def plotting(fig_path, filename, cates_values, labels, cates_stds= None, yaxis_label=None, xaxis_label=None, title=None, figsize=(10, 7)):
    width = 0.4

    fig, ax = plt.subplots(1, 1, figsize=figsize)

    for i, cate_values in enumerate(cates_values):
        bottom = np.zeros(len(labels))
        for cate, value in cate_values.items():
            if labels[0]==None:
                p = ax.bar([float(entry) for entry in cate.split("-")], value, width/len(cates_values), bottom=bottom)
            else:
                p = ax.bar([idx - width/len(cates_values)/2 + i*width/len(cates_values) for idx, _ in enumerate(value)], value, width/len(cates_values), label=cate, bottom=bottom)
            bottom += value
            ax.bar_label(p)
    # if cates_stds!=None:
    #     for i, cate_stds in enumerate(cates_stds):
    #         for cate, (y, yerr) in cate_stds.items():
    #             if labels[0]==None:
    #                 p = ax.bar([float(entry) for entry in cate.split("-")], y, yerr)
    #             else:
    #                 p = ax.bar([idx - width/len(cates_stds)/2 + i*width/len(cates_stds) for idx, _ in enumerate(y)], y, yerr)

    if title == None:
        ax.set_title(" ".join(filename.split("_")))
    else:
        ax.set_title(title, fontsize=20)
    if labels[0]!=None:
        ax.legend(loc="best", prop={'size': 16})
        ax.set_xticks(list(range(len(labels))))
        ax.set_xticklabels(labels)
    if yaxis_label != None:
        ax.set_ylabel(yaxis_label, fontsize=20)
    if xaxis_label != None:
        ax.set_xlabel(xaxis_label, fontsize=20)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)

    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()



if __name__ == "__main__":
    # # ###################### Cross-Validated Plots ##############################
#     # # # ###################### data0 ############################################


    fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/figs-1/'

#     filename = "cvf1scores_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     xs=[10, 20, 40, 80]
#     labels = ("F1 Scores Cross Validated with Submodel's Samples", )
#     ys_d0trainingtimerawinput_l=[#models
#         [#dims
#             [
#                 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.991, 1.0, 0.989, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.991, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.989, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.989, 1.0, 1.0, 0.991, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
#             ]
#         ],    
#         [
#             [
#                 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995, 0.994, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
#             ]
#         ],
#         [
#             [
#                 0.998, 1.0, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998, 1.0, 1.0, 1.0, 0.997, 1.0, 1.0, 1.0, 1.0, 0.998, 1.0, 1.0, 1.0, 0.997, 0.998, 1.0, 1.0, 1.0, 1.0
#             ]
#         ],
#         [
#             [
#                 0.999, 1.0, 0.997, 1.0, 1.0, 0.999, 1.0, 0.997, 1.0, 1.0, 0.999, 1.0, 0.997, 1.0, 1.0
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0trainingtimerawinput_l)):
#         ys_array.append(np.mean(ys_d0trainingtimerawinput_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0trainingtimerawinput_l[m_i], axis=1)/np.sqrt(len(ys_d0trainingtimerawinput_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v", "o", "s"]):
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='^', capsize=10)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
#     ax.set_ylabel("F1 Score", fontsize=20)
#     ax.set_ylim(0.99,1)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()




#     filename = "trainlatency_by_N_models_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     xs=[10, 20, 40, 80]
#     labels = ("Total Training Time (s) for Submodels", )
#     ys_d0trainingtimerawinput_l=[#models
#         [#dims
#             [
#                 [0.644, 0.42, 0.684, 0.819, 0.579, 0.884, 2.204, 10.147], [0.608, 0.352, 0.643, 0.934, 0.537, 0.88, 2.175, 9.785], [0.59, 0.39, 0.642, 0.951, 0.568, 0.888, 2.239, 10.321], [0.599, 0.363, 0.61, 0.942, 0.548, 0.922, 2.208, 10.429], [0.602, 0.372, 0.702, 0.996, 0.593, 0.896, 2.285, 9.878], [0.645, 0.602, 1.471, 0.992, 0.862, 9.166, 0.924, 1.007], [0.697, 0.602, 1.467, 0.994, 0.856, 9.146, 0.927, 1.008], [0.646, 0.59, 1.512, 1.001, 0.863, 9.716, 0.908, 1.025], [0.671, 0.599, 1.489, 0.997, 0.834, 9.973, 0.934, 1.02], [0.653, 0.592, 1.472, 0.994, 0.831, 9.378, 0.924, 0.999], [0.982, 0.494, 1.105, 0.588, 9.233, 0.6, 2.087, 0.677], [1.005, 0.686, 1.129, 0.629, 9.133, 0.602, 2.099, 0.699], [1.004, 0.678, 1.117, 0.606, 9.645, 0.589, 2.105, 0.641], [0.995, 0.651, 1.125, 0.591, 9.587, 0.612, 2.146, 0.688], [0.998, 0.668, 1.11, 0.594, 9.154, 0.6, 2.144, 0.678]
#             ]
#         ],    
#         [
#             [
#                 [2.964, 4.513, 4.406, 42.058], [2.976, 5.196, 4.485, 41.457], [3.009, 5.131, 4.45, 43.875], [2.96, 5.133, 4.39, 42.997], [2.962, 5.129, 4.429, 41.768], [3.69, 8.73, 34.414, 6.352], [3.836, 8.754, 33.727, 6.66], [3.728, 8.796, 35.177, 6.477], [3.681, 8.869, 34.891, 6.395], [3.695, 8.788, 33.353, 6.442], [4.675, 5.429, 33.301, 9.791], [4.783, 5.519, 32.189, 9.737], [4.72, 5.523, 33.118, 9.68], [4.691, 5.444, 34.244, 9.682], [4.71, 5.439, 32.726, 9.706]
#             ]
#         ],
#         [
#             [
#                 [27.973, 163.495], [30.364, 161.446], [30.438, 166.837], [30.318, 167.927], [30.373, 162.891], [46.088, 146.317], [46.361, 144.481], [47.09, 150.83], [46.448, 151.901], [46.562, 146.347], [38.591, 155.273], [38.768, 151.506], [39.134, 157.192], [38.506, 156.736], [38.393, 152.852]
#             ]
#         ],
#         [
#             [
#                 [723.563], [715.752], [735.693], [734.749], [717.41], [717.779], [717.89], [735.082], [734.218], [718.756], [720.906], [707.247], [733.122], [735.112], [717.266]
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0trainingtimerawinput_l)):
#         ys_array.append(np.mean(np.sum(ys_d0trainingtimerawinput_l[m_i],axis=(2,)), axis=(1)))
#         ci_array.append(1.96 * np.std(np.sum(ys_d0trainingtimerawinput_l[m_i],axis=(2,)), axis=1)/np.sqrt(len(np.sum(ys_d0trainingtimerawinput_l[m_i],axis=(2,))[0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v", "o", "s"]):
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='^', capsize=10)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
#     ax.set_ylabel("Training Time(s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/figs-1/'
#     filename = "trainlatency_by_input_size_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[50, 100, 250, 6832, 18000, 27329, 60000, 109319]
#     labels = ("10 labels per Model", "20 labels per Model","40 labels per Model","80 labels per Model")
#     ysestimate_l = [[871.2246/109319*50, 871.2246/109319*100, 871.2246/109319*250, 871.2246/16, 871.2246/109319*18000, 871.2246/4, 871.2246/109319*60000, 871.2246]]
#     ax.scatter(xs, ysestimate_l, label="Estimated with 80 Labels per Model \& 109319 dims", marker="x", s=100)
#     ys_d0trainingtime_l=[#models
#         [#dims
#             [
#                 0.1, 0.109, 0.144, 0.128, 0.117, 0.135, 0.133, 0.12, 0.134, 0.153, 0.128, 0.137, 0.131, 0.118, 0.091, 0.1, 0.102, 0.149, 0.136, 0.125, 0.121, 0.13, 0.139, 0.113, 0.142, 0.146, 0.127, 0.14, 0.129, 0.106, 0.132, 0.112, 0.134, 0.14, 0.131, 0.145, 0.118, 0.155, 0.147, 0.152, 0.134, 0.137, 0.127, 0.119, 0.106, 0.139, 0.102, 0.146, 0.125, 0.123, 0.145, 0.138, 0.143, 0.159, 0.145, 0.151, 0.126, 0.129, 0.128, 0.106, 0.128, 0.113, 0.146, 0.129, 0.133, 0.123, 0.119, 0.151, 0.122, 0.131, 0.129, 0.143, 0.148, 0.146, 0.119, 0.131, 0.117, 0.128, 0.139, 0.141, 0.145, 0.143, 0.151, 0.139, 0.155, 0.128, 0.15, 0.129, 0.168, 0.134, 0.154, 0.135, 0.155, 0.149, 0.138, 0.151, 0.138, 0.127, 0.116, 0.144, 0.132, 0.14, 0.155, 0.122, 0.133, 0.057, 0.054, 0.054, 0.053, 0.053, 0.052, 0.062, 0.057, 0.051, 0.054, 0.057, 0.059, 0.059, 0.055, 0.054
#             ],
#             [
#                 0.102, 0.151, 0.14, 0.137, 0.148, 0.145, 0.142, 0.165, 0.169, 0.153, 0.145, 0.149, 0.154, 0.135, 0.136, 0.132, 0.17, 0.122, 0.135, 0.137, 0.138, 0.137, 0.138, 0.151, 0.146, 0.184, 0.15, 0.181, 0.153, 0.138, 0.124, 0.163, 0.136, 0.131, 0.149, 0.138, 0.14, 0.14, 0.129, 0.136, 0.158, 0.146, 0.163, 0.152, 0.133, 0.119, 0.131, 0.133, 0.136, 0.143, 0.14, 0.132, 0.144, 0.156, 0.133, 0.143, 0.169, 0.163, 0.145, 0.141, 0.137, 0.15, 0.119, 0.144, 0.143, 0.128, 0.132, 0.163, 0.155, 0.145, 0.16, 0.152, 0.152, 0.154, 0.142, 0.145, 0.131, 0.148, 0.131, 0.154, 0.151, 0.148, 0.162, 0.14, 0.136, 0.174, 0.144, 0.155, 0.167, 0.143, 0.146, 0.146, 0.177, 0.156, 0.16, 0.14, 0.159, 0.171, 0.137, 0.148, 0.165, 0.141, 0.154, 0.12, 0.112, 0.058, 0.051, 0.068, 0.073, 0.06, 0.054, 0.051, 0.058, 0.061, 0.057, 0.052, 0.061, 0.051, 0.056, 0.058
#             ],
#             [
#                 0.183, 0.19, 0.207, 0.183, 0.18, 0.224, 0.138, 0.197, 0.183, 0.184, 0.232, 0.179, 0.197, 0.19, 0.174, 0.186, 0.14, 0.184, 0.205, 0.198, 0.208, 0.132, 0.242, 0.194, 0.193, 0.214, 0.172, 0.187, 0.192, 0.176, 0.188, 0.151, 0.23, 0.213, 0.198, 0.249, 0.135, 0.199, 0.205, 0.181, 0.207, 0.176, 0.237, 0.179, 0.176, 0.181, 0.138, 0.205, 0.192, 0.204, 0.241, 0.142, 0.194, 0.157, 0.182, 0.242, 0.177, 0.191, 0.221, 0.193, 0.191, 0.139, 0.192, 0.184, 0.173, 0.256, 0.137, 0.199, 0.153, 0.173, 0.212, 0.184, 0.221, 0.215, 0.184, 0.232, 0.141, 0.186, 0.194, 0.187, 0.261, 0.144, 0.191, 0.164, 0.18, 0.204, 0.177, 0.214, 0.2, 0.175, 0.248, 0.147, 0.215, 0.216, 0.196, 0.257, 0.167, 0.199, 0.157, 0.198, 0.214, 0.186, 0.208, 0.221, 0.19, 0.081, 0.052, 0.087, 0.072, 0.07, 0.074, 0.062, 0.062, 0.061, 0.063, 0.072, 0.069, 0.069, 0.069, 0.067
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 1.098, 1.086, 1.048, 1.102, 1.095, 1.058, 1.076, 1.037, 1.026, 1.063, 1.116, 1.11, 1.108, 1.094, 1.118, 1.091, 1.078, 1.057, 1.095, 1.088, 1.105, 1.086, 1.016, 1.023, 1.081, 1.082, 1.12, 1.092, 1.082, 1.098, 1.073, 1.07, 1.075, 1.104, 1.094, 1.081, 1.103, 1.037, 1.029, 1.081, 1.073, 1.101, 1.109, 1.094, 1.077, 1.093, 1.087, 1.06, 1.078, 1.082, 1.081, 1.085, 1.053, 1.036, 1.078, 1.063, 1.069, 1.093, 1.129, 1.064, 1.065, 1.065, 1.059, 1.025, 1.079, 1.065, 1.09, 1.044, 1.047, 1.074, 1.099, 1.108, 1.135, 1.127, 1.096, 1.07, 1.064, 1.067, 1.042, 1.109, 1.117, 1.085, 1.044, 1.075, 1.111, 1.068, 1.067, 1.079, 1.093, 1.078, 1.082, 1.07, 1.064, 1.051, 1.092, 1.082, 1.056, 1.026, 1.086, 1.098, 1.086, 1.079, 1.094, 1.091, 1.085, 1.094, 1.107, 1.057, 1.072, 1.11, 1.086, 1.044, 1.111, 1.107, 1.104, 1.091, 1.058, 1.083, 1.075, 1.093
#             ],
#             [
#                 3.322, 3.277, 3.265, 3.272, 3.351, 3.262, 3.2, 3.202, 3.215, 3.217, 3.262, 3.211, 3.177, 3.229, 3.335, 3.217, 3.224, 3.194, 3.061, 3.223, 3.16, 3.173, 3.192, 3.209, 3.198, 3.229, 3.223, 3.169, 3.21, 3.248, 3.213, 3.218, 3.228, 3.157, 3.265, 3.142, 3.19, 3.234, 3.211, 3.211, 3.263, 3.195, 3.286, 3.191, 3.242, 3.216, 3.194, 3.276, 3.188, 3.206, 3.184, 3.182, 3.162, 3.164, 3.177, 3.244, 3.232, 3.273, 3.224, 3.25, 3.209, 3.228, 3.237, 3.164, 3.204, 3.169, 3.224, 3.176, 3.204, 3.205, 3.285, 3.268, 3.249, 3.277, 3.261, 3.25, 3.25, 3.217, 3.167, 3.217, 3.276, 3.29, 3.229, 3.229, 3.221, 3.251, 3.231, 3.163, 3.645, 3.271, 3.194, 3.233, 3.233, 3.6, 3.186, 3.208, 3.238, 3.188, 3.234, 3.184, 3.287, 3.166, 3.144, 3.622, 3.226, 0.416, 0.422, 0.427, 0.405, 0.41, 0.418, 0.406, 0.409, 0.439, 0.418, 0.41, 0.412, 0.391, 0.433, 0.415
#             ],
#             [
#                 4.381, 4.367, 4.521, 4.423, 4.33, 4.516, 4.474, 4.315, 4.245, 4.286, 4.416, 4.444, 4.361, 4.561, 4.353, 4.407, 4.598, 4.424, 4.3, 4.353, 4.52, 4.22, 4.187, 4.249, 4.313, 4.373, 4.427, 4.38, 4.325, 4.303, 4.483, 4.392, 4.42, 4.267, 4.464, 4.499, 4.521, 4.151, 4.514, 4.327, 4.337, 4.019, 4.511, 4.167, 4.308, 4.319, 4.41, 4.421, 4.241, 4.371, 4.414, 4.15, 4.493, 4.266, 4.238, 4.452, 4.374, 4.365, 4.224, 4.33, 4.408, 4.215, 4.436, 4.429, 4.237, 4.357, 4.198, 4.416, 4.43, 4.401, 4.604, 4.088, 4.41, 4.28, 4.411, 4.373, 4.285, 4.482, 4.482, 4.264, 4.416, 4.313, 4.425, 4.269, 4.326, 4.36, 4.369, 4.376, 4.516, 4.321, 4.385, 4.524, 4.399, 4.337, 4.303, 4.359, 4.4, 4.455, 4.265, 4.358, 4.517, 4.43, 4.395, 4.21, 4.273, 4.405, 4.572, 4.527, 4.497, 4.318, 4.321, 4.287, 4.353, 4.294, 4.291, 4.51, 4.187, 4.329, 4.314, 4.279
#             ],
#             [
#                 12.328, 12.991, 11.46, 14.434, 12.84, 13.183, 13.013, 13.209, 12.989, 12.975, 12.995, 12.696, 13.242, 12.933, 12.978, 12.433, 13.196, 13.302, 14.369, 12.823, 13.071, 12.995, 13.309, 12.866, 13.182, 13.019, 12.996, 13.11, 13.068, 12.968, 13.2, 13.351, 13.184, 14.417, 12.773, 12.978, 12.774, 13.105, 12.921, 12.991, 13.091, 13.03, 13.05, 13.06, 12.838, 12.548, 13.165, 13.217, 14.294, 12.75, 12.966, 12.854, 13.11, 12.907, 13.003, 12.876, 12.938, 13.238, 13.134, 12.286, 13.049, 13.29, 13.154, 13.153, 12.732, 13.01, 12.945, 13.062, 12.995, 11.827, 12.955, 13.011, 13.153, 13.059, 12.941, 12.203, 13.314, 13.232, 13.104, 12.752, 12.164, 12.826, 12.98, 12.864, 12.998, 13.031, 13.019, 13.204, 12.945, 12.782, 12.048, 12.997, 13.15, 12.945, 12.763, 13.113, 12.997, 13.213, 13.034, 12.866, 12.794, 12.887, 13.01, 13.026, 12.754, 1.225, 1.339, 1.322, 1.483, 1.405, 1.305, 1.427, 1.317, 1.399, 1.407, 1.373, 1.359, 1.369, 1.358, 1.37
#             ],
#             [
#                 22.287, 22.609, 22.545, 21.656, 22.583, 22.578, 22.789, 22.74, 21.392, 21.733, 22.587, 22.71, 21.847, 21.572, 21.221, 22.348, 22.052, 22.307, 22.623, 22.193, 22.649, 22.643, 21.834, 22.552, 22.188, 21.76, 22.712, 21.526, 21.497, 21.5, 22.519, 22.628, 21.685, 21.452, 22.292, 22.718, 22.692, 22.275, 22.03, 21.346, 22.281, 22.715, 22.075, 22.749, 21.309, 22.656, 22.721, 22.057, 22.533, 22.27, 22.499, 21.877, 21.322, 20.577, 20.359, 21.665, 22.854, 22.067, 22.019, 22.051, 22.59, 21.811, 21.575, 20.406, 21.776, 22.45, 22.697, 22.534, 21.518, 20.957, 22.237, 22.678, 21.846, 21.562, 21.035, 22.422, 22.891, 22.076, 21.524, 21.515, 22.554, 22.67, 22.043, 22.488, 21.492, 22.345, 22.68, 21.991, 22.396, 21.497, 22.086, 22.858, 21.342, 21.059, 21.295, 22.289, 22.372, 21.58, 21.618, 22.647, 21.59, 22.938, 22.156, 20.842, 21.202, 22.704, 22.788, 21.741, 21.591, 21.453, 20.515, 22.591, 21.696, 21.919, 21.298, 22.478, 21.906, 21.355, 21.791, 21.085
#             ],
#         ],    
#         [
#             [
#                 0.278, 0.262, 0.269, 0.262, 0.264, 0.256, 0.258, 0.28, 0.255, 0.23, 0.204, 0.26, 0.279, 0.279, 0.307, 0.261, 0.285, 0.277, 0.273, 0.249, 0.284, 0.262, 0.296, 0.251, 0.291, 0.229, 0.25, 0.295, 0.291, 0.302, 0.274, 0.257, 0.273, 0.264, 0.233, 0.299, 0.258, 0.302, 0.257, 0.306, 0.241, 0.278, 0.292, 0.309, 0.312, 0.207, 0.216, 0.218, 0.207, 0.207, 0.19, 0.211, 0.249, 0.237, 0.254, 0.173, 0.227, 0.18, 0.254, 0.26
#             ],
#             [
#                 0.241, 0.318, 0.227, 0.354, 0.322, 0.3, 0.322, 0.276, 0.286, 0.322, 0.308, 0.345, 0.268, 0.321, 0.247, 0.242, 0.319, 0.231, 0.303, 0.342, 0.296, 0.328, 0.268, 0.287, 0.318, 0.325, 0.301, 0.278, 0.302, 0.238, 0.244, 0.322, 0.226, 0.283, 0.327, 0.31, 0.367, 0.262, 0.305, 0.336, 0.336, 0.305, 0.3, 0.319, 0.238, 0.19, 0.295, 0.215, 0.234, 0.273, 0.244, 0.274, 0.229, 0.25, 0.291, 0.263, 0.23, 0.225, 0.244, 0.182
#             ],
#             [
#                 0.435, 0.377, 0.394, 0.411, 0.314, 0.401, 0.402, 0.394, 0.419, 0.391, 0.397, 0.427, 0.388, 0.405, 0.418, 0.416, 0.388, 0.417, 0.41, 0.32, 0.424, 0.408, 0.394, 0.436, 0.417, 0.407, 0.468, 0.392, 0.395, 0.426, 0.393, 0.414, 0.396, 0.416, 0.39, 0.399, 0.417, 0.39, 0.396, 0.458, 0.417, 0.419, 0.401, 0.396, 0.428, 0.31, 0.317, 0.304, 0.361, 0.304, 0.318, 0.338, 0.305, 0.319, 0.335, 0.308, 0.331, 0.306, 0.314, 0.339
#             ],
#             [
#                 3.706, 3.746, 3.699, 3.695, 3.675, 3.66, 3.718, 3.761, 3.678, 3.658, 3.711, 3.72, 3.716, 3.695, 3.692, 3.701, 3.707, 3.701, 3.691, 3.726, 3.698, 3.72, 3.679, 3.668, 3.804, 3.708, 3.708, 3.696, 3.689, 3.715, 3.684, 3.625, 3.738, 3.655, 3.692, 3.752, 3.765, 3.732, 3.776, 3.819, 3.718, 3.774, 3.764, 3.731, 3.737, 3.733, 3.841, 3.752, 3.714, 3.748, 3.762, 3.733, 3.726, 3.733, 3.722, 3.685, 3.71, 3.684, 3.702, 3.694
#             ],
#             [
#                 10.681, 10.697, 10.735, 10.787, 10.707, 10.659, 10.728, 10.828, 10.685, 10.761, 10.709, 10.709, 10.665, 10.717, 10.704, 10.701, 10.725, 10.653, 10.712, 10.711, 10.663, 10.676, 10.778, 10.641, 10.699, 10.66, 10.737, 10.654, 10.643, 10.694, 10.653, 10.712, 10.607, 10.656, 10.64, 10.669, 10.833, 10.753, 10.744, 10.731, 10.722, 10.722, 10.779, 10.736, 10.754, 7.556, 7.64, 7.649, 7.551, 7.46, 7.288, 7.544, 7.545, 7.469, 7.394, 7.485, 7.556, 7.602, 7.528, 7.464
#             ],
#             [
#                 15.431, 15.503, 15.552, 15.518, 15.486, 15.468, 15.425, 15.371, 15.472, 15.34, 15.368, 15.517, 15.4, 15.569, 15.426, 15.439, 15.509, 15.408, 15.486, 15.523, 15.433, 15.257, 15.302, 15.333, 15.345, 15.451, 15.588, 15.432, 15.495, 15.484, 15.534, 15.36, 15.336, 15.25, 15.182, 15.405, 15.486, 15.605, 15.574, 15.463, 15.605, 15.461, 15.495, 15.505, 15.555, 15.567, 15.632, 15.557, 15.579, 15.725, 15.439, 15.519, 15.48, 15.578, 15.489, 15.547, 15.301, 15.214, 15.316, 15.263
#             ],
#             [
#                 38.064, 38.289, 37.597, 37.867, 38.093, 37.317, 38.038, 38.744, 38.504, 37.095, 38.463, 38.711, 37.436, 37.423, 37.959, 37.628, 37.838, 37.66, 37.71, 36.914, 37.806, 37.842, 38.801, 37.739, 37.784, 37.355, 37.574, 38.613, 37.367, 37.347, 38.068, 38.82, 38.575, 38.385, 37.296, 37.016, 37.411, 37.682, 37.319, 36.89, 37.992, 38.04, 37.488, 37.236, 37.169, 26.113, 27.08, 26.782, 27.336, 26.349, 26.962, 27.049, 27.399, 26.406, 27.187, 26.678, 26.255, 27.518, 26.071, 26.485
#             ],
#             [
#                 65.688, 66.429, 68.4, 67.286, 66.539, 64.662, 67.585, 67.363, 65.951, 67.635, 65.223, 66.51, 67.114, 68.04, 67.176, 65.696, 67.452, 67.966, 65.959, 65.335, 66.404, 66.166, 65.976, 65.594, 64.85, 67.197, 66.784, 68.341, 66.761, 65.138, 64.764, 64.905, 66.763, 66.141, 65.605, 65.344, 65.555, 67.748, 66.236, 67.231, 65.167, 66.383, 67.653, 66.864, 66.543, 67.613, 67.647, 67.923, 66.89, 66.579, 67.038, 66.098, 67.973, 67.01, 65.433, 65.915, 66.157, 66.999, 66.24, 65.681
#             ],
#         ],
#         [
#             [
#                 0.521, 0.588, 0.606, 0.586, 0.57, 0.61, 0.58, 0.439, 0.606, 0.576, 0.54, 0.589, 0.451, 0.641, 0.559, 0.559, 0.556, 0.398, 0.566, 0.524, 0.52, 0.546, 0.402, 0.561, 0.518, 0.491, 0.518, 0.567, 0.61, 0.527
#             ],
#             [
#                 0.663, 0.688, 0.553, 0.75, 0.665, 0.68, 0.686, 0.629, 0.705, 0.68, 0.656, 0.713, 0.627, 0.753, 0.717, 0.646, 0.687, 0.587, 0.717, 0.633, 0.676, 0.687, 0.615, 0.691, 0.632, 0.636, 0.682, 0.653, 0.712, 0.639
#             ],
#             [
#                 1.012, 1.071, 1.091, 1.116, 1.025, 1.048, 1.094, 1.042, 0.862, 1.034, 1.053, 1.021, 1.062, 0.849, 1.075, 0.971, 1.02, 0.937, 0.744, 0.921, 0.965, 1.035, 0.95, 0.782, 0.972, 0.985, 1.002, 0.93, 0.785, 0.965
#             ],
#             [
#                 14.017, 14.461, 14.391, 14.14, 14.164, 14.118, 14.199, 14.312, 14.129, 14.216, 15.083, 14.332, 14.358, 14.17, 14.203, 14.257, 14.307, 14.395, 14.212, 14.253, 14.327, 14.42, 14.452, 14.255, 14.357, 14.17, 14.428, 14.417, 14.241, 14.194
#             ],
#             [
#                 38.89, 39.249, 39.334, 38.97, 38.745, 39.204, 39.292, 39.356, 39.666, 39.412, 38.772, 39.143, 39.098, 39.412, 39.146, 35.409, 35.77, 36.136, 36.036, 35.316, 35.558, 35.584, 35.756, 35.698, 35.737, 35.398, 35.425, 35.962, 35.96, 35.503
#             ],
#             [
#                 58.895, 57.412, 58.035, 57.572, 57.647, 57.849, 57.396, 57.187, 57.753, 56.694, 57.472, 57.216, 57.993, 57.283, 56.706, 58.057, 57.689, 58.633, 57.4, 57.53, 56.905, 57.717, 57.526, 57.786, 57.741, 57.034, 57.719, 57.535, 57.024, 57.365
#             ],
#             [
#                 132.087, 132.367, 134.63, 133.618, 132.269, 132.156, 133.94, 135.68, 134.128, 131.84, 131.681, 132.241, 134.689, 132.37, 131.669, 120.242, 120.961, 124.395, 120.443, 120.314, 120.005, 119.831, 122.211, 120.66, 118.987, 120.506, 120.252, 121.239, 120.403, 120.651
#             ],
#             [
#                 232.282, 232.321, 232.542, 233.496, 231.954, 232.797, 233.229, 231.905, 230.397, 230.282, 231.831, 233.269, 233.291, 232.885, 230.353, 231.224, 231.556, 231.963, 230.481, 231.681, 230.094, 233.015, 234.409, 235.052, 231.671, 232.82, 230.776, 232.444, 230.225, 230.629
#             ],
#         ],
#         [
#             [
#                 1.116, 1.099, 1.145, 1.106, 1.267, 1.035, 1.638, 1.594, 1.302, 1.547, 1.057, 1.535, 1.639, 1.318, 1.614
#             ],
#             [
#                 1.628, 1.753, 1.944, 1.746, 1.935, 1.61, 1.785, 1.833, 1.851, 1.971, 1.622, 1.766, 1.76, 1.751, 1.917
#             ],
#             [
#                 2.998, 3.028, 3.315, 3.116, 3.303, 2.957, 3.066, 3.207, 3.238, 3.203, 2.904, 3.05, 3.256, 3.208, 3.187
#             ],
#             [
#                 53.933, 54.448, 54.158, 53.984, 54.035, 53.914, 54.299, 54.129, 54.109, 54.31, 53.883, 54.273, 54.086, 54.198, 54.073
#             ],
#             [
#                 145.263, 145.34, 146.337, 144.229, 144.529, 144.439, 145.235, 146.048, 143.908, 145.084, 145.488, 145.221, 146.02, 144.477, 144.498
#             ],
#             [
#                 219.871, 221.598, 220.384, 221.146, 220.838, 219.945, 222.077, 220.636, 221.284, 221.206, 219.656, 220.707, 220.454, 221.921, 219.983
#             ],
#             [
#                 478.666, 484.638, 483.839, 481.316, 480.119, 482.076, 480.894, 484.2, 479.742, 479.454, 478.566, 479.542, 484.842, 476.382, 474.346
#             ],
#             [
#                 872.889, 874.202, 846.84, 873.451, 871.592, 864.231, 887.441, 874.245, 872.806, 871.109, 865.572, 871.679, 876.482, 873.382, 872.448
#             ],
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0trainingtime_l)):
#         ys_array.append(np.mean(ys_d0trainingtime_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0trainingtime_l[m_i], axis=1)/np.sqrt(len(ys_d0trainingtime_l[m_i][0])))
#     for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v", "o", "s"]):
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Input Dimensions", fontsize=20)
#     ax.set_ylabel("Training Time(s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()


#     # # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/figs-1/'
#     filename = "trainlatency_by_input_size_with_rawinput_data_0_estimated"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[50, 100, 250, 6832, 18000, 27329, 60000, 109319]
#     labels = ("Observed", "Estimated")
#     ysestimate_l = [[871.2246/109319*50, 871.2246/109319*100, 871.2246/109319*250, 871.2246/16, 871.2246/109319*18000, 871.2246/4, 871.2246/109319*60000, 871.2246]]
#     ys_array = np.mean(ys_d0trainingtime_l[3], axis=(1))
#     ci_array = 1.96 * np.std(ys_d0trainingtime_l[3], axis=1)/np.sqrt(len(ys_d0trainingtime_l[3][0]))
#     ax.scatter(xs, ys_array, label=labels[0], marker="^")
#     ax.errorbar(xs, ys_array, yerr=ci_array, fmt='o', capsize=10)
#     ax.scatter(xs, ysestimate_l, label=labels[1], marker="*")
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Input Dimensions", fontsize=20)
#     ax.set_ylabel("Training Time(s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     filename = "trainlatency_by_labels_per_model_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[10, 20, 40, 80]
#     labels = ("50 dims", "100 dims", "250 dims", "6832 dims", "18000 dims", "27329 dims", "60000 dims", "109319 dims")
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0trainingtime_l)):
#         ys_array.append(np.mean(ys_d0trainingtime_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0trainingtime_l[m_i], axis=1)/np.sqrt(len(ys_d0trainingtime_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label in zip(ys_array[[3,5,7],:], ci_array[[3,5,7],:], np.array(labels)[[3,5,7]]):
#         ax.scatter(xs, ys, label=label)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10)
#     ysestimate_l = [[871.2246/(8*((1598.4+1598.4*math.log(1598.4))/(199.8+199.8*math.log(199.8)))), 871.2246/(4*((1598.4+1598.4*math.log(1598.4))/(399.6+399.6*math.log(399.6)))), 871.2246/(2*((1598.4+1598.4*math.log(1598.4))/(799.2+799.2*math.log(799.2)))), 871.2246]]
#     ysestimatenosorting_l = [[871.2246/(8*(1598.4/199.8)), 871.2246/(4*(1598.4/399.6)), 871.2246/(2*(1598.4/799.2)), 871.2246]]
#     ax.scatter(xs, ysestimate_l, marker="x", s=100, label="Estimated with 109319 dims \& 80 packages per model")
#     ax.scatter(xs, ysestimatenosorting_l, marker="+", s=100, label="Estimated with Linear Sorting Time")
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     ax.set_ylabel("Training Time(s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     # # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/figs-1/'
#     filename = "trainlatency_by_labels_per_model_with_rawinput_data_0_estimated"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[10, 20, 40, 80]
#     labels = ("Observed", "Estimated", "Estimated_Linear_Sorting")
#     ysestimate_l = [[871.2246/(8*((1598.4+1598.4*math.log(1598.4))/(199.8+199.8*math.log(199.8)))), 871.2246/(4*((1598.4+1598.4*math.log(1598.4))/(399.6+399.6*math.log(399.6)))), 871.2246/(2*((1598.4+1598.4*math.log(1598.4))/(799.2+799.2*math.log(799.2)))), 871.2246]]
#     ysestimatenosorting_l = [[871.2246/(8*(1598.4/199.8)), 871.2246/(4*(1598.4/399.6)), 871.2246/(2*(1598.4/799.2)), 871.2246]]
#     ys_array = ys_array[-1,:]
#     ci_array = ci_array[-1,:]
#     ax.scatter(xs, ys_array, label=labels[0])
#     ax.errorbar(xs, ys_array, yerr=ci_array, fmt='o', capsize=10)
#     ax.scatter(xs, ysestimate_l, label=labels[1])
#     ax.scatter(xs, ysestimatenosorting_l, label=labels[2])
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     ax.set_ylabel("Training Time(s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()








#     # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
#     filename = "testf1score_by_input_size_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[50, 100, 250, 6832, 18000, 27329, 60000, 109319]
#     labels = ("10 labels", "20 labels","40 labels","80 labels")
#     ys_d0f1score_l=[#models
#         [#dims
#             [
#                 0.363, 0.407, 0.42, 0.39, 0.43, 0.412, 0.391, 0.42, 0.362, 0.406, 0.378, 0.395, 0.411, 0.372, 0.405
#             ],
#             [
#                 0.525, 0.542, 0.541, 0.486, 0.548, 0.538, 0.536, 0.506, 0.47, 0.558, 0.492, 0.532, 0.495, 0.504, 0.534
#             ],
#             [
#                 0.67, 0.715, 0.716, 0.679, 0.71, 0.71, 0.644, 0.696, 0.721, 0.671, 0.654, 0.696, 0.669, 0.702, 0.666
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.825, 0.81, 0.836, 0.849, 0.866, 0.88, 0.852, 0.88, 0.836, 0.914, 0.863, 0.814, 0.855, 0.856, 0.821
#             ],
#             [
#                 0.866, 0.851, 0.818, 0.859, 0.838, 0.863, 0.861, 0.86, 0.843, 0.856, 0.892, 0.871, 0.842, 0.875, 0.873
#             ],
#             [
#                 0.845, 0.807, 0.843, 0.867, 0.869, 0.877, 0.862, 0.874, 0.864, 0.902, 0.863, 0.809, 0.865, 0.859, 0.827
#             ],
#             [
#                 0.873, 0.863, 0.829, 0.856, 0.846, 0.87, 0.854, 0.863, 0.855, 0.857, 0.904, 0.869, 0.85, 0.862, 0.873
#             ],
#             [
#                 0.847, 0.82, 0.849, 0.867, 0.867, 0.87, 0.867, 0.893, 0.872, 0.902, 0.861, 0.816, 0.872, 0.858, 0.821
#             ],
#         ],    
#         [
#             [
#                 0.475, 0.447, 0.427, 0.482, 0.452, 0.42, 0.444, 0.439, 0.465, 0.478, 0.446, 0.446, 0.438, 0.468, 0.435
#             ],
#             [
#                 0.559, 0.597, 0.577, 0.616, 0.576, 0.544, 0.557, 0.601, 0.59, 0.578, 0.55, 0.557, 0.599, 0.57, 0.547
#             ],
#             [
#                 0.679, 0.68, 0.706, 0.715, 0.682, 0.734, 0.722, 0.692, 0.746, 0.727, 0.698, 0.722, 0.693, 0.685, 0.708
#             ],
#             [
#                 0.886, 0.92, 0.865, 0.877, 0.871, 0.877, 0.882, 0.891, 0.905, 0.845, 0.893, 0.85, 0.876, 0.888, 0.876
#             ],
#             [
#                 0.897, 0.903, 0.896, 0.893, 0.891, 0.919, 0.936, 0.89, 0.899, 0.884, 0.907, 0.921, 0.929, 0.883, 0.891
#             ],
#             [
#                 0.89, 0.892, 0.903, 0.877, 0.89, 0.902, 0.905, 0.889, 0.891, 0.879, 0.879, 0.901, 0.885, 0.907, 0.869
#             ],
#             [
#                 0.9, 0.895, 0.883, 0.877, 0.883, 0.912, 0.93, 0.913, 0.915, 0.901, 0.924, 0.917, 0.933, 0.88, 0.905
#             ],
#             [
#                 0.891, 0.897, 0.905, 0.893, 0.875, 0.888, 0.91, 0.881, 0.891, 0.889, 0.903, 0.909, 0.903, 0.894, 0.877
#             ],
#         ],
#         [
#             [
#                 0.429, 0.448, 0.44, 0.455, 0.432, 0.415, 0.418, 0.407, 0.419, 0.424, 0.405, 0.391, 0.415, 0.436, 0.428
#             ],
#             [
#                 0.594, 0.58, 0.585, 0.596, 0.589, 0.566, 0.557, 0.552, 0.565, 0.594, 0.525, 0.576, 0.601, 0.61, 0.578
#             ],
#             [
#                 0.708, 0.729, 0.695, 0.712, 0.712, 0.726, 0.72, 0.716, 0.705, 0.717, 0.691, 0.697, 0.717, 0.69, 0.701
#             ],
#             [
#                 0.897, 0.87, 0.896, 0.896, 0.905, 0.914, 0.932, 0.949, 0.913, 0.92, 0.915, 0.897, 0.896, 0.89, 0.903
#             ],
#             [
#                 0.926, 0.937, 0.917, 0.927, 0.912, 0.945, 0.915, 0.957, 0.947, 0.929, 0.945, 0.925, 0.899, 0.928, 0.941
#             ],
#             [
#                 0.905, 0.92, 0.924, 0.904, 0.905, 0.932, 0.934, 0.923, 0.956, 0.94, 0.925, 0.911, 0.926, 0.914, 0.923
#             ],
#             [
#                 0.923, 0.937, 0.944, 0.921, 0.924, 0.946, 0.935, 0.938, 0.948, 0.948, 0.932, 0.929, 0.931, 0.912, 0.917
#             ],
#             [
#                 0.915, 0.933, 0.916, 0.925, 0.912, 0.932, 0.932, 0.925, 0.944, 0.937, 0.942, 0.93, 0.918, 0.926, 0.907
#             ],
#         ],
#         [
#             [
#                 0.363, 0.365, 0.339, 0.317, 0.365, 0.363, 0.365, 0.339, 0.317, 0.365, 0.363, 0.365, 0.339, 0.317, 0.365
#             ],
#             [
#                 0.522, 0.527, 0.504, 0.48, 0.514, 0.522, 0.527, 0.504, 0.48, 0.514, 0.522, 0.527, 0.504, 0.48, 0.514
#             ],
#             [
#                 0.678, 0.688, 0.683, 0.662, 0.692, 0.678, 0.688, 0.683, 0.662, 0.692, 0.678, 0.688, 0.683, 0.662, 0.692
#             ],
#             [
#                 0.9, 0.908, 0.913, 0.91, 0.909, 0.9, 0.908, 0.913, 0.91, 0.909, 0.9, 0.908, 0.913, 0.91, 0.909
#             ],
#             [
#                 0.945, 0.943, 0.924, 0.929, 0.927, 0.945, 0.943, 0.924, 0.929, 0.927, 0.945, 0.943, 0.924, 0.929, 0.927
#             ],
#             [
#                 0.94, 0.937, 0.94, 0.942, 0.946, 0.94, 0.937, 0.94, 0.942, 0.946, 0.94, 0.937, 0.94, 0.942, 0.946
#             ],
#             [
#                 0.956, 0.937, 0.955, 0.949, 0.962, 0.956, 0.937, 0.955, 0.949, 0.962, 0.956, 0.937, 0.955, 0.949, 0.962
#             ],
#             [
#                 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955
#             ],
#         ]
#     ]
#     ys_array = np.mean(ys_d0f1score_l, axis=(2))
#     ci_array = 1.96 * np.std(ys_d0f1score_l, axis=2)/np.sqrt(len(ys_d0f1score_l[0][0]))
#     for ys, ci, label in zip(ys_array, ci_array, labels):
#         ax.scatter(xs, ys, label=label)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10)
#     ax.set_xlim([0, 300])
#     ax.hlines(ys_array[0,3], xmin=0, xmax=7000, label="10 labels \&\& 6832 dims")
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Input Dimensions", fontsize=20)
#     ax.set_ylabel("F1-Scores", fontsize=20)
#     ax.grid()
#     plt.legend(loc="lower right", prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     filename = "testf1score_by_labels_per_model_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[10, 20, 40, 80]
#     labels = ("50 dims", "100 dims", "250 dims", "6832 dims", "18000 dims", "27329 dims", "60000 dims", "109319 dims")
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0f1score_l)):
#         ys_array.append(np.mean(ys_d0f1score_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0f1score_l[m_i], axis=1)/np.sqrt(len(ys_d0f1score_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label in zip(ys_array[[3,5,7],:], ci_array[[3,5,7],:], np.array(labels)[[3,5,7]]):
#         ax.scatter(xs, ys, label=label)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     ax.set_ylabel("F1-Scores", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()








#     # # # ###################### data3 ############################################
#     filename = "testf1score_by_labels_per_model_with_rawinput_data_3"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     # xs=[20, 50, 500]
#     xs=[10, 20, 50]
#     labels = ("500labels-F1-with-filter", "500labels-F1-no-filter")
#     ys_d3nonef1score_l=[#models
#         [
#             [
#                 0.88, 0.882, 0.881, 0.884, 0.881, 0.886, 0.887, 0.868, 0.885, 0.881, 0.88, 0.877, 0.867, 0.891, 0.884
#             ],
#             [
#                 0.598, 0.614, 0.579, 0.595, 0.618, 0.582, 0.598, 0.567, 0.606, 0.628, 0.587, 0.59, 0.602, 0.622, 0.618
#             ]
#         ],
#         [#filter or not
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.862, 0.875, 0.885, 0.882, 0.878, 0.881, 0.875, 0.885, 0.891, 0.88, 0.873, 0.878, 0.881, 0.888, 0.886
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.65, 0.651, 0.615, 0.64, 0.634, 0.638, 0.643, 0.626, 0.654, 0.65, 0.64, 0.621, 0.636, 0.652, 0.631
#             ]
#         ],    
#         [
#             [
#                 0.874, 0.881, 0.883, 0.897, 0.887, 0.883, 0.876, 0.889, 0.889, 0.894, 0.878, 0.879, 0.882, 0.884, 0.891
#             ],
#             [
#                 0.69, 0.693, 0.679, 0.707, 0.685, 0.7, 0.698, 0.669, 0.718, 0.701, 0.704, 0.707, 0.683, 0.724, 0.697
#             ]
#         ],
#         # [
#         #     [
#         #         0.95, 0.952, 0.951, 0.951, 0.95, 0.95, 0.952, 0.951, 0.951, 0.95, 0.95, 0.952, 0.951, 0.951, 0.95
#         #     ],
#         #     [
#         #         0.915, 0.933, 0.916, 0.925, 0.912, 0.932, 0.932, 0.925, 0.944, 0.937, 0.942, 0.93, 0.918, 0.926, 0.907
#         #     ]
#         # ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d3nonef1score_l)):
#         ys_array.append(np.mean(ys_d3nonef1score_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d3nonef1score_l[m_i], axis=1)/np.sqrt(len(ys_d3nonef1score_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v"]):
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     xs=[10, 20, 40, 80]
#     labels = ("80labels-F1-no-filter",)
#     ys_d0nonef1score_l=[#models
#         [#dims
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.847, 0.82, 0.849, 0.867, 0.867, 0.87, 0.867, 0.893, 0.872, 0.902, 0.861, 0.816, 0.872, 0.858, 0.821
#             ]
#         ],    
#         [
#             [
#                 0.891, 0.897, 0.905, 0.893, 0.875, 0.888, 0.91, 0.881, 0.891, 0.889, 0.903, 0.909, 0.903, 0.894, 0.877
#             ]
#         ],
#         [
#             [
#                 0.915, 0.933, 0.916, 0.925, 0.912, 0.932, 0.932, 0.925, 0.944, 0.937, 0.942, 0.93, 0.918, 0.926, 0.907
#             ]
#         ],
#         [
#             [
#                 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0nonef1score_l)):
#         ys_array.append(np.mean(ys_d0nonef1score_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0nonef1score_l[m_i], axis=1)/np.sqrt(len(ys_d0nonef1score_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label in zip(ys_array, ci_array, labels):
#         ax.scatter(xs, ys, label=label, color='#ff0067', marker="*")
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, color='#ff0067', marker="*")
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     ax.set_ylim(0.3,1)
#     ax.set_ylabel("F1-Score", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()


# # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! "500labels-F1-no-filter": missing partial CV results
#     filename = "trainlatency_by_labels_per_model_with_rawinput_data_3"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     # xs=[20, 50, 500]
#     xs=[10, 20, 50, 100, 500]
#     labels = ("500labels-F1-with-filter", "500labels-F1-no-filter")
#     ys_d3nonetrainlat_l=[#models
#         [
#             [
#                 0.146, 0.139, 0.129, 0.139, 0.143, 0.209, 0.153, 0.208, 0.276, 0.234, 0.378, 0.325, 0.421, 0.401, 0.416, 0.215, 0.212, 0.173, 0.141, 0.206, 0.148, 0.122, 0.156, 0.175, 0.191, 0.175, 0.127, 0.184, 0.194, 0.187, 0.145, 0.123, 0.125, 0.106, 0.133, 0.149, 0.126, 0.14, 0.149, 0.199, 0.166, 0.122, 0.156, 0.181, 0.187, 0.17, 0.16, 0.15, 0.119, 0.152, 0.211, 0.158, 0.195, 0.188, 0.244, 0.138, 0.1, 0.149, 0.161, 0.152, 0.192, 0.202, 0.154, 0.133, 0.184, 0.21, 0.144, 0.226, 0.168, 0.206, 0.158, 0.163, 0.172, 0.188, 0.159, 0.183, 0.167, 0.169, 0.129, 0.175, 0.326, 0.193, 0.345, 0.238, 0.27, 0.165, 0.133, 0.168, 0.198, 0.158, 0.289, 0.285, 0.255, 0.215, 0.269, 0.149, 0.144, 0.156, 0.138, 0.155, 0.16, 0.124, 0.16, 0.169, 0.147, 0.138, 0.159, 0.127, 0.128, 0.158, 0.232, 0.218, 0.255, 0.191, 0.207, 0.188, 0.156, 0.2, 0.183, 0.213, 0.233, 0.238, 0.191, 0.162, 0.211, 0.144, 0.149, 0.154, 0.143, 0.146, 0.159, 0.149, 0.196, 0.194, 0.189, 0.161, 0.173, 0.169, 0.131, 0.162, 0.151, 0.146, 0.151, 0.171, 0.181, 0.156, 0.171, 0.195, 0.163, 0.194, 0.133, 0.167, 0.163, 0.139, 0.149, 0.337, 0.391, 0.325, 0.334, 0.477, 0.185, 0.196, 0.213, 0.201, 0.211, 0.279, 0.292, 0.249, 0.245, 0.246, 0.133, 0.16, 0.144, 0.122, 0.131, 0.168, 0.17, 0.215, 0.183, 0.203, 0.182, 0.142, 0.132, 0.132, 0.162, 0.286, 0.25, 0.256, 0.263, 0.254, 0.204, 0.211, 0.234, 0.226, 0.218, 0.188, 0.163, 0.186, 0.176, 0.173, 0.144, 0.121, 0.149, 0.143, 0.152, 0.179, 0.18, 0.178, 0.16, 0.188, 0.331, 0.313, 0.331, 0.311, 0.349, 0.226, 0.181, 0.229, 0.245, 0.213, 0.162, 0.172, 0.158, 0.149, 0.164, 0.211, 0.201, 0.221, 0.231, 0.225, 0.219, 0.168, 0.197, 0.206, 0.234, 0.164, 0.197, 0.142, 0.217, 0.209, 0.147, 0.154, 0.161, 0.15, 0.165, 0.141, 0.145, 0.143, 0.156, 0.15, 0.275, 0.319, 0.226, 0.334, 0.263, 0.179, 0.15, 0.159, 0.163, 0.177, 0.146, 0.144, 0.176, 0.149, 0.167, 0.236, 0.308, 0.221, 0.281, 0.257, 0.219, 0.177, 0.19, 0.196, 0.214, 0.147, 0.15, 0.155, 0.207, 0.144, 0.248, 0.241, 0.169, 0.233, 0.238, 0.173, 0.107, 0.159, 0.167, 0.14, 0.156, 0.148, 0.154, 0.176, 0.166, 0.304, 0.317, 0.194, 0.308, 0.284, 0.302, 0.23, 0.29, 0.29, 0.278, 0.171, 0.167, 0.182, 0.169, 0.188, 0.178, 0.199, 0.127, 0.173, 0.161, 0.314, 0.248, 0.322, 0.289, 0.388, 0.181, 0.162, 0.19, 0.211, 0.184, 0.183, 0.135, 0.13, 0.181, 0.188, 0.38, 0.361, 0.371, 0.385, 0.38, 0.163, 0.16, 0.149, 0.166, 0.168, 0.173, 0.13, 0.133, 0.166, 0.162, 0.188, 0.146, 0.151, 0.186, 0.177, 0.132, 0.182, 0.136, 0.13, 0.141, 0.161, 0.127, 0.134, 0.166, 0.177, 0.176, 0.185, 0.154, 0.17, 0.175, 0.139, 0.154, 0.131, 0.139, 0.144, 0.159, 0.134, 0.143, 0.165, 0.16, 0.194, 0.158, 0.18, 0.209, 0.197, 0.203, 0.17, 0.187, 0.188, 0.199, 0.208, 0.178, 0.161, 0.21, 0.204, 0.288, 0.226, 0.277, 0.29, 0.274, 0.175, 0.196, 0.186, 0.182, 0.194, 0.331, 0.276, 0.229, 0.45, 0.335, 0.182, 0.153, 0.153, 0.149, 0.165, 0.271, 0.26, 0.267, 0.296, 0.278, 0.134, 0.137, 0.125, 0.176, 0.145, 0.238, 0.2, 0.191, 0.227, 0.228, 0.16, 0.156, 0.175, 0.173, 0.173, 0.173, 0.202, 0.164, 0.19, 0.193, 0.218, 0.19, 0.185, 0.18, 0.204, 0.266, 0.293, 0.258, 0.317, 0.267, 0.171, 0.16, 0.15, 0.159, 0.171, 0.135, 0.149, 0.165, 0.176, 0.166, 0.377, 0.363, 0.367, 0.415, 0.384, 0.18, 0.179, 0.168, 0.193, 0.189, 0.189, 0.225, 0.214, 0.194, 0.216, 0.151, 0.147, 0.141, 0.183, 0.177, 0.179, 0.179, 0.194, 0.191, 0.186, 0.209, 0.214, 0.188, 0.195, 0.22, 0.178, 0.188, 0.184, 0.23, 0.198, 0.154, 0.12, 0.137, 0.123, 0.157, 0.205, 0.198, 0.177, 0.19, 0.196, 0.296, 0.268, 0.279, 0.314, 0.281, 0.143, 0.135, 0.172, 0.119, 0.174, 0.171, 0.16, 0.18, 0.17, 0.207, 0.159, 0.161, 0.144, 0.159, 0.165, 0.192, 0.21, 0.227, 0.213, 0.236, 0.283, 0.215, 0.261, 0.22, 0.242, 0.184, 0.194, 0.174, 0.182, 0.18, 0.244, 0.293, 0.259, 0.26, 0.286, 0.241, 0.171, 0.224, 0.227, 0.251, 0.347, 0.384, 0.303, 0.293, 0.33, 0.218, 0.233, 0.217, 0.245, 0.228, 0.193, 0.15, 0.186, 0.214, 0.187, 0.295, 0.298, 0.305, 0.396, 0.342, 0.166, 0.173, 0.178, 0.185, 0.175, 0.137, 0.123, 0.13, 0.128, 0.147, 0.176, 0.139, 0.16, 0.163, 0.152, 0.345, 0.338, 0.353, 0.341, 0.341, 0.357, 0.336, 0.36, 0.349, 0.363, 0.466, 0.388, 0.449, 0.443, 0.444, 0.138, 0.128, 0.138, 0.15, 0.154, 0.316, 0.196, 0.288, 0.224, 0.306, 0.158, 0.17, 0.157, 0.152, 0.159, 0.2, 0.206, 0.205, 0.263, 0.202, 0.19, 0.129, 0.169, 0.176, 0.197, 0.266, 0.225, 0.26, 0.272, 0.231, 0.383, 0.291, 0.307, 0.3, 0.321, 0.139, 0.128, 0.172, 0.155, 0.146, 0.178, 0.121, 0.163, 0.179, 0.167, 0.196, 0.164, 0.171, 0.188, 0.172, 0.287, 0.252, 0.282, 0.222, 0.264, 0.23, 0.184, 0.263, 0.265, 0.231, 0.201, 0.175, 0.167, 0.181, 0.202, 0.154, 0.174, 0.169, 0.178, 0.21, 0.142, 0.131, 0.155, 0.154, 0.151, 0.272, 0.268, 0.277, 0.298, 0.284, 0.17, 0.174, 0.164, 0.186, 0.183, 0.163, 0.125, 0.174, 0.168, 0.195, 0.483, 0.449, 0.309, 0.347, 0.363, 0.169, 0.177, 0.176, 0.174, 0.167, 0.239, 0.15, 0.268, 0.239, 0.22, 0.189, 0.19, 0.186, 0.22, 0.19, 0.117, 0.097, 0.138, 0.153, 0.138, 0.071, 0.057, 0.075, 0.065, 0.076, 0.077, 0.069, 0.07, 0.071, 0.072
#             ],
#             [
#                 0.309, 0.254, 0.263, 0.354, 0.25, 0.24, 0.279, 0.282, 0.266, 0.212, 0.408, 0.419, 0.381, 0.462, 0.394, 0.497, 0.464, 0.503, 0.455, 0.442, 0.496, 0.506, 0.503, 0.494, 0.412, 0.367, 0.356, 0.345, 0.388, 0.383, 0.332, 0.294, 0.431, 0.312, 0.328, 0.526, 0.521, 0.552, 0.514, 0.457, 0.495, 0.514, 0.51, 0.498, 0.484, 0.444, 0.404, 0.448, 0.392, 0.42, 0.621, 0.633, 0.619, 0.637, 0.55, 0.203, 0.257, 0.246, 0.2, 0.258, 0.668, 0.603, 0.661, 0.639, 0.583, 0.499, 0.464, 0.49, 0.491, 0.468, 0.374, 0.31, 0.286, 0.227, 0.32, 0.644, 0.643, 0.643, 0.637, 0.579, 0.286, 0.26, 0.249, 0.262, 0.281, 0.247, 0.245, 0.244, 0.202, 0.287, 0.347, 0.351, 0.442, 0.351, 0.341, 0.421, 0.37, 0.409, 0.353, 0.414, 0.371, 0.381, 0.358, 0.317, 0.376, 0.386, 0.4, 0.381, 0.395, 0.408, 0.376, 0.362, 0.397, 0.386, 0.389, 0.337, 0.273, 0.272, 0.217, 0.26, 0.365, 0.351, 0.364, 0.37, 0.36, 0.424, 0.379, 0.341, 0.401, 0.403, 0.355, 0.359, 0.343, 0.324, 0.333, 0.395, 0.415, 0.355, 0.348, 0.338, 0.462, 0.451, 0.46, 0.439, 0.449, 0.416, 0.371, 0.347, 0.364, 0.337, 0.678, 0.711, 0.668, 0.69, 0.636, 0.245, 0.228, 0.248, 0.261, 0.257, 0.264, 0.247, 0.244, 0.248, 0.212, 0.261, 0.272, 0.283, 0.259, 0.31, 0.253, 0.274, 0.228, 0.239, 0.235, 0.463, 0.389, 0.337, 0.326, 0.348, 0.542, 0.557, 0.55, 0.534, 0.514, 0.453, 0.583, 0.457, 0.469, 0.617, 0.487, 0.501, 0.512, 0.506, 0.444, 0.411, 0.403, 0.397, 0.392, 0.397, 0.205, 0.236, 0.238, 0.207, 0.201, 0.511, 0.521, 0.511, 0.486, 0.491, 0.308, 0.318, 0.316, 0.284, 0.281, 0.597, 0.643, 0.639, 0.634, 0.578, 0.376, 0.374, 0.365, 0.36, 0.378, 0.375, 0.41, 0.356, 0.364, 0.417, 0.231, 0.273, 0.279, 0.26, 0.284, 0.523, 0.526, 0.54, 0.531, 0.503, 0.361, 0.393, 0.349, 0.33, 0.38, 0.368, 0.423, 0.363, 0.382, 0.357, 0.38, 0.353, 0.358, 0.372, 0.445, 0.386, 0.379, 0.402, 0.37, 0.378, 0.331, 0.378, 0.341, 0.383, 0.358, 0.423, 0.384, 0.419, 0.422, 0.376, 0.383, 0.411, 0.45, 0.452, 0.566, 0.323, 0.35, 0.336, 0.379, 0.362, 0.593, 0.615, 0.632, 0.601, 0.542, 0.345, 0.357, 0.382, 0.382, 0.373, 0.275, 0.269, 0.29, 0.269, 0.307, 0.392, 0.449, 0.395, 0.441, 0.428, 0.58, 0.589, 0.635, 0.585, 0.557, 0.366, 0.383, 0.358, 0.397, 0.348, 0.407, 0.438, 0.412, 0.412, 0.404, 0.381, 0.37, 0.395, 0.349, 0.37, 0.575, 0.583, 0.598, 0.61, 0.565, 0.405, 0.519, 0.464, 0.324, 0.477, 0.367, 0.392, 0.387, 0.365, 0.364, 0.913, 0.938, 0.966, 0.934, 0.908, 0.344, 0.403, 0.383, 0.412, 0.417, 0.459, 0.445, 0.427, 0.409, 0.397, 0.264, 0.285, 0.294, 0.263, 0.269, 0.452, 0.42, 0.392, 0.393, 0.487, 0.693, 0.691, 0.715, 0.711, 0.672, 0.266, 0.307, 0.312, 0.29, 0.307, 0.366, 0.37, 0.394, 0.363, 0.388, 0.416, 0.401, 0.409, 0.357, 0.399, 0.512, 0.517, 0.525, 0.528, 0.503, 0.434, 0.45, 0.448, 0.459, 0.445, 0.282, 0.239, 0.271, 0.265, 0.273, 0.334, 0.428, 0.329, 0.379, 0.302, 0.593, 0.611, 0.627, 0.608, 0.592, 0.592, 0.608, 0.592, 0.572, 0.571, 0.411, 0.388, 0.396, 0.366, 0.4, 0.348, 0.297, 0.29, 0.382, 0.288, 0.415, 0.43, 0.328, 0.378, 0.419, 0.293, 0.296, 0.324, 0.215, 0.306, 0.281, 0.362, 0.334, 0.336, 0.284, 0.258, 0.296, 0.167, 0.226, 0.253, 0.468, 0.364, 0.287, 0.298, 0.371, 0.292, 0.27, 0.288, 0.272, 0.28, 0.292, 0.315, 0.237, 0.367, 0.328, 0.226, 0.223, 0.174, 0.189, 0.236, 0.394, 0.345, 0.359, 0.381, 0.341, 0.386, 0.357, 0.276, 0.383, 0.38, 0.46, 0.416, 0.432, 0.429, 0.473, 0.453, 0.476, 0.501, 0.501, 0.474, 0.386, 0.402, 0.402, 0.387, 0.415, 0.359, 0.358, 0.364, 0.379, 0.408, 0.312, 0.278, 0.282, 0.365, 0.307, 0.337, 0.371, 0.313, 0.369, 0.426, 0.293, 0.26, 0.301, 0.29, 0.382, 0.292, 0.289, 0.266, 0.313, 0.323, 0.276, 0.265, 0.267, 0.334, 0.263, 0.44, 0.514, 0.475, 0.485, 0.469, 0.407, 0.351, 0.394, 0.393, 0.442, 0.309, 0.256, 0.245, 0.306, 0.271, 0.563, 0.437, 0.481, 0.44, 0.428, 0.903, 0.853, 0.862, 0.87, 0.794, 0.349, 0.352, 0.343, 0.388, 0.362, 0.411, 0.359, 0.388, 0.385, 0.386, 0.413, 0.363, 0.33, 0.383, 0.431, 0.245, 0.224, 0.228, 0.231, 0.233, 0.408, 0.372, 0.418, 0.401, 0.363, 0.634, 0.661, 0.662, 0.619, 0.611, 0.227, 0.207, 0.202, 0.172, 0.229, 1.098, 1.1, 1.196, 1.116, 1.059, 0.573, 0.582, 0.583, 0.576, 0.546, 0.298, 0.27, 0.231, 0.202, 0.269, 0.334, 0.281, 0.28, 0.275, 0.336, 0.388, 0.408, 0.397, 0.36, 0.354, 0.305, 0.291, 0.271, 0.236, 0.291, 0.568, 0.519, 0.541, 0.548, 0.531, 0.584, 0.579, 0.594, 0.599, 0.559, 0.482, 0.524, 0.496, 0.47, 0.484, 0.617, 0.427, 0.484, 0.577, 0.464, 0.351, 0.359, 0.32, 0.365, 0.359, 0.47, 0.477, 0.486, 0.492, 0.443, 0.348, 0.274, 0.301, 0.307, 0.32, 0.412, 0.448, 0.463, 0.444, 0.423, 0.434, 0.442, 0.406, 0.411, 0.372, 0.399, 0.403, 0.414, 0.397, 0.44, 0.288, 0.323, 0.283, 0.369, 0.297, 0.467, 0.495, 0.448, 0.477, 0.409, 0.231, 0.291, 0.25, 0.241, 0.288, 0.363, 0.347, 0.379, 0.369, 0.367, 0.133, 0.109, 0.106, 0.132, 0.093, 0.116, 0.143, 0.153, 0.148, 0.153, 0.217, 0.137, 0.231, 0.227, 0.212
#             ]
#         ],
#         [#filter or not
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.366, 0.367, 0.369, 0.376, 0.358, 0.468, 0.491, 0.427, 0.511, 0.487, 1.01, 1.064, 1.048, 1.066, 0.975, 0.218, 0.22, 0.345, 0.338, 0.26, 0.453, 0.471, 0.501, 0.51, 0.46, 0.319, 0.387, 0.322, 0.37, 0.342, 0.266, 0.306, 0.464, 0.457, 0.384, 0.647, 0.684, 0.662, 0.943, 0.648, 0.453, 0.478, 0.37, 0.408, 0.431, 0.467, 0.516, 0.68, 0.924, 0.569, 0.464, 0.496, 0.464, 0.464, 0.454, 0.379, 0.382, 0.331, 0.389, 0.422, 0.266, 0.433, 0.446, 0.474, 0.399, 0.333, 0.335, 0.365, 0.381, 0.35, 0.426, 0.39, 0.337, 0.427, 0.417, 0.321, 0.519, 0.556, 0.812, 0.472, 0.665, 1.081, 0.785, 0.863, 0.853, 0.508, 0.484, 0.394, 0.53, 0.486, 0.202, 0.343, 0.326, 0.358, 0.319, 0.604, 0.587, 0.534, 0.458, 0.577, 0.511, 0.493, 0.422, 0.634, 0.522, 0.401, 0.588, 0.589, 0.704, 0.589, 0.531, 0.574, 0.539, 0.537, 0.553, 0.441, 0.418, 0.401, 0.468, 0.455, 0.259, 0.364, 0.266, 0.358, 0.317, 0.316, 0.362, 0.326, 0.36, 0.349, 0.757, 0.647, 0.705, 0.766, 0.749, 0.348, 0.436, 0.443, 0.442, 0.385, 0.362, 0.375, 0.349, 0.42, 0.394, 0.773, 0.593, 0.556, 0.601, 0.558, 0.659, 0.725, 0.776, 0.739, 0.69, 0.472, 0.472, 0.449, 0.461, 0.44, 0.437, 0.448, 0.412, 0.441, 0.437, 0.945, 0.996, 0.993, 0.98, 0.948, 0.335, 0.356, 0.325, 0.475, 0.373, 0.365, 0.408, 0.337, 0.348, 0.385, 0.411, 0.385, 0.398, 0.405, 0.351, 0.332, 0.37, 0.353, 0.511, 0.369, 0.513, 0.384, 0.478, 0.441, 0.438, 0.603, 0.809, 0.605, 0.806, 0.555, 0.598, 0.866, 0.611, 0.715, 0.624, 0.679, 0.696, 0.681, 0.688, 0.699, 0.469, 0.485, 0.458, 0.491, 0.454, 0.57, 0.562, 0.622, 0.663, 0.661, 0.416, 0.426, 0.481, 0.457, 0.451, 0.408, 0.367, 0.465, 0.475, 0.395, 0.983, 1.013, 0.977, 1.009, 0.955, 0.414, 0.417, 0.43, 0.456, 0.466, 0.588, 0.506, 0.461, 0.465, 0.46, 0.712, 0.598, 0.589, 0.72, 0.675, 0.393, 0.348, 0.408, 0.381, 0.381, 0.54, 0.528, 0.718, 0.557, 0.552, 0.398, 0.385, 0.364, 0.402, 0.429, 0.626, 0.941, 0.759, 0.892, 0.653, 0.553, 0.5, 0.484, 0.766, 0.543, 0.677, 0.691, 0.673, 0.738, 0.637, 0.72, 0.533, 0.659, 0.635, 0.62, 0.882, 0.927, 0.918, 0.909, 0.872, 1.169, 1.191, 1.129, 1.185, 1.126, 0.881, 0.919, 0.915, 0.938, 0.855, 0.567, 0.574, 0.61, 0.903, 0.589, 0.68, 0.517, 0.464, 0.596, 0.548, 0.79, 0.568, 0.578, 0.62, 0.554, 0.482, 0.515, 0.496, 0.472, 0.485, 0.697, 0.517, 0.541, 0.53, 0.507, 0.432, 0.428, 0.5, 0.468, 0.453, 0.388, 0.392, 0.36, 0.496, 0.414, 0.387, 0.36, 0.363, 0.467, 0.391, 0.829, 0.734, 0.74, 0.798, 0.731, 0.467, 0.44, 0.474, 0.416, 0.474, 0.287, 0.333, 0.322, 0.379, 0.308, 0.286, 0.265, 0.28, 0.294, 0.30
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 1.451, 1.387, 1.517, 1.477, 1.392, 1.042, 0.983, 0.939, 0.937, 0.947, 1.151, 1.107, 1.111, 1.168, 1.094, 0.794, 0.733, 0.861, 0.904, 0.864, 2.828, 2.759, 2.761, 2.771, 2.62, 1.308, 1.303, 1.275, 1.274, 1.192, 3.357, 3.331, 3.383, 3.365, 3.155, 1.282, 1.169, 1.217, 1.208, 1.122, 0.767, 0.752, 0.751, 0.716, 1.054, 0.749, 0.699, 1.14, 0.759, 1.113, 0.955, 0.848, 0.91, 0.876, 0.818, 0.885, 0.816, 0.808, 0.89, 0.777, 1.348, 1.384, 1.431, 1.348, 1.335, 1.172, 1.118, 1.156, 1.128, 1.018, 1.015, 0.926, 0.98, 1.037, 0.926, 1.921, 1.958, 2.07, 1.969, 1.784, 0.797, 0.732, 0.739, 0.793, 0.705, 0.65, 0.809, 0.747, 0.965, 0.648, 1.831, 1.861, 1.874, 1.835, 1.757, 0.765, 0.664, 0.721, 0.669, 0.683, 1.4, 1.385, 1.361, 1.385, 1.336, 0.766, 0.813, 1.117, 0.759, 0.753, 1.634, 1.658, 1.691, 1.736, 1.605, 1.716, 1.725, 1.7, 1.684, 1.575, 0.75, 0.953, 0.803, 1.061, 0.857, 0.919, 0.968, 0.855, 0.771, 0.905, 2.101, 2.024, 2.028, 2.02, 1.872, 1.095, 1.101, 1.122, 1.076, 1.087, 0.864, 0.895, 0.837, 0.869, 0.834, 1.642, 1.636, 1.66, 1.584, 1.584, 1.781, 1.85, 1.832, 1.83, 1.74, 1.004, 0.998, 0.954, 0.963, 0.972, 1.283, 1.293, 1.218, 1.189, 1.156, 0.864, 0.885, 0.911, 0.882, 0.869, 3.681, 3.724, 3.758, 3.717, 3.471, 1.062, 1.053, 1.045, 1.07, 0.947, 1.694, 1.624, 1.684, 1.68, 1.57, 0.806, 1.108, 0.732, 1.12, 1.127, 0.82, 0.88, 1.055, 0.851, 0.815, 1.612, 1.53, 1.599, 1.642, 1.487, 1.416, 1.386, 1.385, 1.398, 1.371, 2.181, 2.213, 2.26, 2.269, 2.094, 1.658, 1.587, 1.696, 1.672, 1.524, 1.003, 1.019, 0.999, 0.99, 0.937, 0.823, 0.892, 0.867, 1.293, 0.863, 0.956, 0.931, 1.001, 0.912, 0.915, 1.074, 0.721, 0.722, 0.75, 0.76, 0.605, 0.611, 0.618, 0.611, 0.601, 0.857, 0.872, 0.945, 0.921, 0.841, 1.358, 1.33, 1.315, 1.393, 1.27, 1.398, 1.41, 1.447, 1.394, 1.269, 1.103, 1.066, 1.104, 1.107, 1.04, 0.837, 1.016, 0.846, 0.843, 0.827, 0.71, 1.034, 0.737, 0.709, 0.694, 0.801, 0.702, 0.802, 1.165, 0.798, 1.583, 1.586, 1.638, 1.578, 1.463, 2.817, 2.837, 2.883, 2.877, 2.598, 0.812, 0.741, 0.942, 0.857, 0.81, 2.313, 2.293, 2.354, 2.307, 2.105, 1.799, 1.935, 1.809, 1.822, 1.755, 0.547, 0.529, 0.649, 0.512, 0.522, 1.9, 1.863, 1.886, 1.85, 1.671, 3.02, 3.03, 2.991, 3.12, 2.791, 1.326, 1.296, 1.354, 1.298, 1.289, 1.589, 1.539, 1.56, 1.629, 1.431, 0.72, 1.039, 0.673, 0.692, 0.738, 1.77, 1.821, 1.787, 1.721, 1.681, 0.909, 0.87, 1.418, 0.956, 0.952, 1.165, 0.93, 1.15, 0.916, 1.227, 0.852, 0.848, 0.799, 0.822, 0.803, 0.657, 0.666, 0.628, 0.665, 0.593, 0.902, 0.869, 0.846, 0.876, 0.817
#             ]
#         ],    
#         [
#             [
#                 1.806, 1.79, 1.956, 1.836, 1.841, 2.172, 2.173, 2.748, 1.818, 1.736, 4.609, 4.909, 4.643, 4.532, 4.369, 2.864, 3.0, 2.971, 2.955, 2.86, 2.044, 2.128, 2.056, 2.034, 2.035, 1.708, 1.691, 1.707, 1.579, 1.642, 2.438, 2.532, 2.442, 2.422, 2.338, 2.739, 2.859, 2.747, 2.762, 2.669, 1.945, 2.898, 1.996, 2.749, 2.321, 1.383, 2.061, 1.392, 1.695, 1.33, 1.335, 1.465, 1.363, 2.055, 2.152, 2.758, 2.927, 2.923, 2.789, 2.744, 5.853, 5.942, 5.801, 5.81, 5.521, 1.425, 1.591, 1.621, 1.464, 1.526, 1.906, 1.886, 1.892, 1.847, 2.742, 2.344, 2.498, 2.526, 2.386, 2.26, 2.609, 2.758, 2.647, 2.586, 2.516, 2.298, 2.425, 2.366, 2.283, 2.269, 1.821, 1.956, 1.864, 2.639, 2.617, 5.058, 5.018, 4.856, 4.891, 4.69, 2.248, 2.176, 2.326, 1.46, 1.53, 1.992, 2.131, 2.032, 1.933, 1.888, 2.981, 3.089, 2.991, 2.973, 2.837, 3.063, 3.087, 2.979, 3.025, 2.917, 4.389, 4.541, 4.453, 4.378, 4.014, 5.542, 5.629, 5.545, 5.513, 5.295, 4.91, 5.077, 4.801, 4.725, 4.671, 2.037, 2.181, 2.06, 2.054, 1.933, 2.403, 1.757, 1.761, 2.31, 1.682, 2.939, 2.947, 2.875, 2.911, 2.743
#             ],
#             [
#                 19.543, 18.144, 18.402, 18.576, 16.52, 24.082, 23.9, 23.984, 22.898, 20.422, 10.167, 10.219, 10.31, 10.061, 9.128, 14.531, 14.467, 14.574, 14.305, 12.938, 8.181, 8.208, 8.016, 8.146, 7.481, 5.899, 5.907, 5.908, 5.99, 5.748, 18.479, 18.361, 18.286, 18.263, 16.697, 3.816, 3.855, 3.949, 3.936, 3.667, 8.772, 8.627, 8.77, 8.811, 8.078, 6.906, 6.862, 6.859, 6.962, 6.293, 10.459, 10.608, 10.604, 10.585, 9.684, 24.906, 24.301, 23.681, 24.21, 21.413, 13.257, 13.205, 13.399, 13.214, 12.089, 26.096, 25.327, 26.212, 24.797, 22.786, 8.957, 8.932, 8.994, 9.013, 8.117, 22.624, 21.514, 20.882, 20.944, 19.215, 7.714, 7.636, 7.601, 7.702, 7.14, 13.641, 13.447, 13.803, 13.696, 12.595, 7.101, 6.961, 7.028, 6.917, 6.281, 9.755, 9.634, 9.635, 9.698, 8.818, 7.266, 7.408, 7.343, 7.359, 6.691, 7.28, 7.338, 7.255, 7.35, 6.672, 10.066, 10.04, 10.194, 10.013, 9.052, 23.654, 23.617, 23.573, 25.817, 21.009, 6.312, 6.36, 6.248, 6.249, 5.724, 25.205, 26.321, 27.852, 26.501, 23.067, 17.459, 17.017, 16.602, 16.948, 15.564, 8.571, 8.501, 8.479, 8.502, 7.791, 3.458, 3.438, 3.437, 3.502, 3.229, 4.735, 4.567, 4.539, 4.712, 4.475
#             ]
#         ],  
#         [
#             [
#                 14.348, 14.331, 14.478, 14.355, 13.676, 10.155, 9.981, 10.311, 10.065, 10.026, 19.345, 19.383, 19.364, 19.362, 18.348, 10.579, 10.461, 10.61, 10.299, 9.878, 11.613, 11.443, 11.433, 11.434, 10.902, 13.083, 12.976, 12.914, 13.046, 12.293, 28.064, 28.613, 28.2, 28.393, 27.119, 11.161, 11.288, 11.262, 11.1, 10.711, 10.877, 10.872, 10.875, 10.952, 10.532, 9.778, 9.582, 9.457, 9.701, 9.182, 26.583, 26.822, 26.879, 27.185, 25.392, 12.623, 12.535, 12.532, 12.479, 11.96, 19.336, 19.181, 19.368, 19.3, 18.184, 21.952, 21.942, 22.052, 23.126, 20.937, 25.275, 25.289, 25.871, 26.117, 23.99
#             ],
#             [
#                 122.545, 127.587, 122.861, 126.289, 111.18, 122.806, 118.088, 116.054, 119.474, 108.679, 65.84, 66.289, 66.68, 66.429, 56.346, 109.659, 111.891, 104.767, 131.898, 93.695, 61.53, 60.23, 65.285, 63.32, 48.981, 115.947, 112.436, 111.171, 99.94, 89.634, 136.606, 140.442, 135.875, 130.411, 129.993, 123.765, 122.309, 133.617, 124.448, 109.18, 114.516, 117.052, 114.766, 118.206, 96.567, 51.602, 49.3, 52.972, 47.543, 47.549, 84.523, 87.213, 92.689, 77.506, 74.011, 102.422, 97.899, 90.143, 102.285, 90.927, 71.739, 79.052, 80.888, 76.265, 72.293, 112.549, 109.328, 103.008, 114.478, 97.164, 120.754, 121.572, 111.36, 109.786, 99.919
#             ]
#         ],   
#         [
#             [
#                 1713.504, 1741.591, 1721.646, 1713.08, 1666.09, 1730.114, 1766.36, 1722.91, 1856.862, 1782.397, 1721.183, 1733.468, 1759.488, 1729.669, 1695.352
#             ],
#             [
#                 9894.905, 9899.71, 9906.249, 9969.281, 8907.607, 9902.359, 9892.856, 9907.377, 9963.56, 8906.326, 9890.464, 9897.454, 9901.675, 9970.215, 8901.829
#             ]
#         ],
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d3nonetrainlat_l)):
#         local_ys_array, local_ci_array = [], []
#         for f_i in range(len(ys_d3nonetrainlat_l[m_i])):
#             local_ys_array.append(np.mean(ys_d3nonetrainlat_l[m_i][f_i], axis=(0)))
#             local_ci_array.append(1.96 * np.std(ys_d3nonetrainlat_l[m_i][f_i], axis=0)/np.sqrt(len(ys_d3nonetrainlat_l[m_i][f_i])))
#         ys_array.append(local_ys_array)
#         ci_array.append(local_ci_array)
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v"]):
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     # ax.set_ylim(0.3,1)
#     ax.set_yscale("log")
#     ax.set_ylabel("Incremental Training Time (s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     filename = "testf1score_by_nestimator_with_rawinput_data_3"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 5))
#     xs=[20, 50]
#     labels = ("10 estimators without filter", "50 estimators without filter", "100 estimators without filter")
#     ys_d3nofilterscore_l=[#models
#         [#filter or not
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.641, 0.647, 0.63, 0.634, 0.636, 0.647, 0.642, 0.63, 0.638, 0.64, 0.657, 0.638, 0.61, 0.631, 0.635
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.644, 0.651, 0.635, 0.637, 0.641, 0.655, 0.648, 0.639, 0.646, 0.649, 0.661, 0.643, 0.616, 0.635, 0.639
#             ],
#             [
#                 0.65, 0.651, 0.615, 0.64, 0.634, 0.638, 0.643, 0.626, 0.654, 0.65, 0.64, 0.621, 0.636, 0.652, 0.631
#             ]
#         ],    
#         [
#             [
#                 0.69, 0.697, 0.688, 0.676, 0.678, 0.684, 0.695, 0.68, 0.675, 0.665, 0.696, 0.699, 0.683, 0.676, 0.673
#             ],
#             [
#                 0.696, 0.702, 0.693, 0.682, 0.685, 0.697, 0.708, 0.693, 0.687, 0.677, 0.711, 0.713, 0.696, 0.692, 0.689
#             ],
#             [
#                 0.69, 0.693, 0.679, 0.707, 0.685, 0.7, 0.698, 0.669, 0.718, 0.701, 0.704, 0.707, 0.683, 0.724, 0.697
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d3nofilterscore_l)):
#         ys_array.append(np.mean(ys_d3nofilterscore_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d3nofilterscore_l[m_i], axis=1)/np.sqrt(len(ys_d3nofilterscore_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for idx, (ys, ci, label, mark, c) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", "."], ['#ff0000', '#ff0067', '#ff00ff'])):
#         ax.scatter(xs, ys, label=label, color=c, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10+5*idx, color=c, marker=mark)
#     # xs=[20, 50]
#     # labels = ("10 estimators with filter", "50 estimators with filter", "100 estimators with filter")
#     # ys_d0withfilterf1score_l=[#models
#     #     [#dims
#     #         [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#     #             0.877, 0.869, 0.888, 0.887, 0.888, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.88, 0.87, 0.89, 0.889, 0.891, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.862, 0.875, 0.885, 0.882, 0.878, 0.881, 0.875, 0.885, 0.891, 0.88, 0.873, 0.878, 0.881, 0.888, 0.886
#     #         ]
#     #     ],    
#     #     [
#     #         [
#     #             0.88, 0.88, 0.888, 0.884, 0.893, 0.9, 0.874, 0.887, 0.902, 0.895, 0.892, 0.883, 0.894, 0.893, 0.89
#     #         ],
#     #         [
#     #             0.883, 0.883, 0.893, 0.888, 0.896, 0.903, 0.877, 0.889, 0.905, 0.897, 0.893, 0.884, 0.895, 0.894, 0.891
#     #         ],
#     #         [
#     #             0.874, 0.881, 0.883, 0.897, 0.887, 0.883, 0.876, 0.889, 0.889, 0.894, 0.878, 0.879, 0.882, 0.884, 0.891
#     #         ]
#     #     ]
#     # ]
#     # ys_array, ci_array = [], []
#     # for m_i in range(len(ys_d0withfilterf1score_l)):
#     #     ys_array.append(np.mean(ys_d0withfilterf1score_l[m_i], axis=(1)))
#     #     ci_array.append(1.96 * np.std(ys_d0withfilterf1score_l[m_i], axis=1)/np.sqrt(len(ys_d0withfilterf1score_l[m_i][0])))
#     # ys_array = np.array(ys_array).T
#     # ci_array = np.array(ci_array).T
#     # for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", "."])):
#     #     ax.scatter(xs, ys, label=label, color='#ff6700', marker=mark)
#     #     ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10+5*idx, color='#ff6700', marker=mark)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
#     # ax.set_ylim(0.3,1)
#     ax.set_ylabel("F1-Score", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()

#     filename = "testf1score_by_nestimator_with_500rawinput_data_3"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     xs=[10, 50, 100]
#     labels = ("20 Labels per Model", "50 Labels per Model")
#     ys_d3nofilterscore_l=[#models
#         [#filter or not
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.68, 0.677, 0.679, 0.692, 0.695, 0.709, 0.7, 0.71, 0.696, 0.695, 0.72, 0.727, 0.743, 0.72, 0.699
#             ],
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.823, 0.819, 0.842, 0.842, 0.832, 0.844, 0.836, 0.857, 0.833, 0.832, 0.842, 0.831, 0.844, 0.828, 0.825
#             ],
#             [
#                 0.826, 0.824, 0.822, 0.832, 0.833, 0.821, 0.837, 0.842, 0.854, 0.843, 0.83, 0.835, 0.829, 0.83, 0.82
#             ]
#         ],    
#         [
#             [
#                 0.562, 0.555, 0.544, 0.545, 0.561, 0.535, 0.564, 0.548, 0.535, 0.547, 0.54, 0.52, 0.535, 0.531, 0.546
#             ],
#             [
#                 0.753, 0.764, 0.752, 0.761, 0.763, 0.765, 0.796, 0.762, 0.765, 0.77, 0.771, 0.762, 0.76, 0.758, 0.764
#             ],
#             [
#                 0.8, 0.819, 0.806, 0.82, 0.81, 0.82, 0.817, 0.821, 0.818, 0.818, 0.806, 0.824, 0.814, 0.789, 0.798
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d3nofilterscore_l)):
#         ys_array.append(np.mean(ys_d3nofilterscore_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d3nofilterscore_l[m_i], axis=1)/np.sqrt(len(ys_d3nofilterscore_l[m_i][0])))
#     # ys_array = np.array(ys_array).T
#     # ci_array = np.array(ci_array).T
#     for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", "."])): # ['#f00022', '#00f022', '#ff00ff']
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     # xs=[20, 50]
#     # labels = ("10 estimators with filter", "50 estimators with filter", "100 estimators with filter")
#     # ys_d0withfilterf1score_l=[#models
#     #     [#dims
#     #         [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#     #             0.877, 0.869, 0.888, 0.887, 0.888, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.88, 0.87, 0.89, 0.889, 0.891, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.862, 0.875, 0.885, 0.882, 0.878, 0.881, 0.875, 0.885, 0.891, 0.88, 0.873, 0.878, 0.881, 0.888, 0.886
#     #         ]
#     #     ],    
#     #     [
#     #         [
#     #             0.88, 0.88, 0.888, 0.884, 0.893, 0.9, 0.874, 0.887, 0.902, 0.895, 0.892, 0.883, 0.894, 0.893, 0.89
#     #         ],
#     #         [
#     #             0.883, 0.883, 0.893, 0.888, 0.896, 0.903, 0.877, 0.889, 0.905, 0.897, 0.893, 0.884, 0.895, 0.894, 0.891
#     #         ],
#     #         [
#     #             0.874, 0.881, 0.883, 0.897, 0.887, 0.883, 0.876, 0.889, 0.889, 0.894, 0.878, 0.879, 0.882, 0.884, 0.891
#     #         ]
#     #     ]
#     # ]
#     # ys_array, ci_array = [], []
#     # for m_i in range(len(ys_d0withfilterf1score_l)):
#     #     ys_array.append(np.mean(ys_d0withfilterf1score_l[m_i], axis=(1)))
#     #     ci_array.append(1.96 * np.std(ys_d0withfilterf1score_l[m_i], axis=1)/np.sqrt(len(ys_d0withfilterf1score_l[m_i][0])))
#     # ys_array = np.array(ys_array).T
#     # ci_array = np.array(ci_array).T
#     # for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", "."])):
#     #     ax.scatter(xs, ys, label=label, color='#ff6700', marker=mark)
#     #     ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10+5*idx, color='#ff6700', marker=mark)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Boosting Trees", fontsize=20)
#     # ax.set_ylim(0.3,1)
#     ax.set_ylabel("F1-Score", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     filename = "testf1score_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     # xs=[109319, 54659, 27329, 13664, 6832]
#     xs=[10, 20, 40, 80]
#     labels = ("Weighted F1 Scores for All Labels","Weighted Precision","Weighted Recall",)
#     ys_d0rawinputscores_l=[#models
#         [#dimensions
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 0.847, 0.82, 0.849, 0.867, 0.867, 0.87, 0.867, 0.893, 0.872, 0.902, 0.861, 0.816, 0.872, 0.858, 0.821
#             ],
#             [
#                 0.82, 0.789, 0.825, 0.841, 0.84, 0.832, 0.829, 0.87, 0.843, 0.871, 0.838, 0.787, 0.854, 0.834, 0.79
#             ],
#             [
#                 0.977, 0.958, 0.954, 0.969, 0.978, 0.989, 0.977, 0.97, 0.989, 0.99, 0.978, 0.942, 0.958, 0.965, 0.965
#             ]
#         ],
#         [
#             [
#                 0.891, 0.897, 0.905, 0.893, 0.875, 0.888, 0.91, 0.881, 0.891, 0.889, 0.903, 0.909, 0.903, 0.894, 0.877
#             ],
#             [
#                 0.857, 0.861, 0.87, 0.856, 0.834, 0.852, 0.888, 0.854, 0.861, 0.856, 0.886, 0.901, 0.901, 0.878, 0.848
#             ],
#             [
#                 0.99, 0.982, 0.99, 0.989, 0.99, 0.989, 0.97, 0.97, 0.988, 0.977, 0.977, 0.97, 0.956, 0.976, 0.978
#             ]
#         ],
#         [
#             [
#                 0.915, 0.933, 0.916, 0.925, 0.912, 0.932, 0.932, 0.925, 0.944, 0.937, 0.942, 0.93, 0.918, 0.926, 0.907
#             ],
#             [
#                 0.925, 0.951, 0.925, 0.933, 0.915, 0.909, 0.916, 0.9, 0.924, 0.913, 0.957, 0.946, 0.923, 0.935, 0.906
#             ],
#             [
#                 0.958, 0.958, 0.954, 0.965, 0.965, 0.989, 0.982, 0.988, 0.989, 0.988, 0.965, 0.958, 0.964, 0.965, 0.964
#             ]
#         ],
#         [
#             [
#                 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955, 0.951, 0.952, 0.956, 0.955, 0.955
#             ],
#             [
#                 0.997, 0.998, 0.995, 0.995, 0.993, 0.997, 0.998, 0.995, 0.995, 0.993, 0.997, 0.998, 0.995, 0.995, 0.993
#             ],
#             [
#                 0.945, 0.944, 0.952, 0.95, 0.953, 0.945, 0.944, 0.952, 0.95, 0.953, 0.945, 0.944, 0.952, 0.95, 0.953
#             ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0rawinputscores_l)):
#         ys_array.append(np.mean(ys_d0rawinputscores_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0rawinputscores_l[m_i], axis=1)/np.sqrt(len(ys_d0rawinputscores_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", ".", "*"])): # ['#f00022', '#00f022', '#ff00ff']
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
#     # ax.set_xlim(0,120000)
#     # ax.set_ylim(0.3,1)
#     ax.set_ylabel("Prediction Metrics", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()

#     filename = "inferencelatency_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 3))
#     # xs=[109319, 54659, 27329, 13664, 6832]
#     xs=[10, 20, 40, 80]
#     # labels = ("No Input Dimensions Collapsing", "Collapse to Total 109319 among all Submodels")
#     labels = ("Sum of Inference Times (s) among all Submodels",)
#     ys_d0infertime_l=[#models
#         [#dimensions
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 1.53, 1.48, 1.52, 1.51, 1.49, 1.52, 1.47, 1.5, 1.5, 1.5, 1.54, 1.49, 1.54, 1.52, 1.47
#             ],
#         ],
#         [
#             [
#                 1.53, 1.51, 1.61, 1.5, 1.46, 1.5, 1.47, 1.54, 1.51, 1.48, 1.52, 1.53, 1.54, 1.47, 1.49
#             ],
#         ],
#         [
#             [
#                 1.59, 1.53, 1.64, 1.56, 1.5, 1.51, 1.5, 1.62, 1.57, 1.57, 1.56, 1.51, 1.59, 1.62, 1.49
#             ],
#         ],
#         [
#             [
#                 2.03, 1.58, 1.78, 1.87, 1.6, 1.88, 1.59, 1.75, 1.93, 1.73, 1.83, 1.58, 1.75, 1.85, 1.63
#             ],
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0infertime_l)):
#         ys_array.append(np.mean(ys_d0infertime_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0infertime_l[m_i], axis=1)/np.sqrt(len(ys_d0infertime_l[m_i][0])))
#     ys_array = np.array(ys_array).T
#     ci_array = np.array(ci_array).T
#     for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", ".", "*"])): # ['#f00022', '#00f022', '#ff00ff']
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
#     # ax.set_xlim(0,120000)
#     # ax.set_ylim(0.3,1)
#     ax.set_ylabel("Inference Time (s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()



#     filename = "inferencelatency_by_input_size_with_rawinput_data_0"
#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))
#     xs=[109319, 54659, 27329, 13664]
#     labels = ("10 labels per model", "20 labels per model","40 labels per model","80 labels per model")
#     ys_d0infertime_l=[#models
#         [#dimensions
#             [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#                 13.5, 13.8, 14.16, 13.75, 13.49, 13.54, 13.86, 13.55, 13.77, 13.48, 13.53, 13.82, 13.59, 13.61, 13.5
#             ],
#             [
#                 6.58, 6.61, 6.53, 6.58, 6.59, 6.54, 6.61, 6.68, 6.62, 6.58, 6.56, 6.59, 6.66, 6.58, 6.54
#             ],
#             [
#                 3.28, 3.31, 3.28, 3.29, 3.27, 3.32, 3.35, 3.29, 3.29, 3.27, 3.29, 3.32, 3.29, 3.3, 3.3
#             ],
#             [
#                 1.75, 1.74, 1.74, 1.76, 1.67, 1.75, 1.69, 1.74, 1.75, 1.69, 1.75, 1.72, 1.73, 1.75, 1.73
#             ],
#         ],    
#         [
#             [
#                 6.88, 7.0, 7.38, 7.86, 6.88, 6.87, 6.97, 7.16, 7.11, 6.9, 6.95, 6.89, 7.17, 7.26, 6.89
#             ],
#             [
#                 3.32, 3.48, 3.39, 3.42, 3.36, 3.34, 3.43, 3.43, 3.42, 3.42, 3.46, 3.36, 3.42, 3.42, 3.35
#             ],
#             [
#                 1.71, 1.73, 1.73, 1.71, 1.72, 1.71, 1.72, 1.72, 1.71, 1.7, 1.71, 1.73, 1.71, 1.71, 1.7
#             ],
#             [
#                 0.93, 0.92, 0.94, 0.92, 0.91, 0.92, 0.91, 0.92, 0.92, 0.91, 0.92, 0.92, 0.93, 0.92, 0.91
#             ],
#         ],    
#         [
#             [
#                 3.6, 3.55, 3.78, 3.93, 3.58, 3.6, 3.65, 3.78, 3.77, 3.6, 3.54, 3.6, 3.71, 4.01, 3.59
#             ],
#             [
#                 1.75, 1.76, 1.8, 1.84, 1.78, 1.76, 1.75, 1.82, 1.82, 1.76, 1.77, 1.76, 1.82, 1.82, 1.79
#             ],
#             [
#                 0.94, 0.93, 0.92, 0.93, 0.92, 0.94, 0.92, 0.94, 0.93, 0.93, 0.92, 0.92, 0.94, 0.94, 0.92
#             ],
#             [
#                 0.53, 0.52, 0.53, 0.54, 0.52, 0.54, 0.54, 0.54, 0.53, 0.52, 0.54, 0.53, 0.54, 0.54, 0.53
#             ],
#         ],    
#         [
#             [
#                 1.96, 1.93, 2.08, 2.15, 1.93, 1.96, 1.93, 2.06, 2.14, 1.99, 1.95, 1.98, 2.06, 2.15, 1.96
#             ],
#             [
#                 1.0, 0.98, 1.09, 0.98, 0.99, 0.99, 0.98, 1.03, 0.98, 1.0, 1.01, 0.99, 1.04, 1.04, 0.99
#             ],
#             [
#                 0.55, 0.55, 0.55, 0.56, 0.54, 0.55, 0.55, 0.56, 0.55, 0.55, 0.55, 0.54, 0.56, 0.54, 0.56
#             ],
#             [
#                 0.35, 0.33, 0.34, 0.35, 0.34, 0.35, 0.34, 0.34, 0.35, 0.34, 0.34, 0.34, 0.34, 0.34, 0.34
#             ],
#             # [
#             #     0.07, 0.07, 0.07, 0.06, 0.05, 0.05, 0.07, 0.07, 0.06, 0.07, 0.06, 0.06, 0.08, 0.08, 0.06
#             # ]
#         ]
#     ]
#     ys_array, ci_array = [], []
#     for m_i in range(len(ys_d0infertime_l)):
#         ys_array.append(np.mean(ys_d0infertime_l[m_i], axis=(1)))
#         ci_array.append(1.96 * np.std(ys_d0infertime_l[m_i], axis=1)/np.sqrt(len(ys_d0infertime_l[m_i][0])))
#     # ys_array = np.array(ys_array).T
#     # ci_array = np.array(ci_array).T
#     for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", ".", "*"])): # ['#f00022', '#00f022', '#ff00ff']
#         ax.scatter(xs, ys, label=label, marker=mark)
#         ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
#     # xs=[20, 50]
#     # labels = ("10 estimators with filter", "50 estimators with filter", "100 estimators with filter")
#     # ys_d0withfilterf1score_l=[#models
#     #     [#dims
#     #         [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
#     #             0.877, 0.869, 0.888, 0.887, 0.888, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.88, 0.87, 0.89, 0.889, 0.891, 0.882, 0.868, 0.878, 0.887, 0.884, 0.871, 0.871, 0.885, 0.875, 0.889
#     #         ],
#     #         [
#     #             0.862, 0.875, 0.885, 0.882, 0.878, 0.881, 0.875, 0.885, 0.891, 0.88, 0.873, 0.878, 0.881, 0.888, 0.886
#     #         ]
#     #     ],    
#     #     [
#     #         [
#     #             0.88, 0.88, 0.888, 0.884, 0.893, 0.9, 0.874, 0.887, 0.902, 0.895, 0.892, 0.883, 0.894, 0.893, 0.89
#     #         ],
#     #         [
#     #             0.883, 0.883, 0.893, 0.888, 0.896, 0.903, 0.877, 0.889, 0.905, 0.897, 0.893, 0.884, 0.895, 0.894, 0.891
#     #         ],
#     #         [
#     #             0.874, 0.881, 0.883, 0.897, 0.887, 0.883, 0.876, 0.889, 0.889, 0.894, 0.878, 0.879, 0.882, 0.884, 0.891
#     #         ]
#     #     ]
#     # ]
#     # ys_array, ci_array = [], []
#     # for m_i in range(len(ys_d0withfilterf1score_l)):
#     #     ys_array.append(np.mean(ys_d0withfilterf1score_l[m_i], axis=(1)))
#     #     ci_array.append(1.96 * np.std(ys_d0withfilterf1score_l[m_i], axis=1)/np.sqrt(len(ys_d0withfilterf1score_l[m_i][0])))
#     # ys_array = np.array(ys_array).T
#     # ci_array = np.array(ci_array).T
#     # for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", "."])):
#     #     ax.scatter(xs, ys, label=label, color='#ff6700', marker=mark)
#     #     ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10+5*idx, color='#ff6700', marker=mark)
#     ax.hlines(y=0.39, xmin=0, xmax=120000, label="Decomposing an XGBoost Model")
#     ax.tick_params(axis='both', which='major', labelsize=20)
#     ax.tick_params(axis='both', which='minor', labelsize=18)
#     ax.set_xlabel("Input Feature Dimensions", fontsize=20)
#     ax.set_xlim(0,120000)
#     # ax.set_ylim(0.3,1)
#     ax.set_ylabel("Inference Time (s)", fontsize=20)
#     ax.grid()
#     plt.legend(prop={'size': 16})
#     # plt.show()
#     plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
#     plt.close()

#     # print(0)








    # # # ###################### data4 ############################################

    fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    filename = "nerccostandtrainlatencysummodel_by_N_models_with_rawinput_data_4"
    xs=[str(xlabel) for xlabel in [3000//50, 3000//25, 3000//10, 3000]]
    # ys0_l=[[ys0/60/60*0.013*64 for ys0 in [20.14, 36.68, 51.58, 101.09, 249.6, 942.14, 9847.24]]]
    ys_d4totaltraintime_l=[[sum([2.236, 4.209, 2.428, 2.51, 3.049, 3.11, 2.607, 2.533, 3.219, 3.938, 2.08, 6.05, 4.248, 12.301, 3.214, 2.71, 2.705, 8.022, 2.603, 1.915, 1.512, 1.806, 4.082, 4.392, 1.77, 2.01, 4.226, 2.933, 2.818, 1.923, 3.963, 3.588, 2.467, 2.011, 2.946, 6.876, 2.447, 2.532, 6.21, 2.114, 2.018, 2.968, 3.954, 3.589, 2.191, 2.925, 2.187, 2.762, 5.256, 2.73]), sum([2.234, 2.86, 4.443, 5.557, 3.663, 3.182, 2.515, 3.167, 3.608, 8.675, 1.584, 3.115, 2.161, 2.929, 2.93, 2.393, 3.052, 2.724, 2.692, 6.881, 2.583, 3.26, 2.743, 8.186, 2.19, 1.769, 5.494, 2.457, 2.925, 3.217, 2.459, 1.98, 1.939, 4.588, 2.421, 2.749, 3.017, 3.808, 3.288, 2.447, 3.125, 2.656, 2.631, 1.308, 2.299, 1.406, 4.316, 12.402, 4.02, 2.326]), sum([4.645, 2.372, 2.247, 2.234, 7.856, 3.166, 2.464, 2.423, 3.644, 2.517, 2.389, 2.851, 5.925, 1.914, 2.443, 2.13, 12.786, 2.294, 8.741, 2.384, 1.947, 5.934, 2.177, 2.603, 2.768, 4.504, 2.873, 2.375, 2.054, 3.018, 3.329, 2.804, 1.984, 1.965, 3.333, 2.178, 2.471, 1.786, 2.153, 1.861, 8.07, 1.919, 2.212, 2.479, 2.814, 2.522, 3.072, 3.619, 4.129, 2.732])],
                        [sum([12.156, 6.961, 11.058, 7.813, 9.737, 13.431, 27.468, 8.145, 18.968, 7.589, 5.238, 10.372, 3.881, 10.532, 5.821, 13.131, 3.743, 17.957, 9.06, 15.217, 8.647, 10.563, 5.785, 7.895, 13.402]), sum([5.677, 17.108, 7.737, 7.341, 22.429, 7.752, 4.823, 6.513, 10.254, 16.172, 7.265, 19.48, 6.355, 12.668, 8.653, 6.061, 10.186, 8.095, 12.045, 8.627, 9.406, 6.937, 6.506, 31.496, 11.758]), sum([12.591, 8.109, 20.071, 7.757, 11.948, 9.663, 14.374, 8.164, 28.058, 20.674, 14.071, 7.695, 11.86, 5.301, 7.704, 10.981, 5.641, 9.189, 6.641, 5.436, 17.11, 5.688, 8.005, 8.844, 9.692])],
                        [sum([49.414, 46.608, 101.057, 69.227, 40.1, 42.637, 48.874, 83.239, 45.403, 51.621]), sum([59.401, 73.899, 24.249, 67.144, 68.697, 49.869, 42.909, 52.019, 43.25, 101.657]), sum([72.174, 50.497, 58.942, 128.731, 53.453, 43.789, 45.134, 32.338, 60.207, 47.008])],
                        [5395.731, 5435.128, 5454.122]]
    ys_d4totaltraintimemean_l = np.array(ys_d4totaltraintime_l).mean(axis=1).tolist()
    ys_d4totaltraintimestd_l  = np.array(ys_d4totaltraintime_l).std(axis=1).tolist()
    ys_d4nerccostmean_l = [ys0/60/60*0.013*64 for ys0 in ys_d4totaltraintimemean_l]
    ys_d4nerccoststd_l  = [ys0/60/60*0.013*64 for ys0 in ys_d4totaltraintimestd_l]
    ys_d4awscostmean_l = [ys0/60/60*2.448 for ys0 in ys_d4totaltraintimemean_l]
    ys_d4awscoststd_l  = [ys0/60/60*2.448 for ys0 in ys_d4totaltraintimestd_l]
    fig, ax = plt.subplots(1, 1, figsize=(10, 4))
    # bottom = np.zeros(len(xs))
    entry_count, width = 2, 0.4
    p = ax.bar([idx-width/entry_count-width/entry_count/2 for idx, _ in enumerate(ys_d4nerccostmean_l)], ys_d4nerccostmean_l, width/entry_count, yerr=ys_d4nerccoststd_l, color='#0067ff', edgecolor="black", hatch="x", label="NERC VM (\$)")
    p2 = ax.bar([idx-width/entry_count/2 for idx, _ in enumerate(ys_d4awscostmean_l)], ys_d4awscostmean_l, width/entry_count, yerr=ys_d4awscoststd_l, color='#00ff67', edgecolor="black", hatch="|", label="AWS EC2 (\$)")
    # ax.bar_label(p)
    # ax.set_title("Training Latency by N Models with Data 3", fontsize=20)
    ax.grid()
    ax.legend(loc="upper left", prop={'size': 16})
    ax.set_xticks(list(range(len(xs))))
    ax.set_xticklabels(xs)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
    ax.set_ylabel("Resource Cost (\$)", fontsize=20)
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()


    filename = "testf1score_with_rawinput_300filter_data_4"
    fig, ax = plt.subplots(1, 1, figsize=(10, 3))
    xs=[60, 120, 300, 3000]
    labels = ("Weighted F1 Scores for All Labels","Weighted Precision","Weighted Recall",)
    ys_d4rawinputscores_l=[#models
        [#dimensions
            [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
                0.824, 0.832, 0.84
            ],
            [
                0.798, 0.807, 0.815
            ],
            [
                0.989, 0.989, 0.988
            ]
        ],
        [
            [
                0.826, 0.843, 0.829
            ],
            [
                0.822, 0.818, 0.811
            ],
            [
                0.973, 0.988, 0.976
            ]
        ],
        [
            [
                0.822, 0.802, 0.821
            ],
            [
                0.831, 0.796, 0.805
            ],
            [
                0.955, 0.937, 0.952
            ]
        ],
        [
            [
                0.822, 0.822, 0.822
            ],
            [
                0.808, 0.808, 0.808
            ],
            [
                0.928, 0.928, 0.928
            ]
        ]
    ]
    ys_array, ci_array = [], []
    for m_i in range(len(ys_d4rawinputscores_l)):
        ys_array.append(np.mean(ys_d4rawinputscores_l[m_i], axis=(1)))
        ci_array.append(1.96 * np.std(ys_d4rawinputscores_l[m_i], axis=1)/np.sqrt(len(ys_d4rawinputscores_l[m_i][0])))
    ys_array = np.array(ys_array).T
    ci_array = np.array(ci_array).T
    for idx, (ys, ci, label, mark) in enumerate(zip(ys_array, ci_array, labels, ["^", "v", ".", "*"])): # ['#f00022', '#00f022', '#ff00ff']
        ax.scatter(xs, ys, label=label, marker=mark)
        ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
    ax.set_xscale('log')
    # ax.set_xlim(0,120000)
    # ax.set_ylim(0.3,1)
    ax.set_ylabel("Prediction Metrics", fontsize=20)
    ax.grid()
    plt.legend(prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()



    filename = "testf1score_by_labels_per_model_with_rawinput_data_4"
    fig, ax = plt.subplots(1, 1, figsize=(10, 3))
    xs=[60, 120, 300, 3000]
    labels = ("Filter=300", "Filter=100")
    ys_d4nonef1score_l=[#models
        [#filter or not
            [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
                0.824, 0.832, 0.84
            ],
            [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
                0,0,0
            ]
        ],    
        [
            [
                0.826, 0.843, 0.829
            ],
            [
                0,0,0
            ]
        ],
        [
            [
                0.822, 0.802, 0.821
            ],
            [
                0,0,0
            ]
        ],
        [
            [
                0.822, 0.822, 0.822
            ],
            [
                0,0,0
            ]
        ]
    ]
    ys_array, ci_array = [], []
    for m_i in range(len(ys_d4nonef1score_l)):
        ys_array.append(np.mean(ys_d4nonef1score_l[m_i], axis=(1)))
        ci_array.append(1.96 * np.std(ys_d4nonef1score_l[m_i], axis=1)/np.sqrt(len(ys_d4nonef1score_l[m_i][0])))
    ys_array = np.array(ys_array).T
    ci_array = np.array(ci_array).T
    for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v"]):
        ax.scatter(xs, ys, label=label, marker=mark)
        ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
    ax.set_xscale('log')
    ax.set_ylim(0.3,1)
    ax.set_ylabel("F1-Score", fontsize=20)
    ax.grid()
    plt.legend(prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()


    filename = "trainlatency_by_N_models_with_rawinput_300filter_data_4"
    fig, ax = plt.subplots(1, 1, figsize=(10, 3))
    xs=[60, 120, 300, 3000]
    labels = ("Total Training Time (s) for Submodels", )
    ys_d4trainingtimerawinput_l=[#models
        [#dims
            [
                [2.236, 4.209, 2.428, 2.51, 3.049, 3.11, 2.607, 2.533, 3.219, 3.938, 2.08, 6.05, 4.248, 12.301, 3.214, 2.71, 2.705, 8.022, 2.603, 1.915, 1.512, 1.806, 4.082, 4.392, 1.77, 2.01, 4.226, 2.933, 2.818, 1.923, 3.963, 3.588, 2.467, 2.011, 2.946, 6.876, 2.447, 2.532, 6.21, 2.114, 2.018, 2.968, 3.954, 3.589, 2.191, 2.925, 2.187, 2.762, 5.256, 2.73], [2.234, 2.86, 4.443, 5.557, 3.663, 3.182, 2.515, 3.167, 3.608, 8.675, 1.584, 3.115, 2.161, 2.929, 2.93, 2.393, 3.052, 2.724, 2.692, 6.881, 2.583, 3.26, 2.743, 8.186, 2.19, 1.769, 5.494, 2.457, 2.925, 3.217, 2.459, 1.98, 1.939, 4.588, 2.421, 2.749, 3.017, 3.808, 3.288, 2.447, 3.125, 2.656, 2.631, 1.308, 2.299, 1.406, 4.316, 12.402, 4.02, 2.326], [4.645, 2.372, 2.247, 2.234, 7.856, 3.166, 2.464, 2.423, 3.644, 2.517, 2.389, 2.851, 5.925, 1.914, 2.443, 2.13, 12.786, 2.294, 8.741, 2.384, 1.947, 5.934, 2.177, 2.603, 2.768, 4.504, 2.873, 2.375, 2.054, 3.018, 3.329, 2.804, 1.984, 1.965, 3.333, 2.178, 2.471, 1.786, 2.153, 1.861, 8.07, 1.919, 2.212, 2.479, 2.814, 2.522, 3.072, 3.619, 4.129, 2.732]
            ]
        ],    
        [
            [
                [12.156, 6.961, 11.058, 7.813, 9.737, 13.431, 27.468, 8.145, 18.968, 7.589, 5.238, 10.372, 3.881, 10.532, 5.821, 13.131, 3.743, 17.957, 9.06, 15.217, 8.647, 10.563, 5.785, 7.895, 13.402], [5.677, 17.108, 7.737, 7.341, 22.429, 7.752, 4.823, 6.513, 10.254, 16.172, 7.265, 19.48, 6.355, 12.668, 8.653, 6.061, 10.186, 8.095, 12.045, 8.627, 9.406, 6.937, 6.506, 31.496, 11.758], [12.591, 8.109, 20.071, 7.757, 11.948, 9.663, 14.374, 8.164, 28.058, 20.674, 14.071, 7.695, 11.86, 5.301, 7.704, 10.981, 5.641, 9.189, 6.641, 5.436, 17.11, 5.688, 8.005, 8.844, 9.692]
            ]
        ],
        [
            [
                [49.414, 46.608, 101.057, 69.227, 40.1, 42.637, 48.874, 83.239, 45.403, 51.621], [59.401, 73.899, 24.249, 67.144, 68.697, 49.869, 42.909, 52.019, 43.25, 101.657], [72.174, 50.497, 58.942, 128.731, 53.453, 43.789, 45.134, 32.338, 60.207, 47.008]
            ]
        ],
        [
            [
                [5395.731], [5435.128], [5454.122]
            ]
        ]
    ]
    ys_array, ci_array = [], []
    for m_i in range(len(ys_d4trainingtimerawinput_l)):
        ys_array.append(np.mean(np.sum(ys_d4trainingtimerawinput_l[m_i],axis=(2,)), axis=(1)))
        ci_array.append(1.96 * np.std(np.sum(ys_d4trainingtimerawinput_l[m_i],axis=(2,)), axis=1)/np.sqrt(len(np.sum(ys_d4trainingtimerawinput_l[m_i],axis=(2,))[0])))
    ys_array = np.array(ys_array).T
    ci_array = np.array(ci_array).T
    for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v", "o", "s"]):
        ax.scatter(xs, ys, label=label, marker=mark)
        ax.errorbar(xs, ys, yerr=ci, fmt='^', capsize=10)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
    ax.set_xscale('log')
    ax.set_ylabel("Training Time(s)", fontsize=20)
    ax.grid()
    plt.legend(prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()


    filename = "trainlatency_by_labels_per_model_with_rawinput_data_4"
    fig, ax = plt.subplots(1, 1, figsize=(10, 3))
    xs=[60, 120, 300, 3000]
    labels = ("Filter=300", "Filter=100")
    ys_d4nonetrainlat_l=[#models
        [#filter or not
            [#CV [test_sample_batch_idx: 4,1,2,0,3 [shuffle_idx: 0,1,2]]
                2.236, 2.234, 4.645, 4.209, 2.86, 2.372, 2.428, 4.443, 2.247, 2.51, 5.557, 2.234, 3.049, 3.663, 7.856, 3.11, 3.182, 3.166, 2.607, 2.515, 2.464, 2.533, 3.167, 2.423, 3.219, 3.608, 3.644, 3.938, 8.675, 2.517, 2.08, 1.584, 2.389, 6.05, 3.115, 2.851, 4.248, 2.161, 5.925, 12.301, 2.929, 1.914, 3.214, 2.93, 2.443, 2.71, 2.393, 2.13, 2.705, 3.052, 12.786, 8.022, 2.724, 2.294, 2.603, 2.692, 8.741, 1.915, 6.881, 2.384, 1.512, 2.583, 1.947, 1.806, 3.26, 5.934, 4.082, 2.743, 2.177, 4.392, 8.186, 2.603, 1.77, 2.19, 2.768, 2.01, 1.769, 4.504, 4.226, 5.494, 2.873, 2.933, 2.457, 2.375, 2.818, 2.925, 2.054, 1.923, 3.217, 3.018, 3.963, 2.459, 3.329, 3.588, 1.98, 2.804, 2.467, 1.939, 1.984, 2.011, 4.588, 1.965, 2.946, 2.421, 3.333, 6.876, 2.749, 2.178, 2.447, 3.017, 2.471, 2.532, 3.808, 1.786, 6.21, 3.288, 2.153, 2.114, 2.447, 1.861, 2.018, 3.125, 8.07, 2.968, 2.656, 1.919, 3.954, 2.631, 2.212, 3.589, 1.308, 2.479, 2.191, 2.299, 2.814, 2.925, 1.406, 2.522, 2.187, 4.316, 3.072, 2.762, 12.402, 3.619, 5.256, 4.02, 4.129, 2.73, 2.326, 2.732            
            ],
            [
                2.095, 1.874, 4.702, 3.77, 1.841, 2.413, 1.934, 4.402, 2.421, 1.987, 5.203, 2.56, 3.314, 4.244, 7.664, 2.814, 2.476, 3.161, 2.738, 4.06, 2.043, 2.31, 2.776, 2.084, 3.373, 3.376, 3.647, 2.538, 8.541, 2.661, 2.131, 3.04, 2.506, 5.803, 3.799, 2.828, 2.509, 3.326, 5.685, 12.338, 2.028, 2.399, 2.51, 2.206, 2.461, 2.759, 2.868, 2.685, 2.558, 3.254, 12.503, 7.833, 2.82, 4.315, 2.766, 2.788, 8.704, 2.169, 6.495, 2.331, 1.937, 2.288, 1.655, 2.014, 2.944, 5.367, 4.179, 2.57, 1.74, 2.636, 8.317, 2.531, 1.893, 2.529, 2.151, 1.994, 2.509, 4.256, 3.953, 5.301, 1.396, 2.345, 2.472, 1.879, 2.459, 2.627, 2.219, 2.29, 4.271, 2.916, 4.337, 2.489, 3.079, 3.821, 2.391, 3.201, 1.972, 1.836, 1.925, 1.616, 4.455, 3.379, 2.595, 2.457, 3.03, 6.794, 2.886, 2.513, 2.53, 2.98, 2.471, 2.889, 3.631, 1.655, 6.477, 3.251, 2.05, 3.182, 2.316, 1.842, 2.072, 3.068, 7.88, 3.33, 3.234, 1.323, 3.89, 3.063, 3.247, 2.203, 1.401, 2.135, 1.744, 2.137, 3.258, 1.666, 1.395, 1.984, 1.866, 4.304, 2.762, 2.645, 12.962, 2.624, 5.255, 4.222, 4.203, 2.987, 2.1, 2.138
            ]
        ], 
        [
            [
                12.156, 5.677, 12.591, 6.961, 17.108, 8.109, 11.058, 7.737, 20.071, 7.813, 7.341, 7.757, 9.737, 22.429, 11.948, 13.431, 7.752, 9.663, 27.468, 4.823, 14.374, 8.145, 6.513, 8.164, 18.968, 10.254, 28.058, 7.589, 16.172, 20.674, 5.238, 7.265, 14.071, 10.372, 19.48, 7.695, 3.881, 6.355, 11.86, 10.532, 12.668, 5.301, 5.821, 8.653, 7.704, 13.131, 6.061, 10.981, 3.743, 10.186, 5.641, 17.957, 8.095, 9.189, 9.06, 12.045, 6.641, 15.217, 8.627, 5.436, 8.647, 9.406, 17.11, 10.563, 6.937, 5.688, 5.785, 6.506, 8.005, 7.895, 31.496, 8.844, 13.402, 11.758, 9.692
            ],
            [
                12.161, 5.68, 11.181, 6.54, 17.272, 7.886, 10.715, 7.483, 18.884, 7.405, 8.186, 7.045, 9.809, 22.311, 11.359, 13.818, 5.212, 9.695, 27.713, 8.151, 14.402, 8.118, 6.905, 7.806, 18.653, 9.886, 27.56, 7.684, 16.194, 20.89, 4.224, 6.94, 14.154, 10.173, 19.521, 7.262, 3.82, 6.629, 12.011, 9.966, 13.374, 5.536, 5.864, 8.844, 6.881, 13.045, 9.406, 10.546, 3.847, 10.267, 5.752, 18.465, 7.888, 8.894, 8.911, 12.071, 6.29, 14.755, 8.486, 5.05, 8.608, 9.19, 17.493, 10.4, 7.741, 6.191, 5.372, 5.943, 8.692, 7.692, 31.144, 8.131, 14.36, 10.619, 10.604
            ]
        ],   
        [
            [
                49.414, 59.401, 72.174, 46.608, 73.899, 50.497, 101.057, 24.249, 58.942, 69.227, 67.144, 128.731, 40.1, 68.697, 53.453, 42.637, 49.869, 43.789, 48.874, 42.909, 45.134, 83.239, 52.019, 32.338, 45.403, 43.25, 60.207, 51.621, 101.657, 47.008
            ],
            [
                50.013, 59.292, 73.023, 46.321, 75.352, 49.452, 100.204, 24.584, 59.266, 67.972, 66.658, 132.18, 40.797, 68.893, 53.657, 42.88, 50.518, 42.572, 48.995, 43.17, 45.268, 83.219, 53.171, 31.703, 45.601, 43.256, 59.359, 51.789, 103.644, 49.702
            ]
        ],  
        [
            [
                5395.731, 5435.128, 5454.122
            ],
            [
                5307.999, 5318.343, 5291.452
            ]
        ],
    ]
    ys_array, ci_array = [], []
    for m_i in range(len(ys_d4nonetrainlat_l)):
        local_ys_array, local_ci_array = [], []
        for f_i in range(len(ys_d4nonetrainlat_l[m_i])):
            local_ys_array.append(np.mean(ys_d4nonetrainlat_l[m_i][f_i], axis=(0)))
            local_ci_array.append(1.96 * np.std(ys_d4nonetrainlat_l[m_i][f_i], axis=0)/np.sqrt(len(ys_d4nonetrainlat_l[m_i][f_i])))
        ys_array.append(local_ys_array)
        ci_array.append(local_ci_array)
    ys_array = np.array(ys_array).T
    ci_array = np.array(ci_array).T
    for ys, ci, label, mark in zip(ys_array, ci_array, labels, ["^", "v"]):
        ax.scatter(xs, ys, label=label, marker=mark)
        ax.errorbar(xs, ys, yerr=ci, fmt='o', capsize=10, marker=mark)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Package Labels per Model", fontsize=20)
    # ax.set_ylim(0.3,1)
    ax.set_xscale('log')
    # ax.set_yscale("log")
    ax.set_ylabel("Incremental Training Time (s)", fontsize=20)
    ax.grid()
    plt.legend(prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()




    # ###########################################################################




























    # # by_N_estimators
    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "inference_latency_by_N_estimators_with_data_0\&2"
    # labels = (
    #     "1*",
    #     "1",
    #     "5",
    #     "10",
    #     "50",
    #     "100"
    # )
    # cate_values_1 = {
    #     "M80": np.array([1.68, 1.68, 1.71, 1.73, 1.91, 2.03]),
    #     "M9": np.array([0.05, 0.0495, 0.050, 0.052, 0.058, 0.064])
    # }
    # cate_values_2 = {
    #     "M89": np.array([1.72, 1.72, 1.75, 1.76, 1.92, 2.00])
    # }
    # cate_values = [cate_values_1, cate_values_2]
    # plotting(fig_path, filename, cate_values, labels)

    # # N models
    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "inference_latency_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M12-M23-M80": np.array([0.03, 0.10, 1.87]),
    #     "M11-M18-MX": np.array([0.07, 0.23, 0]),
    #     "M7-M19-MX": np.array([0.02, 0.22, 0]),
    #     "M11-M20-MX": np.array([0.22, 1.23, 0]),
    #     "M7-MX-MX": np.array([0.07, 0, 0]),
    #     "M12-MX-MX": np.array([0.16, 0, 0]),
    #     "M10-MX-MX_1": np.array([1.07, 0, 0]),
    #     "M10-MX-MX_2": np.array([0.14, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0"
    # labels = (
    #     "8_1000trees","4_2000trees","1_8000trees_64jobs","8_8000trees_64jobs","4_8000trees_64jobs"
    # )
    # cate_values_1 = {
    #     "M12-M23-M80-M12_800t-M23_400t": np.array([0.567, 4.024, 954.138, 4.745, 14.930]),
    #     "M11-M18-MX-M11_800t-M18_400t": np.array([1.965, 9.069, 0, 13.004, 37.666]),
    #     "M7-M19-MX-M17_800t-M19_400t": np.array([0.391, 9.117, 0, 2.896, 46.874]),
    #     "M11-M20-MX-M11_800t-M20_400t": np.array([5.592, 163.841, 0, 41.572, 660.138]),
    #     "M7-MX-MX-M7_800t-MX": np.array([1.020, 0, 0, 8.108, 0]),
    #     "M12-MX-MX-M12_800t-MX": np.array([4.595, 0, 0, 33.791, 0]),
    #     "M10-MX-MX_1-M10_800t-MX": np.array([71.705, 0, 0, 567.631, 0]),
    #     "M10-MX-MX_2-M10_800t-MX": np.array([3.430, 0, 0, 23.669, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M80": np.array([4.351, 19.378, 1012.601]),
    #     "M10-M20-MX_1": np.array([4.476, 19.424, 0]),
    #     "M10-M20-MX_2": np.array([4.585, 19.025, 0]),
    #     "M10-M20-MX_3": np.array([4.337, 18.938, 0]),
    #     "M10-MX-MX_1": np.array([4.997, 0, 0]),
    #     "M10-MX-MX_2": np.array([4.649, 0, 0]),
    #     "M10-MX-MX_3": np.array([6.566, 0, 0]),
    #     "M10-MX-MX_4": np.array([4.633, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_randomint10000000_109319_64jobs"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M80": np.array([32.517, 170.592, 2211.328]),
    #     "M10-M20-MX_1": np.array([30.359, 184.956, 0]),
    #     "M10-M20-MX_2": np.array([33.489, 180.434, 0]),
    #     "M10-M20-MX_3": np.array([31.810, 174.204, 0]),
    #     "M10-MX-MX_1": np.array([29.711, 0, 0]),
    #     "M10-MX-MX_2": np.array([32.490, 0, 0]),
    #     "M10-MX-MX_3": np.array([30.686, 0, 0]),
    #     "M10-MX-MX_4": np.array([33.413, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_4jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([3.763, 15.673, 62.270, 1609.677]),
    #     "M10-M20-M40-MX_1": np.array([3.946, 15.368, 64.685, 0]),
    #     "M10-M20-M40-MX_2": np.array([3.891, 15.265, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([3.959, 16.010, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([3.846, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([3.881, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([3.975, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([3.967, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_8jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([3.916, 15.770, 62.100, 870.774]),
    #     "M10-M20-M40-MX_1": np.array([4.182, 15.187, 64.122, 0]),
    #     "M10-M20-M40-MX_2": np.array([3.854, 15.509, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([3.911, 15.700, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([3.835, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([3.966, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([3.861, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([3.918, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_16jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([3.978, 15.304, 63.182, 934.183]),
    #     "M10-M20-M40-MX_1": np.array([4.342, 14.966, 67.809, 0]),
    #     "M10-M20-M40-MX_2": np.array([3.833, 15.322, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([3.941, 15.755, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([4.139, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([4.078, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([3.850, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([3.854, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_32jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([4.062, 16.344, 69.733, 953.460]),
    #     "M10-M20-M40-MX_1": np.array([4.249, 16.941, 70.464, 0]),
    #     "M10-M20-M40-MX_2": np.array([4.023, 16.753, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([4.315, 16.776, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([4.178, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([3.963, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([4.115, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([4.303, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)



    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_64jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([4.141, 17.000, 72.818, 941.116]),
    #     "M10-M20-M40-MX_1": np.array([4.319, 16.714, 70.411, 0]),
    #     "M10-M20-M40-MX_2": np.array([4.381, 16.409, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([4.169, 17.238, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([4.085, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([4.419, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([4.298, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([4.306, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)



    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "train_latency_by_N_models_with_data_0_zeros_109319_128jobs"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([4.458, 18.530, 70.480, 989.714]),
    #     "M10-M20-M40-MX_1": np.array([4.408, 17.776, 70.406, 0]),
    #     "M10-M20-M40-MX_2": np.array([4.385, 18.414, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([4.235, 18.987, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([4.712, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([4.650, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([5.174, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([4.713, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "input_size_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M12-M23-M80": np.array([510, 4183, 109319]),
    #     "M11-M18-MX": np.array([3691, 13930, 0]),
    #     "M7-M19-MX": np.array([454, 13182, 0]),
    #     "M11-M20-MX": np.array([13495, 78424, 0]),
    #     "M7-MX-MX": np.array([3560, 0, 0]),
    #     "M12-MX-MX": np.array([9740, 0, 0]),
    #     "M10-MX-MX_1": np.array([70225, 0, 0]),
    #     "M10-MX-MX_2": np.array([8498, 0, 0]),
    # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "input_size_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M80": np.array([13664, 27329, 109319]),
    #     "M10-M20-MX_1": np.array([13664, 27329, 0]),
    #     "M10-M20-MX_2": np.array([13664, 27329, 0]),
    #     "M10-M20-MX_3": np.array([13664, 27329, 0]),
    #     "M10-MX-MX_1": np.array([13664, 0, 0]),
    #     "M10-MX-MX_2": np.array([13664, 0, 0]),
    #     "M10-MX-MX_3": np.array([13664, 0, 0]),
    #     "M10-MX-MX_4": np.array([13664, 0, 0]),
    # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "Traintime_by_Inputsize_for_Fixed_10_Labels"
    

    # data = np.array([])
    # labels = [None]*len(data)
    # cate_values_1 = {
    #     "": data
    # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "Numoftrees_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M12-M23-M80": np.array([1200, 2300, 8000]),
    #     "M11-M18-MX": np.array([1100, 1800, 0]),
    #     "M7-M19-MX": np.array([700, 1900, 0]),
    #     "M11-M20-MX": np.array([1100, 2000, 0]),
    #     "M7-MX-MX": np.array([700, 0, 0]),
    #     "M12-MX-MX": np.array([1200, 0, 0]),
    #     "M10-MX-MX_1": np.array([1000, 0, 0]),
    #     "M10-MX-MX_2": np.array([1000, 0, 0]),
    # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "Numoflabels_by_N_models_with_data_0"
    # labels = (
    #     "8","4","1"
    # )
    # cate_values_1 = {
    #     "M12-M23-M80": np.array([12, 23, 80]),
    #     "M11-M18-MX": np.array([11, 18, 0]),
    #     "M7-M19-MX": np.array([7, 19, 0]),
    #     "M11-M20-MX": np.array([11, 20, 0]),
    #     "M7-MX-MX": np.array([7, 0, 0]),
    #     "M12-MX-MX": np.array([12, 0, 0]),
    #     "M10-MX-MX_1": np.array([10, 0, 0]),
    #     "M10-MX-MX_2": np.array([10, 0, 0]),
    # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "outputsize_by_N_models_with_rawinput_data_0"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([13664, 27329, 54659, 109319]),
    #     "M10-M20-M40-MX_1": np.array([13664, 27329, 54659, 0]),
    #     "M10-M20-M40-MX_2": np.array([13664, 27329, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([13664, 27329, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([13664, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([13664, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([13664, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([13664, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "-": np.array([0, 0, 0])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels)



    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_N_models_with_rawinput_data_0"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([2.407, 18.840, 143.565, 1090.042]),
    #     "M10-M20-M40-MX_1": np.array([2.396, 18.788, 143.717, 0]),
    #     "M10-M20-M40-MX_2": np.array([2.413, 19.016, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([2.407, 18.862, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([2.406, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([2.396, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([2.440, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([2.396, 0, 0, 0]),
    # }
    # cate_values_2 = {
    #     "Estimated": np.array([8*2.4, 4*2.4*2*(4*math.log(25*20)/math.log(25*10)), 2*19.0*2*(4*math.log(25*40)/math.log(25*20)), 143.5*2*(4*math.log(25*80)/math.log(25*40))])
    # }
    # cate_values = [cate_values_1, cate_values_2]
    # plotting(fig_path, filename, cate_values, labels, xaxis_label="Number of Models", yaxis_label="Training Time(s)", title="Train Latency by N Models with the Same Train Dataset")

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatencypermodel_by_N_models_with_rawinput_data_0"
    # labels = (
    #     "10","20","40","80"
    # )
    # cate_values_1 = {
    #     "M10-M20-M40-M80": np.array([2.407, 18.840, 143.565, 1090.042]),
    #     # "M10-M20-M40-MX_1": np.array([2.396, 18.788, 143.717, 0]),
    #     # "M10-M20-M40-MX_2": np.array([2.413, 19.016, 0, 0]),
    #     # "M10-M20-M40-MX_3": np.array([2.407, 18.862, 0, 0]),
    #     # "M10-MX-MX-MX_1": np.array([2.406, 0, 0, 0]),
    #     # "M10-MX-MX-MX_2": np.array([2.396, 0, 0, 0]),
    #     # "M10-MX-MX-MX_3": np.array([2.440, 0, 0, 0]),
    #     # "M10-MX-MX-MX_4": np.array([2.396, 0, 0, 0]),
    # }
    # # cate_values_2 = {
    # #     "Estimated": np.array([8*2.4, 4*2.4*2*(4*math.log(25*20)/math.log(25*10)), 2*19.0*2*(4*math.log(25*40)/math.log(25*20)), 143.5*2*(4*math.log(25*80)/math.log(25*40))])
    # # }
    # cate_values = [cate_values_1]
    # plotting(fig_path, filename, cate_values, labels, xaxis_label="Number of Labels Per Model", yaxis_label="Training Time(s)", title="Train Latency by N Models with the Same Train Dataset")


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatencypermodel_by_N_models_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=["10","20","40","80"]
    # # labels = ("Observed", "Estimated")
    # ys=[2.407, 18.840, 143.565, 1090.042]
    # # for ys, label in zip(ys_l, labels):
    # p = ax.bar(xs, ys)
    # ax.bar_label(p)
    # ax.set_xticks(list(range(len(xs))))
    # ax.set_xticklabels(xs)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Number of Packages", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "inferencelatency_by_N_models_with_rawinput_data_0"
    # labels = (
    #     "8","4","2","1"
    # )
    # cate_values_1_l = [{
    #     "M10-M20-M40-M80": np.array([0.026, 0.099, 0.179, 2.072]),
    #     "M10-M20-M40-MX_1": np.array([0.075, 0.090, 1.785, 0]),
    #     "M10-M20-M40-MX_2": np.array([0.025, 0.398, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([0.067, 1.325, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([0.236, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([0.157, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([1.110, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([0.149, 0, 0, 0]),
    # },{
    #     "M10-M20-M40-M80": np.array([0.026, 0.092, 0.171, 1.954]),
    #     "M10-M20-M40-MX_1": np.array([0.073, 0.085, 1.713, 0]),
    #     "M10-M20-M40-MX_2": np.array([0.023, 0.374, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([0.064, 1.236, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([0.234, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([0.148, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([1.086, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([0.142, 0, 0, 0]),
    # },{
    #     "M10-M20-M40-M80": np.array([0.024, 0.092, 0.171, 2.192]),
    #     "M10-M20-M40-MX_1": np.array([0.073, 0.088, 1.676, 0]),
    #     "M10-M20-M40-MX_2": np.array([0.023, 0.373, 0, 0]),
    #     "M10-M20-M40-MX_3": np.array([0.066, 1.224, 0, 0]),
    #     "M10-MX-MX-MX_1": np.array([0.235, 0, 0, 0]),
    #     "M10-MX-MX-MX_2": np.array([0.149, 0, 0, 0]),
    #     "M10-MX-MX-MX_3": np.array([1.072, 0, 0, 0]),
    #     "M10-MX-MX-MX_4": np.array([0.143, 0, 0, 0]),
    # }]
    # cate_values_1 = {"M10-M20-M40-M80": [],
    #     "M10-M20-M40-MX_1": [],
    #     "M10-M20-M40-MX_2": [],
    #     "M10-M20-M40-MX_3": [],
    #     "M10-MX-MX-MX_1": [],
    #     "M10-MX-MX-MX_2": [],
    #     "M10-MX-MX-MX_3": [],
    #     "M10-MX-MX-MX_4": []}
    # cate_stds_1 = {"errors": []}
    # for d in cate_values_1_l:
    #     for k, v in cate_values_1.items():
    #         cate_values_1[k].append(d[k])
    # for k, v in cate_values_1.items():
    #     cate_values_1[k] = np.vstack(cate_values_1[k])
    #     cate_stds_1[k] = np.var(cate_values_1[k], axis=0)
    #     cate_values_1[k] = np.mean(cate_values_1[k], axis=0)
    # cate_values = [cate_values_1]
    # cate_stds = [cate_stds_1]
    # plotting(fig_path, filename, cate_values, labels, cates_stds=cate_stds, xaxis_label="Number of Labels Per Model", yaxis_label="Inference Time(s)", title="Inference Latency by N Models with the Same Test Dataset")









    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_input_size_and_by_N_models_with_data_0_3d"
    # # Fixing random state for reproducibility
    # np.random.seed(19680801)

    # fig = plt.figure()
    # ax = fig.add_subplot(projection='3d')

    # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319, 
    #     13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319, 
    #     13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319, 
    #     13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]   # input sizes
    # ys=[80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 
    #     40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 
    #     20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 
    #     10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]   # number of labels
    # zs=[1.227, 2.012, 3.810, 5.275, 9.333, 12.124, 17.675, 34.274, 67.655, 136.472, 274.488, 546.612, 1096.709, 
    #     0.315, 0.550, 1.027, 1.514, 2.580, 3.246, 4.596, 8.739, 17.492, 34.972, 70.854, 143.565, 286.711, 
    #     0.229, 0.290, 0.406, 0.451, 0.700, 0.928, 1.255, 2.366, 4.464, 9.328, 18.840, 39.945, 80.171, 
    #     0.102, 0.103, 0.143, 0.156, 0.226, 0.278, 0.392, 0.664, 1.205, 2.407, 5.449, 11.784, 24.566]   # training latency
    # # xs=[13, 106, 854, 13, 106, 854, 13, 106, 854, 13, 106, 854]   # input sizes
    # # ys=[80, 80, 80, 40, 40, 40, 20, 20, 20, 10, 10, 10]   # number of labels
    # # zs=[1.227, 2.012, 9.333, 0.315, 0.550, 2.580, 0.229, 0.290, 0.700, 0.102, 0.103, 0.226]   # training latency

    # ax.scatter(xs, ys, zs, marker='o')

    # ax.set_xlabel('Feature Dimensions')
    # ax.set_ylabel('Labels Per Model')
    # ax.set_zlabel('Training Latency')

    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()




    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_inputsize_and_by_samplesize_with_data_3_25_3d"

    # fig = plt.figure()
    # ax = fig.add_subplot(projection='3d')

    # samplesize=[357.0, 419.0, 377.0, 357.0, 356.0, 336.0, 360.0, 339.0, 297.0, 338.0, 358.0, 378.0, 393.0, 397.0, 337.0, 375.0, 353.0, 374.0, 294.0, 355.0, 339.0, 330.0, 375.0, 375.0, 317.0, 334.0, 355.0, 335.0, 333.0, 396.0, 378.0, 349.0, 275.0, 395.0, 339.0, 318.0, 399.0, 299.0, 355.0, 357.0, 359.0, 336.0, 337.0, 378.0, 334.0, 334.0, 380.0, 355.0, 337.0, 375.0, 379.0, 357.0, 328.0, 396.0, 317.0, 331.0, 392.0, 358.0, 359.0, 378.0, 356.0, 338.0, 380.0, 399.0, 360.0, 299.0, 374.0, 359.0, 412.0, 278.0, 259.0, 259.0, 357.0, 357.0, 351.0, 395.0, 418.0, 335.0, 398.0, 357.0, 377.0, 313.0, 353.0, 391.0, 333.0, 375.0, 351.0, 316.0, 358.0, 334.0, 337.0, 353.0, 357.0, 318.0, 340.0, 320.0, 372.0, 298.0, 319.0, 418.0, 295.0, 397.0, 400.0, 359.0, 377.0, 395.0, 311.0, 337.0, 357.0, 339.0, 338.0, 319.0, 377.0, 379.0, 280.0, 398.0, 336.0, 332.0, 355.0, 318.0, 375.0, 376.0, 359.0, 336.0, 397.0, 394.0, 378.0, 358.0, 299.0, 337.0, 335.0, 359.0, 375.0, 397.0, 336.0, 339.0, 372.0, 398.0, 356.0, 275.0, 371.0, 296.0, 279.0, 299.0, 336.0, 375.0, 353.0, 336.0, 355.0, 297.0, 335.0, 356.0, 355.0, 353.0, 315.0, 399.0, 276.0, 339.0, 338.0, 359.0, 396.0, 300.0, 355.0, 351.0, 371.0, 337.0, 400.0, 377.0, 396.0, 377.0, 313.0, 330.0, 319.0, 355.0, 314.0, 393.0, 316.0, 378.0, 394.0, 339.0, 400.0, 400.0, 297.0, 340.0, 256.0, 375.0, 357.0, 356.0, 395.0, 398.0, 357.0, 372.0, 336.0, 335.0, 397.0, 358.0, 359.0, 360.0, 359.0, 371.0, 416.0, 375.0, 359.0, 335.0, 358.0, 358.0, 340.0, 336.0, 353.0, 354.0, 376.0, 299.0, 360.0, 334.0, 259.0, 318.0, 314.0, 380.0, 320.0, 377.0, 351.0, 340.0, 377.0, 337.0, 358.0, 319.0, 379.0, 370.0, 339.0, 393.0, 314.0, 378.0, 356.0, 338.0, 355.0, 299.0, 359.0, 392.0, 373.0, 359.0, 337.0, 358.0, 358.0, 298.0, 397.0, 331.0, 399.0, 354.0, 395.0, 376.0, 357.0, 360.0, 395.0, 359.0, 355.0, 357.0, 315.0, 298.0, 359.0, 319.0, 395.0, 355.0, 357.0, 299.0, 337.0, 295.0, 377.0, 355.0, 414.0, 350.0, 394.0, 332.0, 378.0, 316.0, 355.0, 376.0, 375.0, 378.0, 352.0, 315.0, 316.0, 393.0, 354.0, 339.0, 358.0, 259.0, 319.0, 299.0, 353.0, 360.0, 336.0, 319.0, 357.0, 340.0, 337.0, 319.0, 339.0, 316.0, 394.0, 355.0, 360.0, 379.0, 395.0, 348.0, 396.0, 357.0, 331.0, 376.0, 367.0, 338.0, 336.0, 376.0, 360.0, 357.0, 317.0, 338.0, 376.0, 296.0, 394.0, 333.0, 320.0, 313.0, 336.0, 376.0, 354.0, 378.0, 356.0, 360.0, 356.0, 377.0, 420.0, 391.0, 338.0, 356.0, 319.0, 376.0, 415.0, 339.0, 376.0, 337.0, 300.0, 400.0, 335.0, 396.0, 359.0, 319.0, 319.0, 298.0, 396.0, 310.0, 392.0, 380.0, 375.0, 336.0, 357.0, 355.0, 359.0, 296.0, 275.0, 291.0]
    # dimensions=[10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0]
    # traintimes=[1.548, 1.972, 1.678, 1.549, 1.506, 1.4, 1.53, 1.524, 1.134, 1.439, 1.493, 1.616, 1.877, 1.758, 1.373, 1.693, 1.508, 1.604, 1.137, 1.537, 1.426, 1.388, 1.682, 1.66, 1.287, 1.377, 1.376, 1.378, 1.366, 1.675, 1.674, 1.501, 0.944, 1.82, 1.402, 1.184, 1.811, 1.143, 1.468, 1.494, 1.532, 1.363, 1.395, 1.732, 1.452, 1.377, 1.681, 1.506, 1.432, 1.783, 1.697, 1.57, 1.383, 1.852, 1.269, 1.378, 1.771, 1.502, 1.536, 1.673, 1.527, 1.397, 1.689, 1.895, 1.529, 1.178, 1.715, 1.525, 1.945, 1.027, 0.927, 0.907, 0.524, 0.414, 0.458, 0.572, 0.513, 0.433, 0.582, 0.466, 0.482, 0.443, 0.426, 0.532, 0.463, 0.677, 0.457, 0.342, 0.471, 0.431, 0.459, 0.437, 0.481, 0.315, 0.518, 0.437, 0.498, 0.374, 0.431, 0.539, 0.413, 0.52, 0.552, 0.463, 0.483, 0.641, 0.402, 0.473, 0.417, 0.456, 0.59, 0.371, 0.474, 0.505, 0.366, 0.457, 0.608, 0.647, 0.429, 0.604, 0.745, 0.482, 0.507, 0.495, 0.566, 0.476, 0.502, 0.491, 0.507, 0.463, 0.478, 0.483, 0.525, 0.552, 0.385, 0.465, 0.493, 0.503, 0.496, 0.319, 0.533, 0.309, 0.386, 0.43, 0.388, 0.419, 0.41, 0.375, 0.367, 0.248, 0.341, 0.353, 0.279, 0.36, 0.332, 0.325, 0.287, 0.355, 0.254, 0.364, 0.443, 0.223, 0.369, 0.395, 0.444, 0.327, 0.41, 0.364, 0.419, 0.39, 0.36, 0.336, 0.349, 0.412, 0.326, 0.426, 0.386, 0.376, 0.416, 0.372, 0.351, 0.433, 0.316, 0.295, 0.263, 0.41, 0.295, 0.378, 0.401, 0.378, 0.352, 0.459, 0.33, 0.36, 0.44, 0.368, 0.392, 0.368, 0.371, 0.429, 0.439, 0.402, 0.405, 0.368, 0.373, 0.415, 0.393, 0.347, 0.381, 0.395, 0.418, 0.317, 0.398, 0.383, 0.283, 0.382, 0.817, 1.041, 0.804, 0.986, 0.93, 0.882, 0.999, 0.869, 0.955, 0.811, 1.022, 1.016, 0.815, 1.124, 0.773, 1.009, 0.96, 0.868, 0.927, 0.736, 0.946, 1.084, 1.014, 0.95, 0.839, 0.937, 0.933, 0.72, 1.096, 0.818, 1.11, 0.938, 1.059, 1.018, 0.937, 0.935, 1.066, 0.962, 0.94, 0.938, 0.817, 0.733, 0.938, 0.795, 1.13, 0.947, 0.938, 0.713, 0.849, 0.733, 1.012, 0.927, 1.196, 0.959, 1.086, 0.874, 1.018, 0.8, 0.948, 1.064, 1.012, 1.031, 0.955, 0.785, 0.823, 1.117, 0.95, 0.865, 0.951, 0.611, 0.808, 0.721, 2.067, 2.165, 2.045, 1.779, 2.155, 2.015, 2.009, 1.809, 1.878, 1.81, 2.49, 2.053, 2.102, 2.298, 2.488, 2.114, 2.432, 2.055, 2.033, 2.26, 2.266, 2.04, 1.947, 2.254, 2.177, 2.068, 1.752, 1.93, 2.272, 1.633, 2.561, 1.967, 1.706, 1.798, 1.99, 2.268, 2.084, 2.286, 2.081, 2.239, 2.183, 2.288, 2.754, 2.578, 1.886, 2.121, 1.74, 2.251, 2.749, 2.034, 2.298, 1.971, 1.607, 2.479, 1.97, 2.514, 2.083, 1.741, 1.734, 1.615, 2.539, 1.709, 2.469, 2.317, 2.256, 1.859, 2.158, 2.065, 2.091, 1.576, 1.409, 1.549]
    
    # colo = traintimes 
    # color_map = cm.ScalarMappable(cmap=cm.summer) 
    # color_map.set_array(colo) 
    # colo_normalized = [c/max(colo) for c in colo]
    # # ax.plot_trisurf(samplesize, dimensions, traintimes, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    # ax.scatter(samplesize, dimensions, traintimes, facecolors=cm.summer(colo_normalized), edgecolor=cm.summer(colo_normalized), alpha=1)
    # # ax.scatter(samplesize, dimensions, facecolors=cm.PiYG(colo_normalized), edgecolor=cm.PiYG(colo_normalized), alpha=1)
    # # plt.colorbar(color_map,label='Training Latency (s)') 

    # ax.set_xlabel('Sample Size')
    # ax.set_ylabel('Feature Dimension Size')
    # ax.set_zlabel('Training Latency')
    # # ax.zaxis.labelpad = -4
    # ax.view_init(azim=270, elev=0)

    # # plt.show()
    # # fig.set_size_inches(6, 8)
    # fig.tight_layout()
    # # fig.subplots_adjust(left=-2) 
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()






    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_inputsize_and_by_samplesize_with_data_3_10_3d"

    # fig = plt.figure()
    # ax = fig.add_subplot(projection='3d')

    # samplesize=[856.0, 872.0, 896.0, 815.0, 806.0, 916.0, 849.0, 923.0, 895.0, 851.0, 773.0, 845.0, 974.0, 917.0, 851.0, 864.0, 893.0, 868.0, 874.0, 914.0, 794.0, 910.0, 770.0, 871.0, 750.0, 909.0, 862.0, 689.0, 655.0, 634.0, 892.0, 884.0, 794.0, 763.0, 789.0, 927.0, 754.0, 894.0, 886.0, 977.0, 927.0, 850.0, 904.0, 773.0, 835.0, 873.0, 836.0, 871.0, 814.0, 915.0, 773.0, 814.0, 811.0, 910.0, 908.0, 888.0, 912.0, 733.0, 715.0, 674.0, 930.0, 910.0, 874.0, 893.0, 917.0, 847.0, 833.0, 836.0, 929.0, 850.0, 851.0, 891.0, 779.0, 889.0, 807.0, 791.0, 828.0, 835.0, 850.0, 890.0, 933.0, 906.0, 767.0, 873.0, 863.0, 829.0, 749.0, 737.0, 715.0, 694.0, 872.0, 869.0, 851.0, 786.0, 807.0, 828.0, 897.0, 883.0, 914.0, 863.0, 810.0, 830.0, 883.0, 936.0, 808.0, 813.0, 912.0, 830.0, 869.0, 797.0, 936.0, 896.0, 850.0, 810.0, 859.0, 854.0, 892.0, 694.0, 714.0, 733.0, 856.0, 801.0, 855.0, 814.0, 832.0, 829.0, 846.0, 817.0, 933.0, 823.0, 891.0, 777.0, 910.0, 868.0, 890.0, 896.0, 839.0, 908.0, 856.0, 870.0, 767.0, 824.0, 949.0, 889.0, 856.0, 830.0, 893.0, 751.0, 735.0, 691.0]
    # dimensions=[10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0]
    # traintimes=[6.934, 7.13, 7.448, 6.346, 6.214, 7.82, 6.868, 7.988, 7.506, 6.899, 5.759, 6.852, 8.728, 7.82, 6.87, 7.046, 7.462, 7.118, 7.131, 7.794, 6.013, 7.739, 5.673, 7.187, 5.464, 7.775, 7.105, 4.659, 4.217, 3.96, 1.521, 2.004, 1.224, 1.27, 1.189, 1.542, 1.645, 1.961, 1.451, 1.801, 1.53, 2.028, 1.56, 1.175, 1.335, 1.447, 1.267, 1.423, 1.277, 1.477, 1.237, 1.277, 1.63, 2.064, 1.485, 1.467, 1.533, 1.119, 1.081, 0.996, 1.153, 1.545, 1.075, 1.082, 1.183, 1.029, 0.955, 0.977, 1.205, 1.024, 1.012, 1.09, 0.904, 1.09, 0.992, 1.356, 0.989, 1.018, 0.989, 1.109, 1.207, 1.137, 0.887, 1.086, 1.055, 1.012, 0.945, 0.968, 0.865, 1.056, 4.196, 4.138, 3.913, 3.469, 3.61, 3.689, 4.418, 4.229, 4.42, 4.087, 3.536, 3.654, 4.267, 4.571, 3.493, 3.631, 4.295, 3.689, 4.16, 3.438, 4.531, 4.401, 3.872, 3.51, 3.968, 3.897, 4.2, 2.776, 2.812, 2.971, 10.203, 9.079, 10.084, 9.181, 9.764, 9.593, 10.081, 9.308, 12.051, 9.63, 11.009, 8.458, 11.548, 10.519, 10.946, 11.1, 9.797, 11.51, 10.125, 10.545, 8.346, 9.56, 12.463, 11.048, 10.171, 9.644, 11.024, 7.95, 7.699, 6.843]
    
    # colo = traintimes 
    # color_map = cm.ScalarMappable(cmap=cm.summer) 
    # color_map.set_array(colo) 
    # colo_normalized = [c/max(colo) for c in colo]
    # # ax.plot_trisurf(samplesize, dimensions, traintimes, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    # ax.scatter(samplesize, dimensions, traintimes, facecolors=cm.summer(colo_normalized), edgecolor=cm.summer(colo_normalized), alpha=1)
    # # ax.scatter(samplesize, dimensions, facecolors=cm.PiYG(colo_normalized), edgecolor=cm.PiYG(colo_normalized), alpha=1)
    # # plt.colorbar(color_map,label='Training Latency (s)') 

    # ax.set_xlabel('Sample Size')
    # ax.set_ylabel('Feature Dimension Size')
    # ax.set_zlabel('Training Latency')
    # # ax.zaxis.labelpad = -4
    # ax.view_init(azim=180, elev=0)

    # # plt.show()
    # # fig.set_size_inches(6, 8)
    # fig.tight_layout()
    # # fig.subplots_adjust(left=-2) 
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()





    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainf1score_by_inputsize_and_by_samplesize_with_data_3_heat"

    # fig = plt.figure()
    # ax = fig.add_subplot()

    # samplesize=[
    #     336.0, 333.0, 371.0, 373.0, 256.0, 379.0, 356.0, 379.0, 357.0, 353.0, 297.0, 358.0, 378.0, 336.0, 358.0, 339.0, 376.0, 378.0, 359.0, 335.0, 377.0, 372.0, 374.0, 399.0, 313.0, 379.0, 318.0, 334.0, 394.0, 374.0, 333.0, 379.0, 275.0, 374.0, 373.0, 349.0, 338.0, 378.0, 396.0, 378.0, 318.0, 375.0, 379.0, 353.0, 375.0, 398.0, 355.0, 255.0, 319.0, 335.0, 337.0, 360.0, 356.0, 380.0, 336.0, 377.0, 334.0, 336.0, 359.0, 360.0, 379.0, 360.0, 359.0, 336.0, 360.0, 372.0, 357.0, 376.0, 339.0, 296.0, 294.0, 257.0, 357.0, 419.0, 377.0, 357.0, 356.0, 336.0, 360.0, 339.0, 297.0, 338.0, 358.0, 378.0, 393.0, 397.0, 337.0, 375.0, 353.0, 374.0, 294.0, 355.0, 339.0, 330.0, 375.0, 375.0, 317.0, 334.0, 355.0, 335.0, 333.0, 396.0, 378.0, 349.0, 275.0, 395.0, 339.0, 318.0, 399.0, 299.0, 355.0, 357.0, 359.0, 336.0, 337.0, 378.0, 334.0, 334.0, 380.0, 355.0, 337.0, 375.0, 379.0, 357.0, 328.0, 396.0, 317.0, 331.0, 392.0, 358.0, 359.0, 378.0, 356.0, 338.0, 380.0, 399.0, 360.0, 299.0, 374.0, 359.0, 412.0, 278.0, 259.0, 259.0, 357.0, 357.0, 351.0, 395.0, 418.0, 335.0, 398.0, 357.0, 377.0, 313.0, 353.0, 391.0, 333.0, 375.0, 351.0, 316.0, 358.0, 334.0, 337.0, 353.0, 357.0, 318.0, 340.0, 320.0, 372.0, 298.0, 319.0, 418.0, 295.0, 397.0, 400.0, 359.0, 377.0, 395.0, 311.0, 337.0, 357.0, 339.0, 338.0, 319.0, 377.0, 379.0, 280.0, 398.0, 336.0, 332.0, 355.0, 318.0, 375.0, 376.0, 359.0, 336.0, 397.0, 394.0, 378.0, 358.0, 299.0, 337.0, 335.0, 359.0, 375.0, 397.0, 336.0, 339.0, 372.0, 398.0, 356.0, 275.0, 371.0, 296.0, 279.0, 299.0, 336.0, 375.0, 353.0, 336.0, 355.0, 297.0, 335.0, 356.0, 355.0, 353.0, 315.0, 399.0, 276.0, 339.0, 338.0, 359.0, 396.0, 300.0, 355.0, 351.0, 371.0, 337.0, 400.0, 377.0, 396.0, 377.0, 313.0, 330.0, 319.0, 355.0, 314.0, 393.0, 316.0, 378.0, 394.0, 339.0, 400.0, 400.0, 297.0, 340.0, 256.0, 375.0, 357.0, 356.0, 395.0, 398.0, 357.0, 372.0, 336.0, 335.0, 397.0, 358.0, 359.0, 360.0, 359.0, 371.0, 416.0, 375.0, 359.0, 335.0, 358.0, 358.0, 340.0, 336.0, 353.0, 354.0, 376.0, 299.0, 360.0, 334.0, 259.0, 318.0, 314.0, 380.0, 320.0, 377.0, 351.0, 340.0, 377.0, 337.0, 358.0, 319.0, 379.0, 370.0, 339.0, 393.0, 314.0, 378.0, 356.0, 338.0, 355.0, 299.0, 359.0, 392.0, 373.0, 359.0, 337.0, 358.0, 358.0, 298.0, 397.0, 331.0, 399.0, 354.0, 395.0, 376.0, 357.0, 360.0, 395.0, 359.0, 355.0, 357.0, 315.0, 298.0, 359.0, 319.0, 395.0, 355.0, 357.0, 299.0, 337.0, 295.0, 377.0, 355.0, 414.0, 350.0, 394.0, 332.0, 378.0, 316.0, 355.0, 376.0, 375.0, 378.0, 352.0, 315.0, 316.0, 393.0, 354.0, 339.0, 358.0, 259.0, 319.0, 299.0, 353.0, 360.0, 336.0, 319.0, 357.0, 340.0, 337.0, 319.0, 339.0, 316.0, 394.0, 355.0, 360.0, 379.0, 395.0, 348.0, 396.0, 357.0, 331.0, 376.0, 367.0, 338.0, 336.0, 376.0, 360.0, 357.0, 317.0, 338.0, 376.0, 296.0, 394.0, 333.0, 320.0, 313.0, 336.0, 376.0, 354.0, 378.0, 356.0, 360.0, 356.0, 377.0, 420.0, 391.0, 338.0, 356.0, 319.0, 376.0, 415.0, 339.0, 376.0, 337.0, 300.0, 400.0, 335.0, 396.0, 359.0, 319.0, 319.0, 298.0, 396.0, 310.0, 392.0, 380.0, 375.0, 336.0, 357.0, 355.0, 359.0, 296.0, 275.0, 291.0,
    #     933.0, 752.0, 934.0, 732.0, 873.0, 834.0, 837.0, 887.0, 784.0, 913.0, 949.0, 872.0, 865.0, 887.0, 832.0, 895.0, 772.0, 835.0, 848.0, 830.0, 889.0, 849.0, 952.0, 965.0, 828.0, 873.0, 811.0, 732.0, 657.0, 676.0, 856.0, 872.0, 896.0, 815.0, 806.0, 916.0, 849.0, 923.0, 895.0, 851.0, 773.0, 845.0, 974.0, 917.0, 851.0, 864.0, 893.0, 868.0, 874.0, 914.0, 794.0, 910.0, 770.0, 871.0, 750.0, 909.0, 862.0, 689.0, 655.0, 634.0, 892.0, 884.0, 794.0, 763.0, 789.0, 927.0, 754.0, 894.0, 886.0, 977.0, 927.0, 850.0, 904.0, 773.0, 835.0, 873.0, 836.0, 871.0, 814.0, 915.0, 773.0, 814.0, 811.0, 910.0, 908.0, 888.0, 912.0, 733.0, 715.0, 674.0, 930.0, 910.0, 874.0, 893.0, 917.0, 847.0, 833.0, 836.0, 929.0, 850.0, 851.0, 891.0, 779.0, 889.0, 807.0, 791.0, 828.0, 835.0, 850.0, 890.0, 933.0, 906.0, 767.0, 873.0, 863.0, 829.0, 749.0, 737.0, 715.0, 694.0, 872.0, 869.0, 851.0, 786.0, 807.0, 828.0, 897.0, 883.0, 914.0, 863.0, 810.0, 830.0, 883.0, 936.0, 808.0, 813.0, 912.0, 830.0, 869.0, 797.0, 936.0, 896.0, 850.0, 810.0, 859.0, 854.0, 892.0, 694.0, 714.0, 733.0, 856.0, 801.0, 855.0, 814.0, 832.0, 829.0, 846.0, 817.0, 933.0, 823.0, 891.0, 777.0, 910.0, 868.0, 890.0, 896.0, 839.0, 908.0, 856.0, 870.0, 767.0, 824.0, 949.0, 889.0, 856.0, 830.0, 893.0, 751.0, 735.0, 691.0
    # ]
    # dimensions=[
    #     1919.0, 812.0, 5972.0, 1005.0, 504.0, 1338.0, 854.0, 5483.0, 2373.0, 209.0, 2555.0, 701.0, 570.0, 264.0, 1060.0, 359.0, 1426.0, 558.0, 657.0, 791.0, 274.0, 1130.0, 2081.0, 3647.0, 483.0, 1309.0, 1076.0, 446.0, 570.0, 526.0, 2215.0, 934.0, 1170.0, 712.0, 447.0, 683.0, 787.0, 2179.0, 995.0, 6628.0, 651.0, 230.0, 604.0, 285.0, 3025.0, 1879.0, 6594.0, 281.0, 700.0, 2436.0, 6490.0, 2996.0, 2326.0, 2445.0, 6272.0, 399.0, 272.0, 1884.0, 2149.0, 1794.0, 460.0, 891.0, 1012.0, 3686.0, 499.0, 1250.0, 716.0, 997.0, 336.0, 632.0, 1143.0, 252.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0,
    #     3745.0, 1473.0, 1918.0, 6098.0, 2951.0, 4813.0, 2525.0, 3981.0, 6887.0, 8209.0, 3656.0, 2869.0, 826.0, 7237.0, 1211.0, 5554.0, 1921.0, 2998.0, 2778.0, 11220.0, 7947.0, 2094.0, 1190.0, 2327.0, 3749.0, 2375.0, 3150.0, 1771.0, 1453.0, 3085.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0
    # ]   # number of labels
    # traintimes=[
    #     0.506, 0.435, 1.12, 0.371, 0.269, 0.659, 0.317, 1.055, 0.635, 0.242, 0.562, 0.433, 0.315, 0.333, 0.506, 0.254, 0.627, 0.403, 0.345, 0.436, 0.33, 0.518, 0.612, 0.904, 0.302, 0.617, 0.409, 0.329, 0.438, 0.355, 0.533, 0.555, 0.533, 0.65, 0.401, 0.415, 0.391, 0.654, 0.56, 1.189, 0.453, 0.341, 0.418, 0.331, 0.743, 0.774, 1.123, 0.223, 0.4, 0.635, 0.982, 0.675, 0.754, 0.626, 0.969, 0.425, 0.299, 0.989, 0.655, 0.536, 0.399, 0.593, 0.452, 0.705, 0.392, 0.55, 0.449, 0.53, 0.346, 0.335, 0.497, 0.234, 1.548, 1.972, 1.678, 1.549, 1.506, 1.4, 1.53, 1.524, 1.134, 1.439, 1.493, 1.616, 1.877, 1.758, 1.373, 1.693, 1.508, 1.604, 1.137, 1.537, 1.426, 1.388, 1.682, 1.66, 1.287, 1.377, 1.376, 1.378, 1.366, 1.675, 1.674, 1.501, 0.944, 1.82, 1.402, 1.184, 1.811, 1.143, 1.468, 1.494, 1.532, 1.363, 1.395, 1.732, 1.452, 1.377, 1.681, 1.506, 1.432, 1.783, 1.697, 1.57, 1.383, 1.852, 1.269, 1.378, 1.771, 1.502, 1.536, 1.673, 1.527, 1.397, 1.689, 1.895, 1.529, 1.178, 1.715, 1.525, 1.945, 1.027, 0.927, 0.907, 0.524, 0.414, 0.458, 0.572, 0.513, 0.433, 0.582, 0.466, 0.482, 0.443, 0.426, 0.532, 0.463, 0.677, 0.457, 0.342, 0.471, 0.431, 0.459, 0.437, 0.481, 0.315, 0.518, 0.437, 0.498, 0.374, 0.431, 0.539, 0.413, 0.52, 0.552, 0.463, 0.483, 0.641, 0.402, 0.473, 0.417, 0.456, 0.59, 0.371, 0.474, 0.505, 0.366, 0.457, 0.608, 0.647, 0.429, 0.604, 0.745, 0.482, 0.507, 0.495, 0.566, 0.476, 0.502, 0.491, 0.507, 0.463, 0.478, 0.483, 0.525, 0.552, 0.385, 0.465, 0.493, 0.503, 0.496, 0.319, 0.533, 0.309, 0.386, 0.43, 0.388, 0.419, 0.41, 0.375, 0.367, 0.248, 0.341, 0.353, 0.279, 0.36, 0.332, 0.325, 0.287, 0.355, 0.254, 0.364, 0.443, 0.223, 0.369, 0.395, 0.444, 0.327, 0.41, 0.364, 0.419, 0.39, 0.36, 0.336, 0.349, 0.412, 0.326, 0.426, 0.386, 0.376, 0.416, 0.372, 0.351, 0.433, 0.316, 0.295, 0.263, 0.41, 0.295, 0.378, 0.401, 0.378, 0.352, 0.459, 0.33, 0.36, 0.44, 0.368, 0.392, 0.368, 0.371, 0.429, 0.439, 0.402, 0.405, 0.368, 0.373, 0.415, 0.393, 0.347, 0.381, 0.395, 0.418, 0.317, 0.398, 0.383, 0.283, 0.382, 0.817, 1.041, 0.804, 0.986, 0.93, 0.882, 0.999, 0.869, 0.955, 0.811, 1.022, 1.016, 0.815, 1.124, 0.773, 1.009, 0.96, 0.868, 0.927, 0.736, 0.946, 1.084, 1.014, 0.95, 0.839, 0.937, 0.933, 0.72, 1.096, 0.818, 1.11, 0.938, 1.059, 1.018, 0.937, 0.935, 1.066, 0.962, 0.94, 0.938, 0.817, 0.733, 0.938, 0.795, 1.13, 0.947, 0.938, 0.713, 0.849, 0.733, 1.012, 0.927, 1.196, 0.959, 1.086, 0.874, 1.018, 0.8, 0.948, 1.064, 1.012, 1.031, 0.955, 0.785, 0.823, 1.117, 0.95, 0.865, 0.951, 0.611, 0.808, 0.721, 2.067, 2.165, 2.045, 1.779, 2.155, 2.015, 2.009, 1.809, 1.878, 1.81, 2.49, 2.053, 2.102, 2.298, 2.488, 2.114, 2.432, 2.055, 2.033, 2.26, 2.266, 2.04, 1.947, 2.254, 2.177, 2.068, 1.752, 1.93, 2.272, 1.633, 2.561, 1.967, 1.706, 1.798, 1.99, 2.268, 2.084, 2.286, 2.081, 2.239, 2.183, 2.288, 2.754, 2.578, 1.886, 2.121, 1.74, 2.251, 2.749, 2.034, 2.298, 1.971, 1.607, 2.479, 1.97, 2.514, 2.083, 1.741, 1.734, 1.615, 2.539, 1.709, 2.469, 2.317, 2.256, 1.859, 2.158, 2.065, 2.091, 1.576, 1.409, 1.549,
    #     3.562, 1.364, 2.353, 3.614, 2.632, 3.691, 2.229, 3.458, 4.212, 6.476, 3.767, 2.44, 1.397, 5.533, 1.259, 4.448, 1.708, 2.298, 2.432, 7.326, 5.842, 1.981, 1.689, 2.584, 2.925, 2.258, 2.379, 1.46, 1.141, 1.75, 6.934, 7.13, 7.448, 6.346, 6.214, 7.82, 6.868, 7.988, 7.506, 6.899, 5.759, 6.852, 8.728, 7.82, 6.87, 7.046, 7.462, 7.118, 7.131, 7.794, 6.013, 7.739, 5.673, 7.187, 5.464, 7.775, 7.105, 4.659, 4.217, 3.96, 1.521, 2.004, 1.224, 1.27, 1.189, 1.542, 1.645, 1.961, 1.451, 1.801, 1.53, 2.028, 1.56, 1.175, 1.335, 1.447, 1.267, 1.423, 1.277, 1.477, 1.237, 1.277, 1.63, 2.064, 1.485, 1.467, 1.533, 1.119, 1.081, 0.996, 1.153, 1.545, 1.075, 1.082, 1.183, 1.029, 0.955, 0.977, 1.205, 1.024, 1.012, 1.09, 0.904, 1.09, 0.992, 1.356, 0.989, 1.018, 0.989, 1.109, 1.207, 1.137, 0.887, 1.086, 1.055, 1.012, 0.945, 0.968, 0.865, 1.056, 4.196, 4.138, 3.913, 3.469, 3.61, 3.689, 4.418, 4.229, 4.42, 4.087, 3.536, 3.654, 4.267, 4.571, 3.493, 3.631, 4.295, 3.689, 4.16, 3.438, 4.531, 4.401, 3.872, 3.51, 3.968, 3.897, 4.2, 2.776, 2.812, 2.971, 10.203, 9.079, 10.084, 9.181, 9.764, 9.593, 10.081, 9.308, 12.051, 9.63, 11.009, 8.458, 11.548, 10.519, 10.946, 11.1, 9.797, 11.51, 10.125, 10.545, 8.346, 9.56, 12.463, 11.048, 10.171, 9.644, 11.024, 7.95, 7.699, 6.843
    # ]   # training latency

    # colo = traintimes 
    # color_map = cm.ScalarMappable(cmap=cm.PiYG) 
    # color_map.set_array(colo) 
    # colo_normalized = [c/max(colo) for c in colo]
    # # ax.scatter(samplesize, dimensions, traintimes, facecolors=cm.PiYG(colo_normalized), edgecolor=cm.PiYG(colo_normalized), alpha=1)
    # ax.scatter(samplesize, dimensions, facecolors=cm.PiYG(colo_normalized), edgecolor=cm.PiYG(colo_normalized), alpha=1)
    # plt.colorbar(color_map,label='Training Latency (s)') 

    # ax.set_xlabel('Sample Size')
    # ax.set_ylabel('Feature Dimension Size')
    # # ax.set_zlabel('Training Latency')
    # # ax.zaxis.labelpad = -4

    # # plt.show()
    # # fig.set_size_inches(6, 8)
    # fig.tight_layout()
    # # fig.subplots_adjust(left=-2) 
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()






    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatencypermodel_by_N_models_with_rawinput_data_3"
    # xs=[str(xlabel) for xlabel in [500//50,500//25,500//20,500//15+1,500//10,500//5,500//1]]
    # labels = ["model "+str(i+1) for i in range(46)]
    # ys0_l=[[0.164, 0.665, 1.129, 2.287, 6.121, 45.595, 9103.478]]
    # ys0mean_l = np.array(ys0_l).mean(axis=0).tolist()
    # ys0std_l  = np.array(ys0_l).std(axis=0).tolist()
    # # ys0conf_l = list(scipy.stats.t.interval(0.95, len(ys0_l)-1, loc=np.mean(ys0_l,axis=0), scale=scipy.stats.sem(ys0_l,axis=0)))
    # # ys1_l = [1286.38/(5*5)]
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # # bottom = np.zeros(len(xs))
    # entry_count, width = 2, 0.4
    # p = ax.bar([idx for idx, _ in enumerate(ys0mean_l)], ys0mean_l, width/entry_count, yerr=ys0std_l)
    # # ax.errorbar([idx for idx, _ in enumerate(ys0mean_l)], ys0mean_l, yerr=ys0std_l)
    # # p = ax.bar([idx - width/entry_count/2 + 0*width/entry_count for idx, _ in enumerate(ys)], ys, width/entry_count, label="observed")
    # ax.bar_label(p)
    # # p = ax.bar([idx - width/entry_count/2 + 1*width/entry_count for idx, _ in enumerate(ys1_l)], ys1_l, width/entry_count, label="estimated")
    # # ax.set_title("Training Latency by N Models with Data 3", fontsize=20)
    # # ax.legend(loc="best", prop={'size': 16})
    # ax.set_xticks(list(range(len(xs))))
    # ax.set_xticklabels(xs)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Number of Labels", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()



    fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    filename = "nerccostandtrainlatencypermodel_by_N_models_with_rawinput_data_3"
    xs=[str(xlabel) for xlabel in [500//50,500//25,500//20,500//15+1,500//10,500//5,500//1]]
    labels = ["model "+str(i+1) for i in range(46)]
    ys0_l=[[ys0/60/60*0.013*64 for ys0 in [0.421, 0.886, 1.335, 2.112, 5.392, 91.993, 9847.238]]]
    ys0mean_l = np.array(ys0_l).mean(axis=0).tolist()
    ys0std_l  = np.array(ys0_l).std(axis=0).tolist()
    ys2_l=[[ys2/60/60*2.448 for ys2 in [0.421, 0.886, 1.335, 2.112, 5.392, 91.993, 9847.238]]]
    ys2mean_l = np.array(ys2_l).mean(axis=0).tolist()
    ys2std_l  = np.array(ys2_l).std(axis=0).tolist()
    ys1_l=[[0.421, 0.886, 1.335, 2.112, 5.392, 91.993, 9847.238]]
    ys1mean_l = np.array(ys1_l).mean(axis=0).tolist()
    ys1std_l  = np.array(ys1_l).std(axis=0).tolist()
    # ys0conf_l = list(scipy.stats.t.interval(0.95, len(ys0_l)-1, loc=np.mean(ys0_l,axis=0), scale=scipy.stats.sem(ys0_l,axis=0)))
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # bottom = np.zeros(len(xs))
    entry_count, width = 3, 0.4
    p = ax.bar([idx-width/entry_count-width/entry_count/2 for idx, _ in enumerate(ys0mean_l)], ys0mean_l, width/entry_count, yerr=ys0std_l, color='#0067ff', edgecolor="black", hatch="x", label="NERC VM (\$)")
    p2 = ax.bar([idx-width/entry_count/2 for idx, _ in enumerate(ys2mean_l)], ys2mean_l, width/entry_count, yerr=ys2std_l, color='#00ff67', edgecolor="black", hatch="-", label="AWS EC2 (\$)")
    # ax.bar_label(p)
    # ax.set_title("Training Latency by N Models with Data 3", fontsize=20)
    ax.grid()
    ax.legend(loc="upper left", prop={'size': 16})
    ax.set_xticks(list(range(len(xs))))
    ax.set_xticklabels(xs)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Labels", fontsize=20)
    ax.set_ylabel("Resource Cost (\$)", fontsize=20)
    ax1 = ax.twinx()
    p = ax1.bar([idx+width/entry_count/2 for idx, _ in enumerate(ys1mean_l)], ys1mean_l, width/entry_count, yerr=ys1std_l, color='#ff6700', edgecolor="black", hatch="o", label="Time (s)")
    # ax1.bar_label(p)
    ax1.tick_params(axis='both', which='major', labelsize=20)
    ax1.tick_params(axis='both', which='minor', labelsize=18)
    ax1.set_ylabel("Training Time(s)", fontsize=20)
    ax1.legend(loc="upper center", prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()


    fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    filename = "nerccostandtrainlatencysummodel_by_N_models_with_rawinput_data_3"
    xs=[str(xlabel) for xlabel in [500//50,500//25,500//20,500//15+1,500//10,500//5,500//1]]
    labels = ["model "+str(i+1) for i in range(46)]
    ys0_l=[[ys0/60/60*0.013*64 for ys0 in [20.14, 36.68, 51.58, 101.09, 249.6, 942.14, 9847.24]]]
    ys0mean_l = np.array(ys0_l).mean(axis=0).tolist()
    ys0std_l  = np.array(ys0_l).std(axis=0).tolist()
    ys2_l=[[ys2/60/60*2.448 for ys2 in [20.14, 36.68, 51.58, 101.09, 249.6, 942.14, 9847.24]]]
    ys2mean_l = np.array(ys2_l).mean(axis=0).tolist()
    ys2std_l  = np.array(ys2_l).std(axis=0).tolist()
    ys1_l=[
        [0.42, 0.89, 1.33, 2.11, 5.39, 91.99, 9847.24] ,
        [0.46, 0.81, 0.86, 1.36, 6.53, 84.55, 0] ,
        [0.34, 0.59, 1.1, 2.19, 5.14, 221.68, 0] ,
        [0.36, 0.86, 1.11, 2.41, 5.75, 237.0, 0] ,
        [0.28, 0.82, 1.28, 1.45, 33.52, 306.92, 0] ,
        [0.31, 0.87, 1.02, 2.2, 34.58, 0, 0] ,
        [0.36, 0.67, 1.32, 9.1, 26.02, 0, 0] ,
        [0.41, 0.59, 0.86, 2.88, 45.59, 0, 0] ,
        [0.32, 0.88, 3.38, 3.46, 53.61, 0, 0] ,
        [0.27, 0.74, 1.74, 13.32, 33.47, 0, 0] ,
        [0.36, 2.08, 1.06, 10.91, 0, 0, 0] ,
        [0.35, 1.21, 5.5, 8.48, 0, 0, 0] ,
        [0.36, 0.9, 2.04, 1.93, 0, 0, 0] ,
        [0.31, 1.18, 1.84, 26.65, 0, 0, 0] ,
        [0.29, 1.86, 3.45, 12.62, 0, 0, 0] ,
        [0.26, 1.42, 2.4, 0, 0, 0, 0] ,
        [0.32, 0.78, 1.08, 0, 0, 0, 0] ,
        [0.35, 2.21, 15.83, 0, 0, 0, 0] ,
        [0.32, 0.94, 2.06, 0, 0, 0, 0] ,
        [0.26, 1.68, 2.3, 0, 0, 0, 0] ,
        [0.57, 1.11, 0, 0, 0, 0, 0] ,
        [0.44, 9.68, 0, 0, 0, 0, 0] ,
        [0.43, 0.99, 0, 0, 0, 0, 0] ,
        [0.39, 1.31, 0, 0, 0, 0, 0] ,
        [0.31, 1.63, 0, 0, 0, 0, 0] ,
        [0.58, 0, 0, 0, 0, 0, 0] ,
        [0.18, 0, 0, 0, 0, 0, 0] ,
        [0.47, 0, 0, 0, 0, 0, 0] ,
        [0.33, 0, 0, 0, 0, 0, 0] ,
        [0.62, 0, 0, 0, 0, 0, 0] ,
        [0.4, 0, 0, 0, 0, 0, 0] ,
        [0.37, 0, 0, 0, 0, 0, 0] ,
        [0.29, 0, 0, 0, 0, 0, 0] ,
        [0.43, 0, 0, 0, 0, 0, 0] ,
        [0.41, 0, 0, 0, 0, 0, 0] ,
        [0.61, 0, 0, 0, 0, 0, 0] ,
        [0.44, 0, 0, 0, 0, 0, 0] ,
        [0.2, 0, 0, 0, 0, 0, 0] ,
        [0.6, 0, 0, 0, 0, 0, 0] ,
        [0.31, 0, 0, 0, 0, 0, 0] ,
        [0.27, 0, 0, 0, 0, 0, 0] ,
        [0.43, 0, 0, 0, 0, 0, 0] ,
        [0.5, 0, 0, 0, 0, 0, 0] ,
        [1.42, 0, 0, 0, 0, 0, 0] ,
        [0.56, 0, 0, 0, 0, 0, 0] ,
        [0.28, 0, 0, 0, 0, 0, 0]
        ]
    # ys1mean_l = np.array(ys1_l).mean(axis=0).tolist()
    # ys1std_l  = np.array(ys1_l).std(axis=0).tolist()
    # ys0conf_l = list(scipy.stats.t.interval(0.95, len(ys0_l)-1, loc=np.mean(ys0_l,axis=0), scale=scipy.stats.sem(ys0_l,axis=0)))
    fig, ax = plt.subplots(1, 1, figsize=(10, 4))
    # bottom = np.zeros(len(xs))
    entry_count, width = 3, 0.4
    p = ax.bar([idx-width/entry_count-width/entry_count/2 for idx, _ in enumerate(ys0mean_l)], ys0mean_l, width/entry_count, yerr=ys0std_l, color='#0067ff', edgecolor="black", hatch="x", label="NERC VM (\$)")
    p2 = ax.bar([idx-width/entry_count/2 for idx, _ in enumerate(ys2mean_l)], ys2mean_l, width/entry_count, yerr=ys2std_l, color='#00ff67', edgecolor="black", hatch="|", label="AWS EC2 (\$)")
    # ax.bar_label(p)
    # ax.set_title("Training Latency by N Models with Data 3", fontsize=20)
    ax.grid()
    ax.legend(loc="upper left", prop={'size': 16})
    ax.set_xticks(list(range(len(xs))))
    ax.set_xticklabels(xs)
    ax.tick_params(axis='both', which='major', labelsize=20)
    ax.tick_params(axis='both', which='minor', labelsize=18)
    ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
    ax.set_ylabel("Resource Cost (\$)", fontsize=20)
    ax1 = ax.twinx()
    bottom = [0 for _ in range(len(ys1_l[0]))]
    for row in ys1_l[:-1]:
        p = ax1.bar([idx+width/entry_count/2 for idx, _ in enumerate(row)], row, width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o")
        bottom = [v1+v2 for v1,v2 in zip(bottom,row)]
    p = ax1.bar([idx+width/entry_count/2 for idx, _ in enumerate(ys1_l[-1])], ys1_l[-1], width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o", label="Total Training Time (s) for Submodels")
    # p = ax1.bar([idx+width/entry_count/2 for idx, _ in enumerate(ys1mean_l)], ys1mean_l, width/entry_count, yerr=ys1std_l, color='#ff6700', edgecolor="black", hatch="o", label="Time (s)")
    # ax1.bar_label(p)
    ax1.tick_params(axis='both', which='major', labelsize=20)
    ax1.tick_params(axis='both', which='minor', labelsize=18)
    ax1.set_ylabel("Training Time(s)", fontsize=20)
    ax1.legend(loc="center left", prop={'size': 16})
    # plt.show()
    plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    plt.close()


    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_N_models_with_rawinput_data_0"
    # xs=[str(xlabel) for xlabel in [10,20,40,80]]
    # labels = ["model "+str(i+1) for i in range(46)]
    # ys0_l=[[8*2.4, 4*2.4*2*(4*math.log(25*20)/math.log(25*10)), 2*19.0*2*(4*math.log(25*40)/math.log(25*20)), 143.5*2*(4*math.log(25*80)/math.log(25*40))]]
    # ys1_l=[
    #     [2.407, 18.840, 143.565, 1090.042],
    #     [2.396, 18.788, 143.717, 0],
    #     [2.413, 19.016, 0, 0],
    #     [2.407, 18.862, 0, 0],
    #     [2.406, 0, 0, 0],
    #     [2.396, 0, 0, 0],
    #     [2.440, 0, 0, 0],
    #     [2.396, 0, 0, 0]
    #     ]
    # fig, ax = plt.subplots(1, 1, figsize=(10, 4))
    # # bottom = np.zeros(len(xs))
    # entry_count, width = 2, 0.4
    # # p = ax.bar([idx-width/entry_count/2 for idx, _ in enumerate(ys0_l[0])], ys0_l[0], width/entry_count, color='#00ab00', edgecolor="black", hatch="x", label="Estimated")
    # # ax.bar_label(p)
    # bottom = [0 for _ in range(len(ys1_l[0]))]
    # # for row in ys1_l[:-1]:
    # #     p = ax.bar([idx+width/entry_count/2 for idx, _ in enumerate(row)], row, width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o")
    # #     bottom = [v1+v2 for v1,v2 in zip(bottom,row)]
    # # p = ax.bar([idx+width/entry_count/2 for idx, _ in enumerate(ys1_l[-1])], ys1_l[-1], width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o", label="Total Training Time (s) for Submodels")
    # for row in ys1_l[:-1]:
    #     p = ax.bar([idx for idx, _ in enumerate(row)], row, width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o")
    #     bottom = [v1+v2 for v1,v2 in zip(bottom,row)]
    # p = ax.bar([idx for idx, _ in enumerate(ys1_l[-1])], ys1_l[-1], width/entry_count, bottom, color='#ff6700', edgecolor="black", hatch="o", label="Total Training Time (s) for Submodels")
    # ax.bar_label(p)    
    # ax.grid()
    # ax.legend(loc="upper left", prop={'size': 16})
    # ax.set_xticks(list(range(len(xs))))
    # ax.set_xticklabels(xs)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Number of Labels per Submodel", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()



    # by_input_size
    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "testf1score_by_input_size_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]
    # labels = ("80 labels", "40 labels","20 labels","10 labels")
    # ys_l=[[0.077, 0.521, 0.697, 0.727, 0.806, 0.843, 0.847, 0.906, 0.935, 0.934, 0.948, 0.951, 0.954],
    #     [0.166, 0.609, 0.746, 0.789, 0.857, 0.846, 0.891, 0.896, 0.911, 0.905, 0.911, 0.904, 0.916],
    #     [0.210, 0.633, 0.758, 0.763, 0.829, 0.841, 0.828, 0.886, 0.889, 0.884, 0.902, 0.891, 0.898],
    #     [0.259, 0.588, 0.714, 0.775, 0.741, 0.836, 0.851, 0.837, 0.815, 0.843, 0.842, 0.846, 0.843]
    #     ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Input Dimensions", fontsize=20)
    # ax.set_ylabel("F1-Scores", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_input_size_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]
    # labels = ("10 labels", "20 labels","40 labels","80 labels")
    # ys_l=[[0.102, 0.103, 0.143, 0.156, 0.226, 0.278, 0.392, 0.664, 1.205, 2.407, 5.449, 11.784, 24.566],
    #     [0.229, 0.290, 0.406, 0.451, 0.700, 0.928, 1.255, 2.366, 4.464, 9.328, 18.840, 39.945, 80.171],
    #     [0.315, 0.550, 1.027, 1.514, 2.580, 3.246, 4.596, 8.739, 17.492, 34.972, 70.854, 143.565, 286.711],
    #     [1.227, 2.012, 3.810, 5.275, 9.333, 12.124, 17.675, 34.274, 67.655, 136.472, 274.488, 546.612, 1096.709]
    #     ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Input Dimensions", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # filename = "inferencelatency_by_input_size_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]
    # # labels = ("10 labels", "20 labels","40 labels","80 labels")
    # # ys_l=[[0.017+0.015+0.015+0.015+0.015+0.015+0.015+0.015, 0.017+0.024+0.017+0.017+0.017+0.018+0.018+0.018, 0.020+0.020+0.020+0.020+0.021+0.021+0.021+0.022, 0.022+0.023+0.023+0.023+0.024+0.023+0.023+0.024, 0.030+0.030+0.030+0.030+0.031+0.031+0.031+0.031, 0.034+0.036+0.035+0.034+0.035+0.035+0.035+0.035, 0.045+0.042+0.042+0.042+0.042+0.044+0.043+0.043, 0.065+0.065+0.067+0.066+0.065+0.065+0.065+0.065, 0.114+0.115+0.114+0.114+0.114+0.114+0.115+0.115, 0.212+0.214+0.214+0.212+0.214+0.213+0.213+0.218, 0.408+0.409+0.411+0.411+0.409+0.409+0.410+0.409, 0.810+0.817+0.817+0.815+0.814+0.809+0.814+0.813, 1.675+1.686+1.681+1.677+1.672+1.679+1.679+1.678],
    # #     [0.030+0.031+0.031+0.030, 0.031+0.032+0.031+0.032, 0.034+0.034+0.034+0.034, 0.036+0.037+0.037+0.037, 0.045+0.044+0.045+0.045, 0.047+0.047+0.048+0.050, 0.055+0.059+0.056+0.056, 0.079+0.079+0.079+0.081, 0.128+0.132+0.129+0.131, 0.226+0.226+0.225+0.227, 0.425+0.424+0.423+0.425, 0.831+0.830+0.831+0.836, 1.711+1.709+1.706+1.711],
    # #     [0.054+0.054, 0.056+0.058, 0.060+0.063, 0.061+0.061, 0.073+0.074, 0.079+0.081, 0.086+0.090, 0.112+0.122, 0.170+0.165, 0.268+0.263, 0.461+0.465, 0.875+0.883, 1.772+1.779],
    # #     [0.101, 0.107, 0.111, 0.122, 0.136, 0.149, 0.161, 0.199, 0.256, 0.380, 0.547, 0.979, 1.959]
    # #     ]
    # xs=[13664, 27329, 54659, 109319]
    # labels = ("10 labels per model", "20 labels per model","40 labels per model","80 labels per model")
    # ys_l=[[0.212+0.214+0.214+0.212+0.214+0.213+0.213+0.218, 0.408+0.409+0.411+0.411+0.409+0.409+0.410+0.409, 0.810+0.817+0.817+0.815+0.814+0.809+0.814+0.813, 1.675+1.686+1.681+1.677+1.672+1.679+1.679+1.678],
    #     [0.226+0.226+0.225+0.227, 0.425+0.424+0.423+0.425, 0.831+0.830+0.831+0.836, 1.711+1.709+1.706+1.711],
    #     [0.268+0.263, 0.461+0.465, 0.875+0.883, 1.772+1.779],
    #     [0.380, 0.547, 0.979, 1.959]
    #     ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.hlines(1.92, xmin=13664, xmax=109319)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Input Dimensions", fontsize=20)
    # ax.set_ylabel("Inference Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_input_size_with_rawinput_data_0_estimated"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]
    # labels = ("Observed", "Estimated")
    # ys_l=[[1.227, 2.012, 3.810, 5.275, 9.333, 12.124, 17.675, 34.274, 67.655, 136.472, 274.488, 546.612, 1096.709],
    #     #   [1.227, 1.227*2, 1.227*2**2, 1.227*2**3, 1.227*2**4, 1.227*2**5, 1.227*2**6, 1.227*2**7, 1.227*2**8, 1.227*2**9, 1.227*2**10, 1.227*2**11, 1.227*2**12],
    #     # [1.227, 1.227*2, 2.012*2, 3.810*2, 5.275*2, 9.333*2, 12.124*2, 17.675*2, 34.274*2, 67.655*2, 136.472*2, 274.488*2, 546.612*2]
    #     [1096.709/2**12, 1096.709/2**11, 1096.709/2**10, 1096.709/2**9, 1096.709/2**8, 1096.709/2**7, 1096.709/2**6, 1096.709/2**5, 1096.709/2**4, 1096.709/2**3, 1096.709/2**2, 1096.709/2, 1096.709]
    #     ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Input Dimensions", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_input_size_with_rawinput_data_0_estimated_sanitycheck"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[13, 106, 284, 427, 854, 1138, 1708, 3416, 6832, 13664, 27329, 54659, 109319]
    # labels = ("Observed", "Estimated")
    # ys_l=[[1.227, 2.012, 3.810, 5.275, 9.333, 12.124, 17.675, 34.274, 67.655, 136.472, 274.488, 546.612, 1096.709],
    #       [1.227, 1.227*2, 1.227*2**2, 1.227*2**3, 1.227*2**4, 1.227*2**5, 1.227*2**6, 1.227*2**7, 1.227*2**8, 1.227*2**9, 1.227*2**10, 1.227*2**11, 1.227*2**12]
    #     ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.set_xlabel("Input Dimensions")
    # ax.set_ylabel("Training Time(s)")
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()





















    # by_labels_per_model

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("13 dims", "106 dims", "284 dims", "427 dims", "854 dims", "1138 dims", "1708 dims", "3416 dims", "6832 dims", "13664 dims", "27329 dims", "54659 dims", "109319 dims")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[0.102, 0.229, 0.315, 1.227], 
    #         [0.103, 0.29, 0.55, 2.012], 
    #         [0.143, 0.406, 1.027, 3.81], 
    #         [0.156, 0.451, 1.514, 5.275], 
    #         [0.226, 0.7, 2.58, 9.333], 
    #         [0.278, 0.928, 3.246, 12.124], 
    #         [0.392, 1.255, 4.596, 17.675], 
    #         [0.664, 2.366, 8.739, 34.274], 
    #         [1.205, 4.464, 17.492, 67.655], 
    #         [2.407, 9.328, 34.972, 136.472], 
    #         [5.449, 18.84, 70.854, 274.488], 
    #         [11.784, 39.945, 143.565, 546.612], 
    #         [24.566, 80.171, 286.711, 1096.709]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "inferencelatency_by_labels_per_model_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # # xs=[10, 20, 40, 80]
    # # labels = ("13 dims", "106 dims", "284 dims", "427 dims", "854 dims", "1138 dims", "1708 dims", "3416 dims", "6832 dims", "13664 dims", "27329 dims", "54659 dims", "109319 dims")
    # # # ys_l=list(map(list, zip(*ys_l)))
    # # # print(ys_l)
    # # ys_l = [[0.017+0.015+0.015+0.015+0.015+0.015+0.015+0.015, 0.030+0.031+0.031+0.030, 0.054+0.054, 0.101],
    # #         [0.017+0.024+0.017+0.017+0.017+0.018+0.018+0.018, 0.031+0.032+0.031+0.032, 0.056+0.058, 0.107],
    # #         [0.020+0.020+0.020+0.020+0.021+0.021+0.021+0.022, 0.034+0.034+0.034+0.034, 0.060+0.063, 0.111],
    # #         [0.022+0.023+0.023+0.023+0.024+0.023+0.023+0.024, 0.036+0.037+0.037+0.037, 0.061+0.061, 0.122],
    # #         [0.030+0.030+0.030+0.030+0.031+0.031+0.031+0.031, 0.045+0.044+0.045+0.045, 0.073+0.074, 0.136],
    # #         [0.034+0.036+0.035+0.034+0.035+0.035+0.035+0.035, 0.047+0.047+0.048+0.050, 0.079+0.081, 0.149],
    # #         [0.045+0.042+0.042+0.042+0.042+0.044+0.043+0.043, 0.055+0.059+0.056+0.056, 0.086+0.090, 0.161],
    # #         [0.065+0.065+0.067+0.066+0.065+0.065+0.065+0.065, 0.079+0.079+0.079+0.081, 0.112+0.122, 0.199],
    # #         [0.114+0.115+0.114+0.114+0.114+0.114+0.115+0.115, 0.128+0.132+0.129+0.131, 0.170+0.165, 0.256],
    # #         [0.212+0.214+0.214+0.212+0.214+0.213+0.213+0.218, 0.226+0.226+0.225+0.227, 0.268+0.263, 0.380],
    # #         [0.408+0.409+0.411+0.411+0.409+0.409+0.410+0.409, 0.425+0.424+0.423+0.425, 0.461+0.465, 0.547],
    # #         [0.810+0.817+0.817+0.815+0.814+0.809+0.814+0.813, 0.831+0.830+0.831+0.836, 0.875+0.883, 0.979],
    # #         [1.675+1.686+1.681+1.677+1.672+1.679+1.679+1.678, 1.711+1.709+1.706+1.711, 1.772+1.779, 1.959]
    # #         ]
    # xs=[10, 20, 40, 80]
    # labels = ("13664 dims", "27329 dims", "54659 dims", "109319 dims")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[0.212+0.214+0.214+0.212+0.214+0.213+0.213+0.218, 0.226+0.226+0.225+0.227, 0.268+0.263, 0.380],
    #         [0.408+0.409+0.411+0.411+0.409+0.409+0.410+0.409, 0.425+0.424+0.423+0.425, 0.461+0.465, 0.547],
    #         [0.810+0.817+0.817+0.815+0.814+0.809+0.814+0.813, 0.831+0.830+0.831+0.836, 0.875+0.883, 0.979],
    #         [1.675+1.686+1.681+1.677+1.672+1.679+1.679+1.678, 1.711+1.709+1.706+1.711, 1.772+1.779, 1.959]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Inference Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "testf1score_by_labels_per_model_with_rawinput_data_0"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("13 dims", "106 dims", "284 dims", "427 dims", "854 dims", "1138 dims", "1708 dims", "3416 dims", "6832 dims", "13664 dims", "27329 dims", "54659 dims", "109319 dims")
    # ys_l=[[0.259, 0.21, 0.166, 0.077], 
    #       [0.588, 0.633, 0.609, 0.521], 
    #       [0.714, 0.758, 0.746, 0.697], 
    #       [0.775, 0.763, 0.789, 0.727], 
    #       [0.741, 0.829, 0.857, 0.806], 
    #       [0.836, 0.841, 0.846, 0.843], 
    #       [0.851, 0.828, 0.891, 0.847], 
    #       [0.837, 0.886, 0.896, 0.906], 
    #       [0.815, 0.889, 0.911, 0.935], 
    #       [0.843, 0.884, 0.905, 0.934], 
    #       [0.842, 0.902, 0.911, 0.948], 
    #       [0.846, 0.891, 0.904, 0.951], 
    #       [0.843, 0.898, 0.916, 0.954]]
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("F1-Score", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "testf1score_by_labels_per_model_with_rawinput_data_3"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 5))
    # xs=[10, 20, 25, 34, 50, 100]
    # labels = ("500labels-F1-with-filter", "500labels-F1-no-filter")
    # # labels = ("500labels-F1-with-filter", "500labels-Precision-with-filter", "500labels-F1-no-filter", "500labels-Precision-no-filter")
    # ys_l=[
    #     [0.884, 0.885, 0.885, 0.883, 0.884, 0.901],
    #     # [0.834, 0.836, 0.837, 0.834, 0.834, 0.855],
    #     [0.618, 0.641, 0.655, 0.679, 0.696, 0.743],
    #     # [0.557, 0.58, 0.592, 0.617, 0.635, 0]
    # ]
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # xs=[10, 20, 40, 80]
    # labels = ("80labels-F1-with-filter", "80labels-F1-no-filter")
    # # labels = ("80labels-F1-with-filter", "80labels-Precision-with-filter", "80labels-F1-no-filter", "80labels-Precision-no-filter")
    # ys_l=[
    #     [0.883, 0.918, 0.957, 0.991],
    #     # [0.817, -1,-1,-1],
    #     [0.857, 0.903, 0.945, 0.998],
    #     # [0.843, 0.855, 0.868, 0.996]
    # ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # # ax.set_xscale('log')
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylim(0.3,1)
    # ax.set_ylabel("F1-Score", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_0_estimated"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated", "Estimated_nosorting")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[24.566, 80.171, 286.711, 1096.709],
    #         # [24.566,
    #         # 24.566*(4*math.log(25*20)/math.log(25*10)),
    #         # 80.171*(4*math.log(25*40)/math.log(25*20)), 
    #         # 286.711*(4*math.log(25*80)/math.log(25*40))]
    #         [1096.709/(4*math.log(25*80)/math.log(25*40))/(4*math.log(25*40)/math.log(25*20))/(4*math.log(25*20)/math.log(25*10)),
    #         1096.709/(4*math.log(25*80)/math.log(25*40))/(4*math.log(25*40)/math.log(25*20)), 
    #         1096.709/(4*math.log(25*80)/math.log(25*40)),
    #         1096.709],
    #         [1096.709/(4)/(4)/(4),
    #         1096.709/(4)/(4), 
    #         1096.709/(4),
    #         1096.709]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_3_estimated"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated", "Estimated_nosorting")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[24.566, 80.171, 286.711, 1096.709],
    #         # [24.566,
    #         # 24.566*(4*math.log(25*20)/math.log(25*10)),
    #         # 80.171*(4*math.log(25*40)/math.log(25*20)), 
    #         # 286.711*(4*math.log(25*80)/math.log(25*40))]
    #         [1096.709/(4*math.log(25*80)/math.log(25*40))/(4*math.log(25*40)/math.log(25*20))/(4*math.log(25*20)/math.log(25*10)),
    #         1096.709/(4*math.log(25*80)/math.log(25*40))/(4*math.log(25*40)/math.log(25*20)), 
    #         1096.709/(4*math.log(25*80)/math.log(25*40)),
    #         1096.709],
    #         [1096.709/(4)/(4)/(4),
    #         1096.709/(4)/(4), 
    #         1096.709/(4),
    #         1096.709]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_0_estimated_1njob_no_sorting"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[148.500, 555.019, 2153.236, 8474.915],
    #         # [148.500,
    #         # 148.500*(4*(math.log(25*20)+1)/(math.log(25*10)+1)),
    #         # 555.019*(4*(math.log(25*40)+1)/(math.log(25*20)+1)), 
    #         # 2153.236*(4*(math.log(25*80)+1)/(math.log(25*40)+1))]
    #         [148.500,
    #         148.500*(4),
    #         555.019*(4), 
    #         2153.236*(4)]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_0_estimated_1njob_doublesample_nosorting"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[282.00, 1080.01, 4254.03, 17168.33],
    #         # [282.00,
    #         # 282.00*(4*(math.log(50*20)+1)/(math.log(50*10)+1)),
    #         # 1080.01*(4*(math.log(50*40)+1)/(math.log(50*20)+1)), 
    #         # 4254.03*(4*(math.log(50*80)+1)/(math.log(50*40)+1))]
    #         [282.00,
    #         282.00*(4*1),
    #         1080.01*(4*1), 
    #         4254.03*(4*1)]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_randomint10000000_109319_data_0_estimated_1njob_doublesample"
    # # filename = "trainlatency_by_labels_per_model_with_randomint10000000_109319_data_0_estimated_1njob_doublesample_nosorting"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[546.535, 2094.348, 8248.998, 33385.295],
    #         [546.535,
    #         546.535*(4*(math.log(50*20)+1)/(math.log(50*10)+1)),
    #         2094.348*(4*(math.log(50*40)+1)/(math.log(50*20)+1)), 
    #         8248.998*(4*(math.log(50*80)+1)/(math.log(50*40)+1))]
    #         # [546.535,
    #         # 546.535*(4*1),
    #         # 2094.348*(4*1), 
    #         # 8248.998*(4*1)]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_rawinput_data_0_estimated_2njob_no_sorting"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[84.723, 288.412, 1085.795, 4272.973],
    #         # [84.723,
    #         # 84.723*(4*(math.log(25*20)+1)/(math.log(25*10)+1)),
    #         # 288.412*(4*(math.log(25*40)+1)/(math.log(25*20)+1)), 
    #         # 1085.795*(4*(math.log(25*80)+1)/(math.log(25*40)+1))]
    #         [84.723,
    #         84.723*(4),
    #         288.412*(4), 
    #         1085.795*(4)]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.tick_params(axis='both', which='major', labelsize=20)
    # ax.tick_params(axis='both', which='minor', labelsize=18)
    # ax.set_xlabel("Labels Per Model", fontsize=20)
    # ax.set_ylabel("Training Time(s)", fontsize=20)
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()

    # fig_path = '/home/cc/Praxi-study/Praxi-Pipeline/prediction_XGBoost_openshift_image/model_testing_scripts/figs/'
    # filename = "trainlatency_by_labels_per_model_with_randomint10000000_109319_estimated_sanitycheck"
    # fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    # xs=[10, 20, 40, 80]
    # labels = ("Observed", "Estimated")
    # # ys_l=list(map(list, zip(*ys_l)))
    # # print(ys_l)
    # ys_l = [[6.733, 51.841, 397.018, 3195.553],
    #         [6.733,
    #          6.733*(4*math.log(25*20)/math.log(25*10)),
    #          6.733*(4*math.log(25*20)/math.log(25*10))*(4*math.log(25*40)/math.log(25*20)), 
    #          6.733*(4*math.log(25*20)/math.log(25*10))*(4*math.log(25*40)/math.log(25*20))*(4*math.log(25*80)/math.log(25*40))]
    #         ]
    # for ys, label in zip(ys_l, labels):
    #     ax.scatter(xs, ys, label=label)
    # ax.set_xlabel("Labels Per Model")
    # ax.set_ylabel("Training Time(s)")
    # plt.legend(prop={'size': 16})
    # # plt.show()
    # plt.savefig(fig_path+filename+'.pdf', bbox_inches='tight')
    # plt.close()